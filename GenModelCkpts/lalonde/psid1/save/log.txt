2020-11-12 01:47:02.182349 / Namespace(activation='ReLU', atoms=[0.0], batch_size=25000, comet=True, data='lalonde', dataroot='/home/mila/r/raghupas/causal-benchmark/datasets', dim_h=4, dist='SigmoidFlow', dist_args=['ndim=64', 'base_distribution=uniform'], early_stop=True, eval=True, grad_norm=inf, ignore_w=False, lr=0.01, n_hidden_layers=1, num_epochs=10000, num_univariate_tests=100, overwrite_reload='', patience=None, saveroot='save', seed=123, test_prop=0.4, test_size=None, train=True, train_prop=0.5, val_prop=0.1, w_transform='Normalize', y_transform='Normalize') 
2020-11-12 01:47:02.184852 / getting data: lalonde 
2020-11-12 01:47:24.918989 / comet url: https://www.comet.ml/sunandr/causal-benchmark/9684262eadd5427592d6f64bf752c987 
2020-11-12 01:47:24.920754 / ate: None 
2020-11-12 01:47:24.922415 / <models.distributions.distributions.MixedDistribution object at 0x7fe3cd55cb50> 
2020-11-12 01:47:24.924303 / {'n_hidden_layers': 1, 'dim_h': 4, 'activation': ReLU()} 
2020-11-12 01:47:24.926180 / {'batch_size': 25000, 'lr': 0.01, 'num_epochs': 10000, 'verbose': True, 'print_every_iters': 100, 'optim': <class 'torch.optim.adam.Adam'>, 'eval_every': 100, 'plot_every': 100, 'p_every': 100, 'optim_args': {}} 
2020-11-12 01:47:27.805694 / Iteration 100: 0.2641369700431824 -0.006096299737691879 
2020-11-12 01:47:27.815373 / Iteration 100 valid loss 0.26809659600257874 
2020-11-12 01:47:27.816767 / saving best-val-loss model 
2020-11-12 01:47:34.943163 / Iteration 200: 0.24058277904987335 -0.30659085512161255 
2020-11-12 01:47:34.951493 / Iteration 200 valid loss -0.04429924488067627 
2020-11-12 01:47:34.953010 / saving best-val-loss model 
2020-11-12 01:47:42.279669 / Iteration 300: 0.24056892096996307 -0.31790006160736084 
2020-11-12 01:47:42.288145 / Iteration 300 valid loss -0.05994302034378052 
2020-11-12 01:47:42.289852 / saving best-val-loss model 
2020-11-12 01:47:49.517997 / Iteration 400: 0.24056881666183472 -0.3205437660217285 
2020-11-12 01:47:49.527145 / Iteration 400 valid loss -0.0627129077911377 
2020-11-12 01:47:49.528852 / saving best-val-loss model 
2020-11-12 01:47:56.917867 / Iteration 500: 0.2405688315629959 -0.32217440009117126 
2020-11-12 01:47:56.926949 / Iteration 500 valid loss -0.06348264217376709 
2020-11-12 01:47:56.928633 / saving best-val-loss model 
2020-11-12 01:48:04.287887 / Iteration 600: 0.2405688315629959 -0.32410672307014465 
2020-11-12 01:48:04.296989 / Iteration 600 valid loss -0.06249624118208885 
2020-11-12 01:48:11.621727 / Iteration 700: 0.2405688315629959 -0.32593727111816406 
2020-11-12 01:48:11.630374 / Iteration 700 valid loss -0.06215432286262512 
2020-11-12 01:48:19.028779 / Iteration 800: 0.2405688315629959 -0.3292508125305176 
2020-11-12 01:48:19.038076 / Iteration 800 valid loss -0.06350940465927124 
2020-11-12 01:48:19.039809 / saving best-val-loss model 
2020-11-12 01:48:26.561385 / Iteration 900: 0.2405688613653183 -0.3327648341655731 
2020-11-12 01:48:26.569591 / Iteration 900 valid loss -0.06718289852142334 
2020-11-12 01:48:26.571196 / saving best-val-loss model 
2020-11-12 01:48:34.097240 / Iteration 1000: 0.2405688315629959 -0.3342893421649933 
2020-11-12 01:48:34.105213 / Iteration 1000 valid loss -0.06892621517181396 
2020-11-12 01:48:34.106375 / saving best-val-loss model 
2020-11-12 01:48:41.672194 / Iteration 1100: 0.2405688613653183 -0.3349481225013733 
2020-11-12 01:48:41.680524 / Iteration 1100 valid loss -0.06926891207695007 
2020-11-12 01:48:41.681917 / saving best-val-loss model 
2020-11-12 01:48:49.091383 / Iteration 1200: 0.2405688762664795 -0.33760958909988403 
2020-11-12 01:48:49.100556 / Iteration 1200 valid loss -0.06971925497055054 
2020-11-12 01:48:49.102110 / saving best-val-loss model 
2020-11-12 01:48:56.523760 / Iteration 1300: 0.2405688613653183 -0.3400939404964447 
2020-11-12 01:48:56.532506 / Iteration 1300 valid loss -0.06889227032661438 
2020-11-12 01:49:04.052016 / Iteration 1400: 0.2405688315629959 -0.3411712348461151 
2020-11-12 01:49:04.060629 / Iteration 1400 valid loss -0.06889459490776062 
2020-11-12 01:49:11.528820 / Iteration 1500: 0.2405688613653183 -0.3420872390270233 
2020-11-12 01:49:11.537388 / Iteration 1500 valid loss -0.0694015622138977 
2020-11-12 01:49:18.892080 / Iteration 1600: 0.24056890606880188 -0.3428969383239746 
2020-11-12 01:49:18.900935 / Iteration 1600 valid loss -0.07023334503173828 
2020-11-12 01:49:18.903506 / saving best-val-loss model 
2020-11-12 01:49:26.655986 / Iteration 1700: 0.2405688613653183 -0.3434025049209595 
2020-11-12 01:49:26.664768 / Iteration 1700 valid loss -0.07066568732261658 
2020-11-12 01:49:26.666281 / saving best-val-loss model 
2020-11-12 01:49:34.287690 / Iteration 1800: 0.2405688613653183 -0.343932181596756 
2020-11-12 01:49:34.295670 / Iteration 1800 valid loss -0.06958544254302979 
2020-11-12 01:49:41.744386 / Iteration 1900: 0.2405688315629959 -0.34430867433547974 
2020-11-12 01:49:41.752337 / Iteration 1900 valid loss -0.06938022375106812 
2020-11-12 01:49:49.165567 / Iteration 2000: 0.2405688315629959 -0.34459608793258667 
2020-11-12 01:49:49.173778 / Iteration 2000 valid loss -0.069163978099823 
2020-11-12 01:49:56.499915 / Iteration 2100: 0.2405688315629959 -0.3448152542114258 
2020-11-12 01:49:56.508241 / Iteration 2100 valid loss -0.06893375515937805 
2020-11-12 01:50:03.761263 / Iteration 2200: 0.2405688315629959 -0.34499892592430115 
2020-11-12 01:50:03.770654 / Iteration 2200 valid loss -0.06878188252449036 
2020-11-12 01:50:11.495725 / Iteration 2300: 0.2405688315629959 -0.3451509475708008 
2020-11-12 01:50:11.504376 / Iteration 2300 valid loss -0.06860312819480896 
2020-11-12 01:50:18.957128 / Iteration 2400: 0.2405688315629959 -0.3453041613101959 
2020-11-12 01:50:18.965235 / Iteration 2400 valid loss -0.0686216652393341 
2020-11-12 01:50:26.329625 / Iteration 2500: 0.2405688613653183 -0.3455873131752014 
2020-11-12 01:50:26.337333 / Iteration 2500 valid loss -0.06891709566116333 
2020-11-12 01:50:33.760538 / Iteration 2600: 0.2405688315629959 -0.3459676206111908 
2020-11-12 01:50:33.769109 / Iteration 2600 valid loss -0.06892746686935425 
2020-11-12 01:50:41.175476 / Iteration 2700: 0.2405688613653183 -0.3463667333126068 
2020-11-12 01:50:41.184258 / Iteration 2700 valid loss -0.06880462169647217 
2020-11-12 01:50:48.985259 / Iteration 2800: 0.2405688613653183 -0.34672170877456665 
2020-11-12 01:50:48.994231 / Iteration 2800 valid loss -0.06857481598854065 
2020-11-12 01:50:56.755674 / Iteration 2900: 0.2405688315629959 -0.34700584411621094 
2020-11-12 01:50:56.764575 / Iteration 2900 valid loss -0.06834721565246582 
2020-11-12 01:51:04.393967 / Iteration 3000: 0.2405688613653183 -0.34721383452415466 
2020-11-12 01:51:04.401771 / Iteration 3000 valid loss -0.06813046336174011 
2020-11-12 01:51:11.872492 / Iteration 3100: 0.2405688315629959 -0.34734711050987244 
2020-11-12 01:51:11.881119 / Iteration 3100 valid loss -0.06777864694595337 
2020-11-12 01:51:19.457567 / Iteration 3200: 0.24056881666183472 -0.3474564850330353 
2020-11-12 01:51:19.466119 / Iteration 3200 valid loss -0.0678015649318695 
2020-11-12 01:51:26.752189 / Iteration 3300: 0.24056878685951233 -0.3475417196750641 
2020-11-12 01:51:26.759966 / Iteration 3300 valid loss -0.06774002313613892 
2020-11-12 01:51:34.122659 / Iteration 3400: 0.24056881666183472 -0.3476000726222992 
2020-11-12 01:51:34.130309 / Iteration 3400 valid loss -0.06769302487373352 
2020-11-12 01:51:41.567980 / Iteration 3500: 0.2405688315629959 -0.34764808416366577 
2020-11-12 01:51:41.576719 / Iteration 3500 valid loss -0.0676824152469635 
2020-11-12 01:51:49.075184 / Iteration 3600: 0.2405688762664795 -0.3476581573486328 
2020-11-12 01:51:49.083960 / Iteration 3600 valid loss -0.06766310334205627 
2020-11-12 01:51:56.495341 / Iteration 3700: 0.2405688762664795 -0.34772610664367676 
2020-11-12 01:51:56.503832 / Iteration 3700 valid loss -0.06766626238822937 
2020-11-12 01:52:04.088872 / Iteration 3800: 0.24056890606880188 -0.3477637767791748 
2020-11-12 01:52:04.096713 / Iteration 3800 valid loss -0.06773144006729126 
2020-11-12 01:52:11.632437 / Iteration 3900: 0.24056890606880188 -0.34780076146125793 
2020-11-12 01:52:11.639886 / Iteration 3900 valid loss -0.06774282455444336 
2020-11-12 01:52:19.158667 / Iteration 4000: 0.24056890606880188 -0.34783878922462463 
2020-11-12 01:52:19.167358 / Iteration 4000 valid loss -0.06775188446044922 
2020-11-12 01:52:26.567722 / Iteration 4100: 0.2405688613653183 -0.3479323089122772 
2020-11-12 01:52:26.576032 / Iteration 4100 valid loss -0.06781831383705139 
2020-11-12 01:52:33.991686 / Iteration 4200: 0.2405688762664795 -0.34823596477508545 
2020-11-12 01:52:33.999427 / Iteration 4200 valid loss -0.06739276647567749 
2020-11-12 01:52:41.389712 / Iteration 4300: 0.2405688762664795 -0.348438560962677 
2020-11-12 01:52:41.398360 / Iteration 4300 valid loss -0.0673866868019104 
2020-11-12 01:52:48.834038 / Iteration 4400: 0.2405688613653183 -0.34886306524276733 
2020-11-12 01:52:48.843182 / Iteration 4400 valid loss -0.06785345077514648 
2020-11-12 01:52:56.290508 / Iteration 4500: 0.2405688613653183 -0.35018301010131836 
2020-11-12 01:52:56.298858 / Iteration 4500 valid loss -0.06739440560340881 
2020-11-12 01:53:03.844002 / Iteration 4600: 0.24056890606880188 -0.3508189618587494 
2020-11-12 01:53:03.852583 / Iteration 4600 valid loss -0.06654107570648193 
2020-11-12 01:53:11.259195 / Iteration 4700: 0.2405688613653183 -0.3510447144508362 
2020-11-12 01:53:11.267889 / Iteration 4700 valid loss -0.0657360851764679 
2020-11-12 01:53:18.608087 / Iteration 4800: 0.2405688315629959 -0.35113775730133057 
2020-11-12 01:53:18.616037 / Iteration 4800 valid loss -0.0653870701789856 
2020-11-12 01:53:25.941033 / Iteration 4900: 0.2405688762664795 -0.35120341181755066 
2020-11-12 01:53:25.950049 / Iteration 4900 valid loss -0.06521493196487427 
2020-11-12 01:53:33.266402 / Iteration 5000: 0.2405688762664795 -0.3512652516365051 
2020-11-12 01:53:33.274284 / Iteration 5000 valid loss -0.06513276696205139 
2020-11-12 01:53:40.649671 / Iteration 5100: 0.24056881666183472 -0.35132890939712524 
2020-11-12 01:53:40.659118 / Iteration 5100 valid loss -0.06508809328079224 
2020-11-12 01:53:48.100136 / Iteration 5200: 0.2405688762664795 -0.35217440128326416 
2020-11-12 01:53:48.108041 / Iteration 5200 valid loss -0.06601572036743164 
2020-11-12 01:53:55.479946 / Iteration 5300: 0.2405688315629959 -0.35273441672325134 
2020-11-12 01:53:55.487991 / Iteration 5300 valid loss -0.06517601013183594 
2020-11-12 01:54:03.011937 / Iteration 5400: 0.24056890606880188 -0.3529788851737976 
2020-11-12 01:54:03.020829 / Iteration 5400 valid loss -0.0658525824546814 
2020-11-12 01:54:10.392115 / Iteration 5500: 0.2405688613653183 -0.35325801372528076 
2020-11-12 01:54:10.399696 / Iteration 5500 valid loss -0.06727761030197144 
2020-11-12 01:54:17.896336 / Iteration 5600: 0.24056890606880188 -0.35362499952316284 
2020-11-12 01:54:17.905014 / Iteration 5600 valid loss -0.06885778903961182 
2020-11-12 01:54:25.355976 / Iteration 5700: 0.2405688762664795 -0.35406675934791565 
2020-11-12 01:54:25.364985 / Iteration 5700 valid loss -0.07071980834007263 
2020-11-12 01:54:25.366534 / saving best-val-loss model 
2020-11-12 02:00:23.589859 / Namespace(activation='ReLU', atoms=[0.0], batch_size=25000, comet=True, data='lalonde', dataroot='/home/mila/r/raghupas/causal-benchmark/datasets', dim_h=4, dist='SigmoidFlow', dist_args=['ndim=2', 'base_distribution=uniform'], early_stop=True, eval=True, grad_norm=inf, ignore_w=False, lr=0.01, n_hidden_layers=1, num_epochs=10000, num_univariate_tests=100, overwrite_reload='', patience=None, saveroot='save', seed=123, test_prop=0.4, test_size=None, train=True, train_prop=0.5, val_prop=0.1, w_transform='Normalize', y_transform='Normalize') 
2020-11-12 02:00:23.592431 / getting data: lalonde 
2020-11-12 02:00:44.567838 / comet url: https://www.comet.ml/sunandr/causal-benchmark/8b45ee0f545d459fb5deef319a6bf5bd 
2020-11-12 02:00:44.571155 / ate: None 
2020-11-12 02:00:44.573008 / <models.distributions.distributions.MixedDistribution object at 0x7f93a3884390> 
2020-11-12 02:00:44.574920 / {'n_hidden_layers': 1, 'dim_h': 4, 'activation': ReLU()} 
2020-11-12 02:00:44.576819 / {'batch_size': 25000, 'lr': 0.01, 'num_epochs': 10000, 'verbose': True, 'print_every_iters': 100, 'optim': <class 'torch.optim.adam.Adam'>, 'eval_every': 100, 'plot_every': 100, 'p_every': 100, 'optim_args': {}} 
2020-11-12 02:00:46.965673 / Iteration 100: 0.21214598417282104 -0.24547943472862244 
2020-11-12 02:00:46.973073 / Iteration 100 valid loss 0.031138896942138672 
2020-11-12 02:00:46.974559 / saving best-val-loss model 
2020-11-12 02:00:53.474902 / Iteration 200: 0.1593914031982422 -0.37020111083984375 
2020-11-12 02:00:53.483254 / Iteration 200 valid loss -0.14869700372219086 
2020-11-12 02:00:53.484552 / saving best-val-loss model 
2020-11-12 02:01:00.042791 / Iteration 300: 0.14040641486644745 -0.6114034652709961 
2020-11-12 02:01:00.049480 / Iteration 300 valid loss -0.3839937150478363 
2020-11-12 02:01:00.051022 / saving best-val-loss model 
2020-11-12 02:01:06.771522 / Iteration 400: 0.1420125961303711 -0.663002073764801 
2020-11-12 02:01:06.778972 / Iteration 400 valid loss -0.44898903369903564 
2020-11-12 02:01:06.780518 / saving best-val-loss model 
2020-11-12 02:01:13.480428 / Iteration 500: 0.14215920865535736 -0.6895809173583984 
2020-11-12 02:01:13.486627 / Iteration 500 valid loss -0.4851379990577698 
2020-11-12 02:01:13.488070 / saving best-val-loss model 
2020-11-12 02:01:20.329176 / Iteration 600: 0.14242959022521973 -0.7053882479667664 
2020-11-12 02:01:20.338600 / Iteration 600 valid loss -0.508552074432373 
2020-11-12 02:01:20.340058 / saving best-val-loss model 
2020-11-12 02:01:27.144615 / Iteration 700: 0.14223381876945496 -0.7201803922653198 
2020-11-12 02:01:27.151397 / Iteration 700 valid loss -0.5221400260925293 
2020-11-12 02:01:27.152918 / saving best-val-loss model 
2020-11-12 02:01:33.838502 / Iteration 800: 0.14197483658790588 -0.7290751934051514 
2020-11-12 02:01:33.845408 / Iteration 800 valid loss -0.5260872840881348 
2020-11-12 02:01:33.846727 / saving best-val-loss model 
2020-11-12 02:01:40.567749 / Iteration 900: 0.1417095810174942 -0.7350112795829773 
2020-11-12 02:01:40.574952 / Iteration 900 valid loss -0.5389328002929688 
2020-11-12 02:01:40.576216 / saving best-val-loss model 
2020-11-12 02:01:47.205049 / Iteration 1000: 0.14123061299324036 -0.7397300004959106 
2020-11-12 02:01:47.212571 / Iteration 1000 valid loss -0.5417658686637878 
2020-11-12 02:01:47.214154 / saving best-val-loss model 
2020-11-12 02:01:54.051136 / Iteration 1100: 0.14114555716514587 -0.7436162829399109 
2020-11-12 02:01:54.057764 / Iteration 1100 valid loss -0.5456726551055908 
2020-11-12 02:01:54.059237 / saving best-val-loss model 
2020-11-12 02:02:00.672277 / Iteration 1200: 0.14103779196739197 -0.7469072937965393 
2020-11-12 02:02:00.678735 / Iteration 1200 valid loss -0.5484230518341064 
2020-11-12 02:02:00.680089 / saving best-val-loss model 
2020-11-12 02:02:07.531886 / Iteration 1300: 0.14097252488136292 -0.7496273517608643 
2020-11-12 02:02:07.538356 / Iteration 1300 valid loss -0.5513681173324585 
2020-11-12 02:02:07.539764 / saving best-val-loss model 
2020-11-12 02:02:14.312838 / Iteration 1400: 0.14091487228870392 -0.7523443102836609 
2020-11-12 02:02:14.319808 / Iteration 1400 valid loss -0.5522858500480652 
2020-11-12 02:02:14.321523 / saving best-val-loss model 
2020-11-12 02:02:21.100886 / Iteration 1500: 0.14088374376296997 -0.7546818256378174 
2020-11-12 02:02:21.109423 / Iteration 1500 valid loss -0.554113507270813 
2020-11-12 02:02:21.111146 / saving best-val-loss model 
2020-11-12 02:02:27.730222 / Iteration 1600: 0.14083096385002136 -0.7568109631538391 
2020-11-12 02:02:27.737275 / Iteration 1600 valid loss -0.5553895831108093 
2020-11-12 02:02:27.738984 / saving best-val-loss model 
2020-11-12 02:02:34.537164 / Iteration 1700: 0.14080104231834412 -0.7587233185768127 
2020-11-12 02:02:34.543903 / Iteration 1700 valid loss -0.5546818375587463 
2020-11-12 02:02:41.426074 / Iteration 1800: 0.140981525182724 -0.7604048252105713 
2020-11-12 02:02:41.432631 / Iteration 1800 valid loss -0.5552094578742981 
2020-11-12 02:02:48.325594 / Iteration 1900: 0.140809565782547 -0.7623312473297119 
2020-11-12 02:02:48.332182 / Iteration 1900 valid loss -0.5543376803398132 
2020-11-12 02:02:55.483756 / Iteration 2000: 0.14081580936908722 -0.765618085861206 
2020-11-12 02:02:55.490427 / Iteration 2000 valid loss -0.5574741363525391 
2020-11-12 02:02:55.492115 / saving best-val-loss model 
2020-11-12 02:03:02.189486 / Iteration 2100: 0.1408301442861557 -0.767383337020874 
2020-11-12 02:03:02.196470 / Iteration 2100 valid loss -0.5556979179382324 
2020-11-12 02:03:08.909658 / Iteration 2200: 0.14088159799575806 -0.7687749266624451 
2020-11-12 02:03:08.916253 / Iteration 2200 valid loss -0.5554313659667969 
2020-11-12 02:03:15.670346 / Iteration 2300: 0.1409304141998291 -0.770341157913208 
2020-11-12 02:03:15.677188 / Iteration 2300 valid loss -0.5543860197067261 
2020-11-12 02:03:22.511999 / Iteration 2400: 0.1409461945295334 -0.7717654705047607 
2020-11-12 02:03:22.518406 / Iteration 2400 valid loss -0.555299699306488 
2020-11-12 02:03:29.122005 / Iteration 2500: 0.14096227288246155 -0.7727068066596985 
2020-11-12 02:03:29.128485 / Iteration 2500 valid loss -0.5511634349822998 
2020-11-12 02:03:35.909407 / Iteration 2600: 0.14099235832691193 -0.7741580605506897 
2020-11-12 02:03:35.916364 / Iteration 2600 valid loss -0.5541039109230042 
2020-11-12 02:03:42.725522 / Iteration 2700: 0.14104938507080078 -0.7753261923789978 
2020-11-12 02:03:42.733309 / Iteration 2700 valid loss -0.5515936613082886 
2020-11-12 02:03:49.411307 / Iteration 2800: 0.1410834789276123 -0.7763752937316895 
2020-11-12 02:03:49.418069 / Iteration 2800 valid loss -0.5501818060874939 
2020-11-12 02:03:56.084606 / Iteration 2900: 0.14113086462020874 -0.7773913741111755 
2020-11-12 02:03:56.091484 / Iteration 2900 valid loss -0.5475683808326721 
2020-11-12 02:04:02.870277 / Iteration 3000: 0.14117436110973358 -0.7782827019691467 
2020-11-12 02:04:02.876670 / Iteration 3000 valid loss -0.5458963513374329 
