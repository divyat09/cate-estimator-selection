2020-11-12 01:47:02.182349 / Namespace(activation='ReLU', atoms=[0.0], batch_size=25000, comet=True, data='lalonde', dataroot='/home/mila/r/raghupas/causal-benchmark/datasets', dim_h=4, dist='SigmoidFlow', dist_args=['ndim=64', 'base_distribution=uniform'], early_stop=True, eval=True, grad_norm=inf, ignore_w=False, lr=0.01, n_hidden_layers=1, num_epochs=10000, num_univariate_tests=100, overwrite_reload='', patience=None, saveroot='save', seed=123, test_prop=0.4, test_size=None, train=True, train_prop=0.5, val_prop=0.1, w_transform='Normalize', y_transform='Normalize') 
2020-11-12 01:47:02.184852 / getting data: lalonde 
2020-11-12 01:47:24.918989 / comet url: https://www.comet.ml/sunandr/causal-benchmark/9684262eadd5427592d6f64bf752c987 
2020-11-12 01:47:24.920754 / ate: None 
2020-11-12 01:47:24.922415 / <models.distributions.distributions.MixedDistribution object at 0x7fe3cd55cb50> 
2020-11-12 01:47:24.924303 / {'n_hidden_layers': 1, 'dim_h': 4, 'activation': ReLU()} 
2020-11-12 01:47:24.926180 / {'batch_size': 25000, 'lr': 0.01, 'num_epochs': 10000, 'verbose': True, 'print_every_iters': 100, 'optim': <class 'torch.optim.adam.Adam'>, 'eval_every': 100, 'plot_every': 100, 'p_every': 100, 'optim_args': {}} 
2020-11-12 01:47:27.805694 / Iteration 100: 0.2641369700431824 -0.006096299737691879 
2020-11-12 01:47:27.815373 / Iteration 100 valid loss 0.26809659600257874 
2020-11-12 01:47:27.816767 / saving best-val-loss model 
2020-11-12 01:47:34.943163 / Iteration 200: 0.24058277904987335 -0.30659085512161255 
2020-11-12 01:47:34.951493 / Iteration 200 valid loss -0.04429924488067627 
2020-11-12 01:47:34.953010 / saving best-val-loss model 
2020-11-12 01:47:42.279669 / Iteration 300: 0.24056892096996307 -0.31790006160736084 
2020-11-12 01:47:42.288145 / Iteration 300 valid loss -0.05994302034378052 
2020-11-12 01:47:42.289852 / saving best-val-loss model 
2020-11-12 01:47:49.517997 / Iteration 400: 0.24056881666183472 -0.3205437660217285 
2020-11-12 01:47:49.527145 / Iteration 400 valid loss -0.0627129077911377 
2020-11-12 01:47:49.528852 / saving best-val-loss model 
2020-11-12 01:47:56.917867 / Iteration 500: 0.2405688315629959 -0.32217440009117126 
2020-11-12 01:47:56.926949 / Iteration 500 valid loss -0.06348264217376709 
2020-11-12 01:47:56.928633 / saving best-val-loss model 
2020-11-12 01:48:04.287887 / Iteration 600: 0.2405688315629959 -0.32410672307014465 
2020-11-12 01:48:04.296989 / Iteration 600 valid loss -0.06249624118208885 
2020-11-12 01:48:11.621727 / Iteration 700: 0.2405688315629959 -0.32593727111816406 
2020-11-12 01:48:11.630374 / Iteration 700 valid loss -0.06215432286262512 
2020-11-12 01:48:19.028779 / Iteration 800: 0.2405688315629959 -0.3292508125305176 
2020-11-12 01:48:19.038076 / Iteration 800 valid loss -0.06350940465927124 
2020-11-12 01:48:19.039809 / saving best-val-loss model 
2020-11-12 01:48:26.561385 / Iteration 900: 0.2405688613653183 -0.3327648341655731 
2020-11-12 01:48:26.569591 / Iteration 900 valid loss -0.06718289852142334 
2020-11-12 01:48:26.571196 / saving best-val-loss model 
2020-11-12 01:48:34.097240 / Iteration 1000: 0.2405688315629959 -0.3342893421649933 
2020-11-12 01:48:34.105213 / Iteration 1000 valid loss -0.06892621517181396 
2020-11-12 01:48:34.106375 / saving best-val-loss model 
2020-11-12 01:48:41.672194 / Iteration 1100: 0.2405688613653183 -0.3349481225013733 
2020-11-12 01:48:41.680524 / Iteration 1100 valid loss -0.06926891207695007 
2020-11-12 01:48:41.681917 / saving best-val-loss model 
2020-11-12 01:48:49.091383 / Iteration 1200: 0.2405688762664795 -0.33760958909988403 
2020-11-12 01:48:49.100556 / Iteration 1200 valid loss -0.06971925497055054 
2020-11-12 01:48:49.102110 / saving best-val-loss model 
2020-11-12 01:48:56.523760 / Iteration 1300: 0.2405688613653183 -0.3400939404964447 
2020-11-12 01:48:56.532506 / Iteration 1300 valid loss -0.06889227032661438 
2020-11-12 01:49:04.052016 / Iteration 1400: 0.2405688315629959 -0.3411712348461151 
2020-11-12 01:49:04.060629 / Iteration 1400 valid loss -0.06889459490776062 
2020-11-12 01:49:11.528820 / Iteration 1500: 0.2405688613653183 -0.3420872390270233 
2020-11-12 01:49:11.537388 / Iteration 1500 valid loss -0.0694015622138977 
2020-11-12 01:49:18.892080 / Iteration 1600: 0.24056890606880188 -0.3428969383239746 
2020-11-12 01:49:18.900935 / Iteration 1600 valid loss -0.07023334503173828 
2020-11-12 01:49:18.903506 / saving best-val-loss model 
2020-11-12 01:49:26.655986 / Iteration 1700: 0.2405688613653183 -0.3434025049209595 
2020-11-12 01:49:26.664768 / Iteration 1700 valid loss -0.07066568732261658 
2020-11-12 01:49:26.666281 / saving best-val-loss model 
2020-11-12 01:49:34.287690 / Iteration 1800: 0.2405688613653183 -0.343932181596756 
2020-11-12 01:49:34.295670 / Iteration 1800 valid loss -0.06958544254302979 
2020-11-12 01:49:41.744386 / Iteration 1900: 0.2405688315629959 -0.34430867433547974 
2020-11-12 01:49:41.752337 / Iteration 1900 valid loss -0.06938022375106812 
2020-11-12 01:49:49.165567 / Iteration 2000: 0.2405688315629959 -0.34459608793258667 
2020-11-12 01:49:49.173778 / Iteration 2000 valid loss -0.069163978099823 
2020-11-12 01:49:56.499915 / Iteration 2100: 0.2405688315629959 -0.3448152542114258 
2020-11-12 01:49:56.508241 / Iteration 2100 valid loss -0.06893375515937805 
2020-11-12 01:50:03.761263 / Iteration 2200: 0.2405688315629959 -0.34499892592430115 
2020-11-12 01:50:03.770654 / Iteration 2200 valid loss -0.06878188252449036 
2020-11-12 01:50:11.495725 / Iteration 2300: 0.2405688315629959 -0.3451509475708008 
2020-11-12 01:50:11.504376 / Iteration 2300 valid loss -0.06860312819480896 
2020-11-12 01:50:18.957128 / Iteration 2400: 0.2405688315629959 -0.3453041613101959 
2020-11-12 01:50:18.965235 / Iteration 2400 valid loss -0.0686216652393341 
2020-11-12 01:50:26.329625 / Iteration 2500: 0.2405688613653183 -0.3455873131752014 
2020-11-12 01:50:26.337333 / Iteration 2500 valid loss -0.06891709566116333 
2020-11-12 01:50:33.760538 / Iteration 2600: 0.2405688315629959 -0.3459676206111908 
2020-11-12 01:50:33.769109 / Iteration 2600 valid loss -0.06892746686935425 
2020-11-12 01:50:41.175476 / Iteration 2700: 0.2405688613653183 -0.3463667333126068 
2020-11-12 01:50:41.184258 / Iteration 2700 valid loss -0.06880462169647217 
2020-11-12 01:50:48.985259 / Iteration 2800: 0.2405688613653183 -0.34672170877456665 
2020-11-12 01:50:48.994231 / Iteration 2800 valid loss -0.06857481598854065 
2020-11-12 01:50:56.755674 / Iteration 2900: 0.2405688315629959 -0.34700584411621094 
2020-11-12 01:50:56.764575 / Iteration 2900 valid loss -0.06834721565246582 
2020-11-12 01:51:04.393967 / Iteration 3000: 0.2405688613653183 -0.34721383452415466 
2020-11-12 01:51:04.401771 / Iteration 3000 valid loss -0.06813046336174011 
2020-11-12 01:51:11.872492 / Iteration 3100: 0.2405688315629959 -0.34734711050987244 
2020-11-12 01:51:11.881119 / Iteration 3100 valid loss -0.06777864694595337 
2020-11-12 01:51:19.457567 / Iteration 3200: 0.24056881666183472 -0.3474564850330353 
2020-11-12 01:51:19.466119 / Iteration 3200 valid loss -0.0678015649318695 
2020-11-12 01:51:26.752189 / Iteration 3300: 0.24056878685951233 -0.3475417196750641 
2020-11-12 01:51:26.759966 / Iteration 3300 valid loss -0.06774002313613892 
2020-11-12 01:51:34.122659 / Iteration 3400: 0.24056881666183472 -0.3476000726222992 
2020-11-12 01:51:34.130309 / Iteration 3400 valid loss -0.06769302487373352 
2020-11-12 01:51:41.567980 / Iteration 3500: 0.2405688315629959 -0.34764808416366577 
2020-11-12 01:51:41.576719 / Iteration 3500 valid loss -0.0676824152469635 
2020-11-12 01:51:49.075184 / Iteration 3600: 0.2405688762664795 -0.3476581573486328 
2020-11-12 01:51:49.083960 / Iteration 3600 valid loss -0.06766310334205627 
2020-11-12 01:51:56.495341 / Iteration 3700: 0.2405688762664795 -0.34772610664367676 
2020-11-12 01:51:56.503832 / Iteration 3700 valid loss -0.06766626238822937 
2020-11-12 01:52:04.088872 / Iteration 3800: 0.24056890606880188 -0.3477637767791748 
2020-11-12 01:52:04.096713 / Iteration 3800 valid loss -0.06773144006729126 
2020-11-12 01:52:11.632437 / Iteration 3900: 0.24056890606880188 -0.34780076146125793 
2020-11-12 01:52:11.639886 / Iteration 3900 valid loss -0.06774282455444336 
2020-11-12 01:52:19.158667 / Iteration 4000: 0.24056890606880188 -0.34783878922462463 
2020-11-12 01:52:19.167358 / Iteration 4000 valid loss -0.06775188446044922 
2020-11-12 01:52:26.567722 / Iteration 4100: 0.2405688613653183 -0.3479323089122772 
2020-11-12 01:52:26.576032 / Iteration 4100 valid loss -0.06781831383705139 
2020-11-12 01:52:33.991686 / Iteration 4200: 0.2405688762664795 -0.34823596477508545 
2020-11-12 01:52:33.999427 / Iteration 4200 valid loss -0.06739276647567749 
2020-11-12 01:52:41.389712 / Iteration 4300: 0.2405688762664795 -0.348438560962677 
2020-11-12 01:52:41.398360 / Iteration 4300 valid loss -0.0673866868019104 
2020-11-12 01:52:48.834038 / Iteration 4400: 0.2405688613653183 -0.34886306524276733 
2020-11-12 01:52:48.843182 / Iteration 4400 valid loss -0.06785345077514648 
2020-11-12 01:52:56.290508 / Iteration 4500: 0.2405688613653183 -0.35018301010131836 
2020-11-12 01:52:56.298858 / Iteration 4500 valid loss -0.06739440560340881 
2020-11-12 01:53:03.844002 / Iteration 4600: 0.24056890606880188 -0.3508189618587494 
2020-11-12 01:53:03.852583 / Iteration 4600 valid loss -0.06654107570648193 
2020-11-12 01:53:11.259195 / Iteration 4700: 0.2405688613653183 -0.3510447144508362 
2020-11-12 01:53:11.267889 / Iteration 4700 valid loss -0.0657360851764679 
2020-11-12 01:53:18.608087 / Iteration 4800: 0.2405688315629959 -0.35113775730133057 
2020-11-12 01:53:18.616037 / Iteration 4800 valid loss -0.0653870701789856 
2020-11-12 01:53:25.941033 / Iteration 4900: 0.2405688762664795 -0.35120341181755066 
2020-11-12 01:53:25.950049 / Iteration 4900 valid loss -0.06521493196487427 
2020-11-12 01:53:33.266402 / Iteration 5000: 0.2405688762664795 -0.3512652516365051 
2020-11-12 01:53:33.274284 / Iteration 5000 valid loss -0.06513276696205139 
2020-11-12 01:53:40.649671 / Iteration 5100: 0.24056881666183472 -0.35132890939712524 
2020-11-12 01:53:40.659118 / Iteration 5100 valid loss -0.06508809328079224 
2020-11-12 01:53:48.100136 / Iteration 5200: 0.2405688762664795 -0.35217440128326416 
2020-11-12 01:53:48.108041 / Iteration 5200 valid loss -0.06601572036743164 
2020-11-12 01:53:55.479946 / Iteration 5300: 0.2405688315629959 -0.35273441672325134 
2020-11-12 01:53:55.487991 / Iteration 5300 valid loss -0.06517601013183594 
2020-11-12 01:54:03.011937 / Iteration 5400: 0.24056890606880188 -0.3529788851737976 
2020-11-12 01:54:03.020829 / Iteration 5400 valid loss -0.0658525824546814 
2020-11-12 01:54:10.392115 / Iteration 5500: 0.2405688613653183 -0.35325801372528076 
2020-11-12 01:54:10.399696 / Iteration 5500 valid loss -0.06727761030197144 
2020-11-12 01:54:17.896336 / Iteration 5600: 0.24056890606880188 -0.35362499952316284 
2020-11-12 01:54:17.905014 / Iteration 5600 valid loss -0.06885778903961182 
2020-11-12 01:54:25.355976 / Iteration 5700: 0.2405688762664795 -0.35406675934791565 
2020-11-12 01:54:25.364985 / Iteration 5700 valid loss -0.07071980834007263 
2020-11-12 01:54:25.366534 / saving best-val-loss model 
2020-11-12 02:00:23.589859 / Namespace(activation='ReLU', atoms=[0.0], batch_size=25000, comet=True, data='lalonde', dataroot='/home/mila/r/raghupas/causal-benchmark/datasets', dim_h=4, dist='SigmoidFlow', dist_args=['ndim=2', 'base_distribution=uniform'], early_stop=True, eval=True, grad_norm=inf, ignore_w=False, lr=0.01, n_hidden_layers=1, num_epochs=10000, num_univariate_tests=100, overwrite_reload='', patience=None, saveroot='save', seed=123, test_prop=0.4, test_size=None, train=True, train_prop=0.5, val_prop=0.1, w_transform='Normalize', y_transform='Normalize') 
2020-11-12 02:00:23.592431 / getting data: lalonde 
2020-11-12 02:00:44.567838 / comet url: https://www.comet.ml/sunandr/causal-benchmark/8b45ee0f545d459fb5deef319a6bf5bd 
2020-11-12 02:00:44.571155 / ate: None 
2020-11-12 02:00:44.573008 / <models.distributions.distributions.MixedDistribution object at 0x7f93a3884390> 
2020-11-12 02:00:44.574920 / {'n_hidden_layers': 1, 'dim_h': 4, 'activation': ReLU()} 
2020-11-12 02:00:44.576819 / {'batch_size': 25000, 'lr': 0.01, 'num_epochs': 10000, 'verbose': True, 'print_every_iters': 100, 'optim': <class 'torch.optim.adam.Adam'>, 'eval_every': 100, 'plot_every': 100, 'p_every': 100, 'optim_args': {}} 
2020-11-12 02:00:46.965673 / Iteration 100: 0.21214598417282104 -0.24547943472862244 
2020-11-12 02:00:46.973073 / Iteration 100 valid loss 0.031138896942138672 
2020-11-12 02:00:46.974559 / saving best-val-loss model 
2020-11-12 02:00:53.474902 / Iteration 200: 0.1593914031982422 -0.37020111083984375 
2020-11-12 02:00:53.483254 / Iteration 200 valid loss -0.14869700372219086 
2020-11-12 02:00:53.484552 / saving best-val-loss model 
2020-11-12 02:01:00.042791 / Iteration 300: 0.14040641486644745 -0.6114034652709961 
2020-11-12 02:01:00.049480 / Iteration 300 valid loss -0.3839937150478363 
2020-11-12 02:01:00.051022 / saving best-val-loss model 
2020-11-12 02:01:06.771522 / Iteration 400: 0.1420125961303711 -0.663002073764801 
2020-11-12 02:01:06.778972 / Iteration 400 valid loss -0.44898903369903564 
2020-11-12 02:01:06.780518 / saving best-val-loss model 
2020-11-12 02:01:13.480428 / Iteration 500: 0.14215920865535736 -0.6895809173583984 
2020-11-12 02:01:13.486627 / Iteration 500 valid loss -0.4851379990577698 
2020-11-12 02:01:13.488070 / saving best-val-loss model 
2020-11-12 02:01:20.329176 / Iteration 600: 0.14242959022521973 -0.7053882479667664 
2020-11-12 02:01:20.338600 / Iteration 600 valid loss -0.508552074432373 
2020-11-12 02:01:20.340058 / saving best-val-loss model 
2020-11-12 02:01:27.144615 / Iteration 700: 0.14223381876945496 -0.7201803922653198 
2020-11-12 02:01:27.151397 / Iteration 700 valid loss -0.5221400260925293 
2020-11-12 02:01:27.152918 / saving best-val-loss model 
2020-11-12 02:01:33.838502 / Iteration 800: 0.14197483658790588 -0.7290751934051514 
2020-11-12 02:01:33.845408 / Iteration 800 valid loss -0.5260872840881348 
2020-11-12 02:01:33.846727 / saving best-val-loss model 
2020-11-12 02:01:40.567749 / Iteration 900: 0.1417095810174942 -0.7350112795829773 
2020-11-12 02:01:40.574952 / Iteration 900 valid loss -0.5389328002929688 
2020-11-12 02:01:40.576216 / saving best-val-loss model 
2020-11-12 02:01:47.205049 / Iteration 1000: 0.14123061299324036 -0.7397300004959106 
2020-11-12 02:01:47.212571 / Iteration 1000 valid loss -0.5417658686637878 
2020-11-12 02:01:47.214154 / saving best-val-loss model 
2020-11-12 02:01:54.051136 / Iteration 1100: 0.14114555716514587 -0.7436162829399109 
2020-11-12 02:01:54.057764 / Iteration 1100 valid loss -0.5456726551055908 
2020-11-12 02:01:54.059237 / saving best-val-loss model 
2020-11-12 02:02:00.672277 / Iteration 1200: 0.14103779196739197 -0.7469072937965393 
2020-11-12 02:02:00.678735 / Iteration 1200 valid loss -0.5484230518341064 
2020-11-12 02:02:00.680089 / saving best-val-loss model 
2020-11-12 02:02:07.531886 / Iteration 1300: 0.14097252488136292 -0.7496273517608643 
2020-11-12 02:02:07.538356 / Iteration 1300 valid loss -0.5513681173324585 
2020-11-12 02:02:07.539764 / saving best-val-loss model 
2020-11-12 02:02:14.312838 / Iteration 1400: 0.14091487228870392 -0.7523443102836609 
2020-11-12 02:02:14.319808 / Iteration 1400 valid loss -0.5522858500480652 
2020-11-12 02:02:14.321523 / saving best-val-loss model 
2020-11-12 02:02:21.100886 / Iteration 1500: 0.14088374376296997 -0.7546818256378174 
2020-11-12 02:02:21.109423 / Iteration 1500 valid loss -0.554113507270813 
2020-11-12 02:02:21.111146 / saving best-val-loss model 
2020-11-12 02:02:27.730222 / Iteration 1600: 0.14083096385002136 -0.7568109631538391 
2020-11-12 02:02:27.737275 / Iteration 1600 valid loss -0.5553895831108093 
2020-11-12 02:02:27.738984 / saving best-val-loss model 
2020-11-12 02:02:34.537164 / Iteration 1700: 0.14080104231834412 -0.7587233185768127 
2020-11-12 02:02:34.543903 / Iteration 1700 valid loss -0.5546818375587463 
2020-11-12 02:02:41.426074 / Iteration 1800: 0.140981525182724 -0.7604048252105713 
2020-11-12 02:02:41.432631 / Iteration 1800 valid loss -0.5552094578742981 
2020-11-12 02:02:48.325594 / Iteration 1900: 0.140809565782547 -0.7623312473297119 
2020-11-12 02:02:48.332182 / Iteration 1900 valid loss -0.5543376803398132 
2020-11-12 02:02:55.483756 / Iteration 2000: 0.14081580936908722 -0.765618085861206 
2020-11-12 02:02:55.490427 / Iteration 2000 valid loss -0.5574741363525391 
2020-11-12 02:02:55.492115 / saving best-val-loss model 
2020-11-12 02:03:02.189486 / Iteration 2100: 0.1408301442861557 -0.767383337020874 
2020-11-12 02:03:02.196470 / Iteration 2100 valid loss -0.5556979179382324 
2020-11-12 02:03:08.909658 / Iteration 2200: 0.14088159799575806 -0.7687749266624451 
2020-11-12 02:03:08.916253 / Iteration 2200 valid loss -0.5554313659667969 
2020-11-12 02:03:15.670346 / Iteration 2300: 0.1409304141998291 -0.770341157913208 
2020-11-12 02:03:15.677188 / Iteration 2300 valid loss -0.5543860197067261 
2020-11-12 02:03:22.511999 / Iteration 2400: 0.1409461945295334 -0.7717654705047607 
2020-11-12 02:03:22.518406 / Iteration 2400 valid loss -0.555299699306488 
2020-11-12 02:03:29.122005 / Iteration 2500: 0.14096227288246155 -0.7727068066596985 
2020-11-12 02:03:29.128485 / Iteration 2500 valid loss -0.5511634349822998 
2020-11-12 02:03:35.909407 / Iteration 2600: 0.14099235832691193 -0.7741580605506897 
2020-11-12 02:03:35.916364 / Iteration 2600 valid loss -0.5541039109230042 
2020-11-12 02:03:42.725522 / Iteration 2700: 0.14104938507080078 -0.7753261923789978 
2020-11-12 02:03:42.733309 / Iteration 2700 valid loss -0.5515936613082886 
2020-11-12 02:03:49.411307 / Iteration 2800: 0.1410834789276123 -0.7763752937316895 
2020-11-12 02:03:49.418069 / Iteration 2800 valid loss -0.5501818060874939 
2020-11-12 02:03:56.084606 / Iteration 2900: 0.14113086462020874 -0.7773913741111755 
2020-11-12 02:03:56.091484 / Iteration 2900 valid loss -0.5475683808326721 
2020-11-12 02:04:02.870277 / Iteration 3000: 0.14117436110973358 -0.7782827019691467 
2020-11-12 02:04:02.876670 / Iteration 3000 valid loss -0.5458963513374329 
2020-11-14 15:10:39.215982 / Namespace(activation='ReLU', atoms=[], batch_size=8192, comet=True, data='lbidd_log_50k', dataroot='/home/mila/r/raghupas/causal-benchmark/datasets', dim_h=8, dist='LogNormal', dist_args=[], early_stop=True, eval=True, grad_norm=inf, ignore_w=False, lr=0.001, n_hidden_layers=1, num_epochs=10000, num_univariate_tests=100, overwrite_reload='', patience=None, saveroot='save', seed=123, test_prop=0.4, test_size=None, train=True, train_prop=0.5, val_prop=0.1, w_transform='Standardize', y_transform='Standardize') 
2020-11-14 15:10:39.219094 / getting data: lbidd_log_50k 
2020-11-14 15:11:00.686783 / comet url: https://www.comet.ml/sunandr/causal-benchmark/105ab65716f9409a94ce52802dceea14 
2020-11-14 15:11:00.688277 / ate: 0.054972401602067546 
2020-11-14 15:11:00.689603 / <models.distributions.distributions.LogNormal object at 0x7efdf1d5b990> 
2020-11-14 15:11:00.691443 / {'n_hidden_layers': 1, 'dim_h': 8, 'activation': ReLU()} 
2020-11-14 15:11:00.693414 / {'batch_size': 8192, 'lr': 0.001, 'num_epochs': 10000, 'verbose': True, 'print_every_iters': 100, 'optim': <class 'torch.optim.adam.Adam'>, 'eval_every': 100, 'plot_every': 100, 'p_every': 100, 'optim_args': {}} 
2020-11-14 15:11:13.203861 / Iteration 100: nan nan 
2020-11-14 15:11:13.312247 / Iteration 100 valid loss nan 
2020-11-14 15:12:41.576851 / Namespace(activation='ReLU', atoms=[], batch_size=8192, comet=True, data='lbidd_log_50k', dataroot='/home/mila/r/raghupas/causal-benchmark/datasets', dim_h=8, dist='LogNormal', dist_args=[], early_stop=True, eval=True, grad_norm=inf, ignore_w=False, lr=0.001, n_hidden_layers=1, num_epochs=10000, num_univariate_tests=100, overwrite_reload='', patience=None, saveroot='save', seed=123, test_prop=0.4, test_size=None, train=True, train_prop=0.5, val_prop=0.1, w_transform='Standardize', y_transform='Standardize') 
2020-11-14 15:12:41.579426 / getting data: lbidd_log_50k 
2020-11-14 15:13:03.076995 / comet url: https://www.comet.ml/sunandr/causal-benchmark/ffc8094e0c4b4cedbadec5a94634c9a4 
2020-11-14 15:13:03.078473 / ate: 0.054972401602067546 
2020-11-14 15:13:03.079807 / <models.distributions.distributions.LogNormal object at 0x7f6c0c594e90> 
2020-11-14 15:13:03.081322 / {'n_hidden_layers': 1, 'dim_h': 8, 'activation': ReLU()} 
2020-11-14 15:13:03.082855 / {'batch_size': 8192, 'lr': 0.001, 'num_epochs': 10000, 'verbose': True, 'print_every_iters': 100, 'optim': <class 'torch.optim.adam.Adam'>, 'eval_every': 100, 'plot_every': 1000, 'p_every': 100, 'optim_args': {}} 
2020-11-14 15:13:16.368290 / Iteration 100: nan nan 
2020-11-14 15:13:16.447295 / Iteration 100 valid loss nan 
2020-11-14 15:14:30.801658 / Namespace(activation='ReLU', atoms=[], batch_size=25000, comet=True, data='lbidd_log_50k', dataroot='/home/mila/r/raghupas/causal-benchmark/datasets', dim_h=4, dist='SigmoidFlow', dist_args=['ndim=8', 'base_distribution=normal'], early_stop=True, eval=True, grad_norm=inf, ignore_w=False, lr=0.001, n_hidden_layers=1, num_epochs=10000, num_univariate_tests=100, overwrite_reload='', patience=None, saveroot='save', seed=123, test_prop=0.4, test_size=None, train=True, train_prop=0.5, val_prop=0.1, w_transform='Standardize', y_transform='Standardize') 
2020-11-14 15:14:30.804429 / getting data: lbidd_log_50k 
2020-11-14 15:14:53.536231 / comet url: https://www.comet.ml/sunandr/causal-benchmark/9fa27de74b1842c8b857b4cca833eefc 
2020-11-14 15:14:53.538111 / ate: 0.054972401602067546 
2020-11-14 15:14:53.539942 / <models.distributions.distributions.SigmoidFlow object at 0x7fa593a96a10> ndim:8 base_distribution:normal 
2020-11-14 15:14:53.541896 / {'n_hidden_layers': 1, 'dim_h': 4, 'activation': ReLU()} 
2020-11-14 15:14:53.543711 / {'batch_size': 25000, 'lr': 0.001, 'num_epochs': 10000, 'verbose': True, 'print_every_iters': 100, 'optim': <class 'torch.optim.adam.Adam'>, 'eval_every': 100, 'plot_every': 100, 'p_every': 100, 'optim_args': {}} 
2020-11-14 15:15:47.233835 / Iteration 100: 0.6880795359611511 1.3990732431411743 
2020-11-14 15:15:47.309156 / Iteration 100 valid loss 2.0957210063934326 
2020-11-14 15:15:47.310821 / saving best-val-loss model 
2020-11-14 15:16:47.140181 / Iteration 200: 0.6807884573936462 1.2338511943817139 
2020-11-14 15:16:47.216538 / Iteration 200 valid loss 1.9346822500228882 
2020-11-14 15:16:47.217948 / saving best-val-loss model 
2020-11-14 15:17:51.458611 / Iteration 300: 0.6681150197982788 1.0236101150512695 
2020-11-14 15:17:51.528653 / Iteration 300 valid loss 1.7160701751708984 
2020-11-14 15:17:51.529900 / saving best-val-loss model 
2020-11-14 15:18:52.126658 / Iteration 400: 0.6650285720825195 0.7060832977294922 
2020-11-14 15:18:52.237886 / Iteration 400 valid loss 1.3931047916412354 
2020-11-14 15:18:52.239179 / saving best-val-loss model 
2020-11-14 15:19:54.397866 / Iteration 500: 0.6602915525436401 0.4295165538787842 
2020-11-14 15:19:54.490403 / Iteration 500 valid loss 1.119264841079712 
2020-11-14 15:19:54.491873 / saving best-val-loss model 
2020-11-14 15:20:57.890134 / Iteration 600: 0.657677173614502 0.22315296530723572 
2020-11-14 15:20:57.999821 / Iteration 600 valid loss 0.9113163352012634 
2020-11-14 15:20:58.001374 / saving best-val-loss model 
2020-11-14 15:22:02.375028 / Iteration 700: 0.656191885471344 0.04692131280899048 
2020-11-14 15:22:02.493746 / Iteration 700 valid loss 0.7378658056259155 
2020-11-14 15:22:02.495094 / saving best-val-loss model 
2020-11-14 15:23:08.786099 / Iteration 800: 0.6547956466674805 -0.15374387800693512 
2020-11-14 15:23:08.866070 / Iteration 800 valid loss 0.551584780216217 
2020-11-14 15:23:08.867239 / saving best-val-loss model 
2020-11-14 15:24:12.922025 / Iteration 900: 0.6542791724205017 -0.30614638328552246 
2020-11-14 15:24:12.993294 / Iteration 900 valid loss 0.41426748037338257 
2020-11-14 15:24:12.994472 / saving best-val-loss model 
2020-11-14 15:25:18.467092 / Iteration 1000: 0.65416419506073 -0.41067004203796387 
2020-11-14 15:25:18.574167 / Iteration 1000 valid loss 0.31905218958854675 
2020-11-14 15:25:18.575699 / saving best-val-loss model 
2020-11-14 15:26:21.506212 / Iteration 1100: 0.6541557908058167 -0.48200613260269165 
2020-11-14 15:26:21.617247 / Iteration 1100 valid loss 0.2567119598388672 
2020-11-14 15:26:21.618955 / saving best-val-loss model 
2020-11-14 15:27:25.629362 / Iteration 1200: 0.6541206240653992 -0.5279372334480286 
2020-11-14 15:27:25.747928 / Iteration 1200 valid loss 0.20931376516819 
2020-11-14 15:27:25.749589 / saving best-val-loss model 
2020-11-14 15:28:29.810832 / Iteration 1300: 0.6541434526443481 -0.5644550919532776 
2020-11-14 15:28:29.886996 / Iteration 1300 valid loss 0.17156898975372314 
2020-11-14 15:28:29.888572 / saving best-val-loss model 
2020-11-14 15:29:32.970086 / Iteration 1400: 0.6541122794151306 -0.6038742661476135 
2020-11-14 15:29:33.043929 / Iteration 1400 valid loss 0.13377058506011963 
2020-11-14 15:29:33.045197 / saving best-val-loss model 
2020-11-14 15:30:39.252246 / Iteration 1500: 0.6541144251823425 -0.6267601847648621 
2020-11-14 15:30:39.370328 / Iteration 1500 valid loss 0.11531294137239456 
2020-11-14 15:30:39.371798 / saving best-val-loss model 
2020-11-14 15:31:45.967021 / Iteration 1600: 0.6541014313697815 -0.6444047689437866 
2020-11-14 15:31:46.055521 / Iteration 1600 valid loss 0.10062527656555176 
2020-11-14 15:31:46.057247 / saving best-val-loss model 
2020-11-14 15:32:50.643278 / Iteration 1700: 0.654086709022522 -0.6593258380889893 
2020-11-14 15:32:50.766671 / Iteration 1700 valid loss 0.0874820351600647 
2020-11-14 15:32:50.768225 / saving best-val-loss model 
2020-11-14 15:33:57.046126 / Iteration 1800: 0.6540884971618652 -0.6722884178161621 
2020-11-14 15:33:57.153747 / Iteration 1800 valid loss 0.07756829261779785 
2020-11-14 15:33:57.155478 / saving best-val-loss model 
2020-11-14 15:35:04.280178 / Iteration 1900: 0.6540687084197998 -0.6875910758972168 
2020-11-14 15:35:04.359325 / Iteration 1900 valid loss 0.06580168008804321 
2020-11-14 15:35:04.360886 / saving best-val-loss model 
2020-11-14 15:36:10.901105 / Iteration 2000: 0.6540402770042419 -0.7036415338516235 
2020-11-14 15:36:10.975800 / Iteration 2000 valid loss 0.051509320735931396 
2020-11-14 15:36:10.977071 / saving best-val-loss model 
2020-11-14 15:37:16.857670 / Iteration 2100: 0.6540184617042542 -0.7210811972618103 
2020-11-14 15:37:16.986550 / Iteration 2100 valid loss 0.036132216453552246 
2020-11-14 15:37:16.987747 / saving best-val-loss model 
2020-11-14 15:38:22.829216 / Iteration 2200: 0.6540240049362183 -0.738368809223175 
2020-11-14 15:38:22.911992 / Iteration 2200 valid loss 0.020527780055999756 
2020-11-14 15:38:22.913643 / saving best-val-loss model 
2020-11-14 15:39:24.198874 / Iteration 2300: 0.653982400894165 -0.7637043595314026 
2020-11-14 15:39:24.313436 / Iteration 2300 valid loss -0.00699615478515625 
2020-11-14 15:39:24.314840 / saving best-val-loss model 
2020-11-14 15:40:29.280761 / Iteration 2400: 0.6539552211761475 -0.7884806990623474 
2020-11-14 15:40:29.404166 / Iteration 2400 valid loss -0.03277558088302612 
2020-11-14 15:40:29.405558 / saving best-val-loss model 
2020-11-14 15:41:36.545901 / Iteration 2500: 0.653938889503479 -0.8086128830909729 
2020-11-14 15:41:36.636054 / Iteration 2500 valid loss -0.052647173404693604 
2020-11-14 15:41:36.637719 / saving best-val-loss model 
2020-11-14 15:42:42.921561 / Iteration 2600: 0.6539263725280762 -0.8285902142524719 
2020-11-14 15:42:42.994188 / Iteration 2600 valid loss -0.06992757320404053 
2020-11-14 15:42:42.995478 / saving best-val-loss model 
2020-11-14 15:43:47.211236 / Iteration 2700: 0.6539080739021301 -0.8426215052604675 
2020-11-14 15:43:47.289780 / Iteration 2700 valid loss -0.08216303586959839 
2020-11-14 15:43:47.291682 / saving best-val-loss model 
2020-11-14 15:44:52.692860 / Iteration 2800: 0.6538889408111572 -0.8555469512939453 
2020-11-14 15:44:52.802755 / Iteration 2800 valid loss -0.0924978256225586 
2020-11-14 15:44:52.804217 / saving best-val-loss model 
2020-11-14 15:56:24.247536 / Namespace(activation='ReLU', atoms=[], batch_size=25000, comet=True, data='lbidd_log_50k', dataroot='/home/mila/r/raghupas/causal-benchmark/datasets', dim_h=4, dist='SigmoidFlow', dist_args=['ndim=8', 'base_distribution=normal'], early_stop=True, eval=True, grad_norm=inf, ignore_w=False, lr=0.001, n_hidden_layers=1, num_epochs=10000, num_univariate_tests=100, overwrite_reload='', patience=None, saveroot='save', seed=123, test_prop=0.4, test_size=None, train=True, train_prop=0.5, val_prop=0.1, w_transform='Standardize', y_transform='Normalize') 
2020-11-14 15:56:24.250069 / getting data: lbidd_log_50k 
2020-11-14 15:56:46.156664 / comet url: https://www.comet.ml/sunandr/causal-benchmark/a1117ea982264fcfbcf46240f8c29a7f 
2020-11-14 15:56:46.158225 / ate: 0.054972401602067546 
2020-11-14 15:56:46.159895 / <models.distributions.distributions.SigmoidFlow object at 0x7f7055669dd0> ndim:8 base_distribution:normal 
2020-11-14 15:56:46.161496 / {'n_hidden_layers': 1, 'dim_h': 4, 'activation': ReLU()} 
2020-11-14 15:56:46.163112 / {'batch_size': 25000, 'lr': 0.001, 'num_epochs': 10000, 'verbose': True, 'print_every_iters': 100, 'optim': <class 'torch.optim.adam.Adam'>, 'eval_every': 100, 'plot_every': 100, 'p_every': 100, 'optim_args': {}} 
2020-11-14 15:57:41.417139 / Iteration 100: 0.6865424513816833 0.9565437436103821 
2020-11-14 15:57:41.529150 / Iteration 100 valid loss 1.6206625699996948 
2020-11-14 15:57:41.530869 / saving best-val-loss model 
2020-11-14 15:58:43.661861 / Iteration 200: 0.6842060685157776 -0.2910490930080414 
2020-11-14 15:58:43.768636 / Iteration 200 valid loss 0.3893337845802307 
2020-11-14 15:58:43.770165 / saving best-val-loss model 
2020-11-14 15:59:48.187741 / Iteration 300: 0.6864205598831177 -0.6401333212852478 
2020-11-14 15:59:48.265260 / Iteration 300 valid loss 0.06452131271362305 
2020-11-14 15:59:48.266581 / saving best-val-loss model 
2020-11-14 16:00:49.028499 / Iteration 400: 0.6874001026153564 -0.7165974378585815 
2020-11-14 16:00:49.102805 / Iteration 400 valid loss -0.006505608558654785 
2020-11-14 16:00:49.104013 / saving best-val-loss model 
2020-11-14 16:01:50.973651 / Iteration 500: 0.6864555478096008 -0.749725341796875 
2020-11-14 16:01:51.080263 / Iteration 500 valid loss -0.037424683570861816 
2020-11-14 16:01:51.081401 / saving best-val-loss model 
2020-11-14 16:02:51.674951 / Iteration 600: 0.6816722750663757 -0.7739017009735107 
2020-11-14 16:02:51.748459 / Iteration 600 valid loss -0.0651618242263794 
2020-11-14 16:02:51.749763 / saving best-val-loss model 
2020-11-14 16:03:53.371516 / Iteration 700: 0.6738700866699219 -0.8381928205490112 
2020-11-14 16:03:53.448626 / Iteration 700 valid loss -0.13438791036605835 
2020-11-14 16:03:53.450146 / saving best-val-loss model 
2020-11-14 16:04:56.268321 / Iteration 800: 0.6650480628013611 -0.8659639954566956 
2020-11-14 16:04:56.345682 / Iteration 800 valid loss -0.1660008430480957 
2020-11-14 16:04:56.347097 / saving best-val-loss model 
2020-11-14 16:05:57.396287 / Iteration 900: 0.65950608253479 -0.8844906091690063 
2020-11-14 16:05:57.480649 / Iteration 900 valid loss -0.18676602840423584 
2020-11-14 16:05:57.481852 / saving best-val-loss model 
2020-11-14 16:07:00.373756 / Iteration 1000: 0.6568979620933533 -0.9068665504455566 
2020-11-14 16:07:00.492002 / Iteration 1000 valid loss -0.2107866406440735 
2020-11-14 16:07:00.493415 / saving best-val-loss model 
2020-11-14 16:08:02.393743 / Iteration 1100: 0.6555507779121399 -0.9485698342323303 
2020-11-14 16:08:02.471039 / Iteration 1100 valid loss -0.25562238693237305 
2020-11-14 16:08:02.472480 / saving best-val-loss model 
2020-11-14 16:09:03.638367 / Iteration 1200: 0.6550307869911194 -1.367793321609497 
2020-11-14 16:09:03.763723 / Iteration 1200 valid loss -0.6815341711044312 
2020-11-14 16:09:03.765004 / saving best-val-loss model 
2020-11-14 16:10:05.634589 / Iteration 1300: 0.656157910823822 -1.824810266494751 
2020-11-14 16:10:05.725237 / Iteration 1300 valid loss -1.1314706802368164 
2020-11-14 16:10:05.726424 / saving best-val-loss model 
2020-11-14 16:11:07.637790 / Iteration 1400: 0.6577275991439819 -1.9692786931991577 
2020-11-14 16:11:07.725184 / Iteration 1400 valid loss -1.2824273109436035 
2020-11-14 16:11:07.726457 / saving best-val-loss model 
2020-11-14 16:12:12.167777 / Iteration 1500: 0.6579746007919312 -2.0642409324645996 
2020-11-14 16:12:12.243278 / Iteration 1500 valid loss -1.3787108659744263 
2020-11-14 16:12:12.244757 / saving best-val-loss model 
2020-11-14 16:13:18.809494 / Iteration 1600: 0.6582684516906738 -2.2340810298919678 
2020-11-14 16:13:18.920616 / Iteration 1600 valid loss -1.5481696128845215 
2020-11-14 16:13:18.922545 / saving best-val-loss model 
2020-11-14 16:14:23.576798 / Iteration 1700: 0.6590375900268555 -2.3890442848205566 
2020-11-14 16:14:23.660332 / Iteration 1700 valid loss -1.694410800933838 
2020-11-14 16:14:23.661662 / saving best-val-loss model 
2020-11-14 16:15:28.690531 / Iteration 1800: 0.6614454984664917 -2.492446184158325 
2020-11-14 16:15:28.762556 / Iteration 1800 valid loss -1.7922176122665405 
2020-11-14 16:15:28.763927 / saving best-val-loss model 
2020-11-14 16:16:30.002005 / Iteration 1900: 0.6623651385307312 -2.5795986652374268 
2020-11-14 16:16:30.113377 / Iteration 1900 valid loss -1.878466248512268 
2020-11-14 16:16:30.114549 / saving best-val-loss model 
2020-11-14 16:17:35.099187 / Iteration 2000: 0.6578553318977356 -2.6595568656921387 
2020-11-14 16:17:35.188304 / Iteration 2000 valid loss -1.9602500200271606 
2020-11-14 16:17:35.189735 / saving best-val-loss model 
2020-11-14 16:18:44.343450 / Iteration 2100: 0.6555418968200684 -2.6956796646118164 
2020-11-14 16:18:44.460620 / Iteration 2100 valid loss -1.9942657947540283 
2020-11-14 16:18:44.462049 / saving best-val-loss model 
2020-11-14 16:19:51.819565 / Iteration 2200: 0.6550924181938171 -2.719311237335205 
2020-11-14 16:19:51.938196 / Iteration 2200 valid loss -2.0173816680908203 
2020-11-14 16:19:51.939502 / saving best-val-loss model 
2020-11-14 16:20:57.229090 / Iteration 2300: 0.6549338102340698 -2.7354323863983154 
2020-11-14 16:20:57.348559 / Iteration 2300 valid loss -2.032637357711792 
2020-11-14 16:20:57.350409 / saving best-val-loss model 
2020-11-14 16:22:03.194260 / Iteration 2400: 0.654826819896698 -2.7471091747283936 
2020-11-14 16:22:03.314092 / Iteration 2400 valid loss -2.0440869331359863 
2020-11-14 16:22:03.315422 / saving best-val-loss model 
2020-11-14 16:23:10.251905 / Iteration 2500: 0.6547669768333435 -2.7570927143096924 
2020-11-14 16:23:10.385036 / Iteration 2500 valid loss -2.0531368255615234 
2020-11-14 16:23:10.386356 / saving best-val-loss model 
2020-11-14 16:24:19.692956 / Iteration 2600: 0.6547130346298218 -2.76461124420166 
2020-11-14 16:24:19.823981 / Iteration 2600 valid loss -2.061168670654297 
2020-11-14 16:24:19.825507 / saving best-val-loss model 
2020-11-14 16:25:30.158245 / Iteration 2700: 0.6546786427497864 -2.7714765071868896 
2020-11-14 16:25:30.275529 / Iteration 2700 valid loss -2.0676703453063965 
2020-11-14 16:25:30.276744 / saving best-val-loss model 
2020-11-14 16:26:37.389043 / Iteration 2800: 0.6546531915664673 -2.7774064540863037 
2020-11-14 16:26:37.502010 / Iteration 2800 valid loss -2.072829008102417 
2020-11-14 16:26:37.503696 / saving best-val-loss model 
2020-11-14 16:27:44.234777 / Iteration 2900: 0.6546797156333923 -2.7864019870758057 
2020-11-14 16:27:44.366005 / Iteration 2900 valid loss -2.0808308124542236 
2020-11-14 16:27:44.367170 / saving best-val-loss model 
2020-11-14 16:28:51.139134 / Iteration 3000: 0.6547424793243408 -2.7933034896850586 
2020-11-14 16:28:51.227684 / Iteration 3000 valid loss -2.0869696140289307 
2020-11-14 16:28:51.228974 / saving best-val-loss model 
2020-11-14 16:30:00.952145 / Iteration 3100: 0.654742419719696 -2.7990763187408447 
2020-11-14 16:30:01.065579 / Iteration 3100 valid loss -2.0921287536621094 
2020-11-14 16:30:01.066900 / saving best-val-loss model 
2020-11-14 16:31:09.259656 / Iteration 3200: 0.6547532081604004 -2.8039114475250244 
2020-11-14 16:31:09.346499 / Iteration 3200 valid loss -2.0963385105133057 
2020-11-14 16:31:09.347875 / saving best-val-loss model 
2020-11-14 16:32:16.329180 / Iteration 3300: 0.6547577977180481 -2.80879807472229 
2020-11-14 16:32:16.413015 / Iteration 3300 valid loss -2.1011459827423096 
2020-11-14 16:32:16.414237 / saving best-val-loss model 
2020-11-14 16:33:24.533777 / Iteration 3400: 0.6547788381576538 -2.813558340072632 
2020-11-14 16:33:24.662597 / Iteration 3400 valid loss -2.106031656265259 
2020-11-14 16:33:24.664148 / saving best-val-loss model 
2020-11-14 16:34:32.873468 / Iteration 3500: 0.6548094749450684 -2.8183634281158447 
2020-11-14 16:34:32.960386 / Iteration 3500 valid loss -2.1105659008026123 
2020-11-14 16:34:32.961534 / saving best-val-loss model 
2020-11-14 16:35:40.920073 / Iteration 3600: 0.654837965965271 -2.823776960372925 
2020-11-14 16:35:41.026464 / Iteration 3600 valid loss -2.116442918777466 
2020-11-14 16:35:41.027892 / saving best-val-loss model 
2020-11-14 16:36:49.795386 / Iteration 3700: 0.6548774838447571 -2.829887866973877 
2020-11-14 16:36:49.922434 / Iteration 3700 valid loss -2.122910976409912 
2020-11-14 16:36:49.923838 / saving best-val-loss model 
2020-11-14 16:37:58.548418 / Iteration 3800: 0.6549171805381775 -2.836291790008545 
2020-11-14 16:37:58.631343 / Iteration 3800 valid loss -2.1301286220550537 
2020-11-14 16:37:58.632585 / saving best-val-loss model 
2020-11-14 16:39:05.908567 / Iteration 3900: 0.6549916863441467 -2.8446197509765625 
2020-11-14 16:39:06.030189 / Iteration 3900 valid loss -2.1391990184783936 
2020-11-14 16:39:06.031543 / saving best-val-loss model 
2020-11-14 16:40:13.870614 / Iteration 4000: 0.6550596952438354 -2.8529090881347656 
2020-11-14 16:40:13.994248 / Iteration 4000 valid loss -2.1477034091949463 
2020-11-14 16:40:13.995921 / saving best-val-loss model 
2020-11-14 16:41:21.979349 / Iteration 4100: 0.6551029682159424 -2.8595969676971436 
2020-11-14 16:41:22.068121 / Iteration 4100 valid loss -2.1544206142425537 
2020-11-14 16:41:22.069814 / saving best-val-loss model 
2020-11-14 16:42:28.206432 / Iteration 4200: 0.6551678776741028 -2.867173671722412 
2020-11-14 16:42:28.293092 / Iteration 4200 valid loss -2.160306453704834 
2020-11-14 16:42:28.294409 / saving best-val-loss model 
2020-11-14 16:43:33.960087 / Iteration 4300: 0.6552127599716187 -2.873749256134033 
2020-11-14 16:43:34.040891 / Iteration 4300 valid loss -2.1675775051116943 
2020-11-14 16:43:34.042148 / saving best-val-loss model 
2020-11-14 16:44:43.653046 / Iteration 4400: 0.6552402377128601 -2.8799893856048584 
2020-11-14 16:44:43.792457 / Iteration 4400 valid loss -2.1735079288482666 
2020-11-14 16:44:43.793868 / saving best-val-loss model 
2020-11-14 16:45:51.080441 / Iteration 4500: 0.655081033706665 -2.885316848754883 
2020-11-14 16:45:51.204133 / Iteration 4500 valid loss -2.179006814956665 
2020-11-14 16:45:51.205735 / saving best-val-loss model 
2020-11-14 16:46:58.103056 / Iteration 4600: 0.6536926627159119 -2.8925108909606934 
2020-11-14 16:46:58.185846 / Iteration 4600 valid loss -2.187525749206543 
2020-11-14 16:46:58.187173 / saving best-val-loss model 
2020-11-14 16:48:35.206698 / Namespace(activation='ReLU', atoms=[], batch_size=25000, comet=True, data='lbidd_log_50k', dataroot='/home/mila/r/raghupas/causal-benchmark/datasets', dim_h=4, dist='SigmoidFlow', dist_args=['ndim=8', 'base_distribution=normal'], early_stop=True, eval=True, grad_norm=inf, ignore_w=False, lr=0.01, n_hidden_layers=1, num_epochs=10000, num_univariate_tests=100, overwrite_reload='', patience=None, saveroot='save', seed=123, test_prop=0.4, test_size=None, train=True, train_prop=0.5, val_prop=0.1, w_transform='Normalize', y_transform='Normalize') 
2020-11-14 16:48:35.209146 / getting data: lbidd_log_50k 
2020-11-14 16:48:56.761583 / comet url: https://www.comet.ml/sunandr/causal-benchmark/c3e94d94f6614f298042d798451f9aa7 
2020-11-14 16:48:56.762934 / ate: 0.054972401602067546 
2020-11-14 16:48:56.764292 / <models.distributions.distributions.SigmoidFlow object at 0x7f937f49b090> ndim:8 base_distribution:normal 
2020-11-14 16:48:56.765705 / {'n_hidden_layers': 1, 'dim_h': 4, 'activation': ReLU()} 
2020-11-14 16:48:56.767354 / {'batch_size': 25000, 'lr': 0.01, 'num_epochs': 10000, 'verbose': True, 'print_every_iters': 100, 'optim': <class 'torch.optim.adam.Adam'>, 'eval_every': 100, 'plot_every': 100, 'p_every': 100, 'optim_args': {}} 
2020-11-14 16:49:49.737127 / Iteration 100: 0.6850168704986572 -0.6360487341880798 
2020-11-14 16:49:49.809511 / Iteration 100 valid loss 0.06551748514175415 
2020-11-14 16:49:49.810828 / saving best-val-loss model 
2020-11-14 16:50:50.605493 / Iteration 200: 0.6848717331886292 -0.6956305503845215 
2020-11-14 16:50:50.688807 / Iteration 200 valid loss 0.00736004114151001 
2020-11-14 16:50:50.690410 / saving best-val-loss model 
2020-11-14 16:51:52.092865 / Iteration 300: 0.6846945881843567 -0.702520489692688 
2020-11-14 16:51:52.173799 / Iteration 300 valid loss 0.0002307295799255371 
2020-11-14 16:51:52.175344 / saving best-val-loss model 
2020-11-14 16:52:52.009771 / Iteration 400: 0.684557318687439 -0.7096278071403503 
2020-11-14 16:52:52.080587 / Iteration 400 valid loss -0.005274415016174316 
2020-11-14 16:52:52.081863 / saving best-val-loss model 
2020-11-14 16:53:53.304465 / Iteration 500: 0.6841626763343811 -0.7137575745582581 
2020-11-14 16:53:53.422104 / Iteration 500 valid loss -0.010799288749694824 
2020-11-14 16:53:53.423339 / saving best-val-loss model 
2020-11-14 16:54:54.596711 / Iteration 600: 0.6822858452796936 -0.7197214961051941 
2020-11-14 16:54:54.669514 / Iteration 600 valid loss -0.019646108150482178 
2020-11-14 16:54:54.670749 / saving best-val-loss model 
2020-11-14 16:55:57.137392 / Iteration 700: 0.6719580292701721 -0.737514078617096 
2020-11-14 16:55:57.211547 / Iteration 700 valid loss -0.04813331365585327 
2020-11-14 16:55:57.212683 / saving best-val-loss model 
2020-11-14 16:56:59.206721 / Iteration 800: 0.658992350101471 -0.7604231238365173 
2020-11-14 16:56:59.316091 / Iteration 800 valid loss -0.08229511976242065 
2020-11-14 16:56:59.317529 / saving best-val-loss model 
2020-11-14 16:58:01.412360 / Iteration 900: 0.65462327003479 -0.8196248412132263 
2020-11-14 16:58:01.501381 / Iteration 900 valid loss -0.14056819677352905 
2020-11-14 16:58:01.502913 / saving best-val-loss model 
2020-11-14 16:59:04.633550 / Iteration 1000: 0.6542341113090515 -0.8342338800430298 
2020-11-14 16:59:04.716172 / Iteration 1000 valid loss -0.15200519561767578 
2020-11-14 16:59:04.718101 / saving best-val-loss model 
2020-11-14 17:00:07.058155 / Iteration 1100: 0.6543689370155334 -0.8622817397117615 
2020-11-14 17:00:07.130938 / Iteration 1100 valid loss -0.1857997179031372 
2020-11-14 17:00:07.132146 / saving best-val-loss model 
2020-11-14 17:01:09.297380 / Iteration 1200: 0.6551734209060669 -0.94996178150177 
2020-11-14 17:01:09.377377 / Iteration 1200 valid loss -0.2820321321487427 
2020-11-14 17:01:09.378518 / saving best-val-loss model 
2020-11-14 17:02:14.571551 / Iteration 1300: 0.6552425622940063 -1.0210522413253784 
2020-11-14 17:02:14.665489 / Iteration 1300 valid loss -0.3524329662322998 
2020-11-14 17:02:14.666831 / saving best-val-loss model 
2020-11-14 17:03:18.259491 / Iteration 1400: 0.6552113890647888 -1.0584580898284912 
2020-11-14 17:03:18.336795 / Iteration 1400 valid loss -0.38845890760421753 
2020-11-14 17:03:18.338093 / saving best-val-loss model 
2020-11-14 17:04:20.458404 / Iteration 1500: 0.655028223991394 -1.0873730182647705 
2020-11-14 17:04:20.537573 / Iteration 1500 valid loss -0.42015355825424194 
2020-11-14 17:04:20.539006 / saving best-val-loss model 
2020-11-14 17:05:22.820402 / Iteration 1600: 0.6550441384315491 -1.1048197746276855 
2020-11-14 17:05:22.931196 / Iteration 1600 valid loss -0.43765467405319214 
2020-11-14 17:05:22.932725 / saving best-val-loss model 
2020-11-14 17:06:26.052971 / Iteration 1700: 0.655098557472229 -1.1172609329223633 
2020-11-14 17:06:26.163403 / Iteration 1700 valid loss -0.4476040005683899 
2020-11-14 17:06:26.164642 / saving best-val-loss model 
2020-11-14 17:07:30.532228 / Iteration 1800: 0.6551659107208252 -1.1267200708389282 
2020-11-14 17:07:30.647241 / Iteration 1800 valid loss -0.45821136236190796 
2020-11-14 17:07:30.648392 / saving best-val-loss model 
2020-11-14 17:08:32.333082 / Iteration 1900: 0.6552221775054932 -1.136128544807434 
2020-11-14 17:08:32.459677 / Iteration 1900 valid loss -0.46772003173828125 
2020-11-14 17:08:32.461594 / saving best-val-loss model 
2020-11-14 17:09:33.719765 / Iteration 2000: 0.6554240584373474 -1.131409764289856 
2020-11-14 17:09:33.794554 / Iteration 2000 valid loss -0.45957332849502563 
2020-11-14 17:10:37.363335 / Iteration 2100: 0.6553743481636047 -1.1494543552398682 
2020-11-14 17:10:37.444759 / Iteration 2100 valid loss -0.47946420311927795 
2020-11-14 17:10:37.446618 / saving best-val-loss model 
2020-11-14 17:11:40.145452 / Iteration 2200: 0.655427873134613 -1.1564054489135742 
2020-11-14 17:11:40.233230 / Iteration 2200 valid loss -0.4869762659072876 
2020-11-14 17:11:40.234975 / saving best-val-loss model 
2020-11-14 17:12:41.948320 / Iteration 2300: 0.6554957032203674 -1.1621450185775757 
2020-11-14 17:12:42.028119 / Iteration 2300 valid loss -0.4923853576183319 
2020-11-14 17:12:42.029386 / saving best-val-loss model 
2020-11-14 17:13:42.048243 / Iteration 2400: 0.6555647850036621 -1.166995644569397 
2020-11-14 17:13:42.164743 / Iteration 2400 valid loss -0.49662813544273376 
2020-11-14 17:13:42.165990 / saving best-val-loss model 
2020-11-14 17:14:47.394201 / Iteration 2500: 0.6556936502456665 -1.1659353971481323 
2020-11-14 17:14:47.521607 / Iteration 2500 valid loss -0.4927298426628113 
2020-11-14 17:15:49.777045 / Iteration 2600: 0.6559088826179504 -1.1559364795684814 
2020-11-14 17:15:49.891589 / Iteration 2600 valid loss -0.4863246977329254 
2020-11-14 17:16:55.468778 / Iteration 2700: 0.6558260917663574 -1.1719553470611572 
2020-11-14 17:16:55.546983 / Iteration 2700 valid loss -0.4966552257537842 
2020-11-14 17:16:55.548819 / saving best-val-loss model 
2020-11-14 17:18:00.640706 / Iteration 2800: 0.6557838916778564 -1.1859874725341797 
2020-11-14 17:18:00.759529 / Iteration 2800 valid loss -0.5154896378517151 
2020-11-14 17:18:00.760831 / saving best-val-loss model 
2020-11-14 17:19:03.147491 / Iteration 2900: 0.6558358073234558 -1.1899902820587158 
2020-11-14 17:19:03.224816 / Iteration 2900 valid loss -0.5194055438041687 
2020-11-14 17:19:03.226061 / saving best-val-loss model 
2020-11-14 17:20:04.881723 / Iteration 3000: 0.6558853387832642 -1.1937248706817627 
2020-11-14 17:20:04.992705 / Iteration 3000 valid loss -0.5231829285621643 
2020-11-14 17:20:04.994091 / saving best-val-loss model 
2020-11-14 17:21:09.532997 / Iteration 3100: 0.6560409665107727 -1.1864428520202637 
2020-11-14 17:21:09.602675 / Iteration 3100 valid loss -0.5125873684883118 
2020-11-14 17:22:10.830151 / Iteration 3200: 0.6559799313545227 -1.2014240026474 
2020-11-14 17:22:10.949386 / Iteration 3200 valid loss -0.530488908290863 
2020-11-14 17:22:10.951064 / saving best-val-loss model 
2020-11-14 17:23:12.410397 / Iteration 3300: 0.656024158000946 -1.205397605895996 
2020-11-14 17:23:12.498186 / Iteration 3300 valid loss -0.5343371033668518 
2020-11-14 17:23:12.499630 / saving best-val-loss model 
2020-11-14 17:24:17.427076 / Iteration 3400: 0.6560677289962769 -1.2095285654067993 
2020-11-14 17:24:17.506747 / Iteration 3400 valid loss -0.5382978916168213 
2020-11-14 17:24:17.508169 / saving best-val-loss model 
2020-11-14 17:25:20.476601 / Iteration 3500: 0.6561102271080017 -1.2141505479812622 
2020-11-14 17:25:20.572708 / Iteration 3500 valid loss -0.5427186489105225 
2020-11-14 17:25:20.574044 / saving best-val-loss model 
2020-11-14 17:26:26.889601 / Iteration 3600: 0.6561524271965027 -1.2200993299484253 
2020-11-14 17:26:26.970342 / Iteration 3600 valid loss -0.5475907921791077 
2020-11-14 17:26:26.971826 / saving best-val-loss model 
2020-11-14 17:27:28.930370 / Iteration 3700: 0.6561900973320007 -1.2285280227661133 
2020-11-14 17:27:29.009626 / Iteration 3700 valid loss -0.5563535094261169 
2020-11-14 17:27:29.010866 / saving best-val-loss model 
2020-11-14 17:28:31.807117 / Iteration 3800: 0.656225860118866 -1.2573418617248535 
2020-11-14 17:28:31.917196 / Iteration 3800 valid loss -0.5865573287010193 
2020-11-14 17:28:31.919210 / saving best-val-loss model 
2020-11-14 17:29:38.534989 / Iteration 3900: 0.6566389203071594 -1.3304237127304077 
2020-11-14 17:29:38.663483 / Iteration 3900 valid loss -0.6600701808929443 
2020-11-14 17:29:38.665540 / saving best-val-loss model 
2020-11-14 17:30:44.781816 / Iteration 4000: 0.6562909483909607 -1.4454541206359863 
2020-11-14 17:30:44.862477 / Iteration 4000 valid loss -0.7711523771286011 
2020-11-14 17:30:44.863909 / saving best-val-loss model 
2020-11-14 17:31:47.849517 / Iteration 4100: 0.6563214063644409 -1.5039551258087158 
2020-11-14 17:31:47.954591 / Iteration 4100 valid loss -0.827900767326355 
2020-11-14 17:31:47.955918 / saving best-val-loss model 
2020-11-14 17:32:48.752694 / Iteration 4200: 0.6566517353057861 -1.5071123838424683 
2020-11-14 17:32:48.831384 / Iteration 4200 valid loss -0.8037748336791992 
2020-11-14 17:33:51.400937 / Iteration 4300: 0.6563649773597717 -1.5870507955551147 
2020-11-14 17:33:51.475968 / Iteration 4300 valid loss -0.9106457829475403 
2020-11-14 17:33:51.477812 / saving best-val-loss model 
2020-11-14 17:34:56.028650 / Iteration 4400: 0.6563768982887268 -1.6202011108398438 
2020-11-14 17:34:56.108471 / Iteration 4400 valid loss -0.9426577091217041 
2020-11-14 17:34:56.110087 / saving best-val-loss model 
2020-11-14 17:35:58.095897 / Iteration 4500: 0.6563974022865295 -1.6479815244674683 
2020-11-14 17:35:58.180925 / Iteration 4500 valid loss -0.970238208770752 
2020-11-14 17:35:58.182220 / saving best-val-loss model 
2020-11-14 17:37:05.011377 / Iteration 4600: 0.6564147472381592 -1.672476053237915 
2020-11-14 17:37:05.141715 / Iteration 4600 valid loss -0.9947003126144409 
2020-11-14 17:37:05.143148 / saving best-val-loss model 
2020-11-14 17:38:10.900852 / Iteration 4700: 0.656441330909729 -1.6934709548950195 
2020-11-14 17:38:11.018635 / Iteration 4700 valid loss -1.016054630279541 
2020-11-14 17:38:11.020007 / saving best-val-loss model 
2020-11-14 17:39:14.156933 / Iteration 4800: 0.6565403938293457 -1.7077696323394775 
2020-11-14 17:39:14.246745 / Iteration 4800 valid loss -1.0272014141082764 
2020-11-14 17:39:14.247899 / saving best-val-loss model 
2020-11-14 17:40:16.585657 / Iteration 4900: 0.6564896106719971 -1.737180471420288 
2020-11-14 17:40:16.670165 / Iteration 4900 valid loss -1.062757968902588 
2020-11-14 17:40:16.671363 / saving best-val-loss model 
2020-11-14 17:41:18.391291 / Iteration 5000: 0.6564961671829224 -1.7609803676605225 
2020-11-14 17:41:18.468324 / Iteration 5000 valid loss -1.0855340957641602 
2020-11-14 17:41:18.469835 / saving best-val-loss model 
2020-11-14 17:42:20.841940 / Iteration 5100: 0.6566110849380493 -1.7631921768188477 
2020-11-14 17:42:20.966125 / Iteration 5100 valid loss -1.083897352218628 
2020-11-14 17:43:27.574770 / Iteration 5200: 0.656531810760498 -1.7979507446289062 
2020-11-14 17:43:27.701292 / Iteration 5200 valid loss -1.1228816509246826 
2020-11-14 17:43:27.702713 / saving best-val-loss model 
2020-11-14 17:44:29.994773 / Iteration 5300: 0.6565374732017517 -1.8492176532745361 
2020-11-14 17:44:30.108479 / Iteration 5300 valid loss -1.169234037399292 
2020-11-14 17:44:30.109671 / saving best-val-loss model 
2020-11-14 17:45:32.759704 / Iteration 5400: 0.656552791595459 -1.8698506355285645 
2020-11-14 17:45:32.838776 / Iteration 5400 valid loss -1.1894696950912476 
2020-11-14 17:45:32.840022 / saving best-val-loss model 
2020-11-14 17:46:36.752060 / Iteration 5500: 0.656581461429596 -1.8845912218093872 
2020-11-14 17:46:36.875462 / Iteration 5500 valid loss -1.2101819515228271 
2020-11-14 17:46:36.877020 / saving best-val-loss model 
2020-11-14 17:47:40.452607 / Iteration 5600: 0.6565753221511841 -1.908984899520874 
2020-11-14 17:47:40.577432 / Iteration 5600 valid loss -1.22862708568573 
2020-11-14 17:47:40.578730 / saving best-val-loss model 
2020-11-14 17:48:45.689762 / Iteration 5700: 0.656589150428772 -1.9267417192459106 
2020-11-14 17:48:45.767320 / Iteration 5700 valid loss -1.246354103088379 
2020-11-14 17:48:45.768493 / saving best-val-loss model 
2020-11-14 17:49:50.568496 / Iteration 5800: 0.6566371321678162 -1.938665747642517 
2020-11-14 17:49:50.690977 / Iteration 5800 valid loss -1.2555829286575317 
2020-11-14 17:49:50.692198 / saving best-val-loss model 
2020-11-14 17:50:53.470015 / Iteration 5900: 0.6566165685653687 -1.9652546644210815 
2020-11-14 17:50:53.546996 / Iteration 5900 valid loss -1.2845649719238281 
2020-11-14 17:50:53.548223 / saving best-val-loss model 
2020-11-14 17:51:56.134970 / Iteration 6000: 0.6566293239593506 -1.9844554662704468 
2020-11-14 17:51:56.236822 / Iteration 6000 valid loss -1.3035061359405518 
2020-11-14 17:51:56.238348 / saving best-val-loss model 
2020-11-14 17:53:01.332449 / Iteration 6100: 0.6566593647003174 -1.9955097436904907 
2020-11-14 17:53:01.414065 / Iteration 6100 valid loss -1.3090957403182983 
2020-11-14 17:53:01.415284 / saving best-val-loss model 
2020-11-14 17:54:05.362941 / Iteration 6200: 0.6566532850265503 -2.0195460319519043 
2020-11-14 17:54:05.477171 / Iteration 6200 valid loss -1.337059736251831 
2020-11-14 17:54:05.478338 / saving best-val-loss model 
2020-11-14 17:55:10.555131 / Iteration 6300: 0.6566690802574158 -2.0337328910827637 
2020-11-14 17:55:10.636784 / Iteration 6300 valid loss -1.3501579761505127 
2020-11-14 17:55:10.638042 / saving best-val-loss model 
2020-11-14 17:56:17.061768 / Iteration 6400: 0.6566907167434692 -2.043212890625 
2020-11-14 17:56:17.133509 / Iteration 6400 valid loss -1.3417539596557617 
2020-11-14 17:57:20.542512 / Iteration 6500: 0.6566967368125916 -2.058900833129883 
2020-11-14 17:57:20.666555 / Iteration 6500 valid loss -1.3739075660705566 
2020-11-14 17:57:20.668367 / saving best-val-loss model 
2020-11-14 17:58:45.857519 / Namespace(activation='ReLU', atoms=[], batch_size=25000, comet=True, data='lbidd_log_50k', dataroot='/home/mila/r/raghupas/causal-benchmark/datasets', dim_h=4, dist='SigmoidFlow', dist_args=['ndim=8', 'base_distribution=normal'], early_stop=True, eval=True, grad_norm=inf, ignore_w=False, lr=0.001, n_hidden_layers=1, num_epochs=10000, num_univariate_tests=100, overwrite_reload='', patience=None, saveroot='save', seed=123, test_prop=0.4, test_size=None, train=True, train_prop=0.5, val_prop=0.1, w_transform='Normalize', y_transform='Normalize') 
2020-11-14 17:58:45.860169 / getting data: lbidd_log_50k 
2020-11-14 17:59:07.049523 / comet url: https://www.comet.ml/sunandr/causal-benchmark/f5d92d25c7f844af9adb5cc48a500522 
2020-11-14 17:59:07.050884 / ate: 0.054972401602067546 
2020-11-14 17:59:07.052503 / <models.distributions.distributions.SigmoidFlow object at 0x7f1d51e9b850> ndim:8 base_distribution:normal 
2020-11-14 17:59:07.053984 / {'n_hidden_layers': 1, 'dim_h': 4, 'activation': ReLU()} 
2020-11-14 17:59:07.055434 / {'batch_size': 25000, 'lr': 0.001, 'num_epochs': 10000, 'verbose': True, 'print_every_iters': 100, 'optim': <class 'torch.optim.adam.Adam'>, 'eval_every': 100, 'plot_every': 100, 'p_every': 100, 'optim_args': {}} 
2020-11-14 18:00:01.186697 / Iteration 100: 0.69489586353302 1.317285180091858 
2020-11-14 18:00:01.256408 / Iteration 100 valid loss 2.0092668533325195 
2020-11-14 18:00:01.257702 / saving best-val-loss model 
2020-11-14 18:01:03.448852 / Iteration 200: 0.6851512789726257 0.8443019390106201 
2020-11-14 18:01:03.546239 / Iteration 200 valid loss 1.5215859413146973 
2020-11-14 18:01:03.547743 / saving best-val-loss model 
2020-11-14 18:02:04.452091 / Iteration 300: 0.6850573420524597 0.11164572089910507 
2020-11-14 18:02:04.559106 / Iteration 300 valid loss 0.7897313833236694 
2020-11-14 18:02:04.560325 / saving best-val-loss model 
2020-11-14 18:03:07.523603 / Iteration 400: 0.6850211024284363 -0.34793031215667725 
2020-11-14 18:03:07.602236 / Iteration 400 valid loss 0.33640772104263306 
2020-11-14 18:03:07.603685 / saving best-val-loss model 
2020-11-14 18:04:12.705265 / Iteration 500: 0.6850103735923767 -0.5387563705444336 
2020-11-14 18:04:12.791998 / Iteration 500 valid loss 0.15124166011810303 
2020-11-14 18:04:12.794355 / saving best-val-loss model 
2020-11-14 18:20:18.170576 / Namespace(activation='ReLU', atoms=[], batch_size=25000, comet=True, data='lbidd_log_50k', dataroot='/home/mila/r/raghupas/causal-benchmark/datasets', dim_h=4, dist='SigmoidFlow', dist_args=['ndim=16', 'base_distribution=normal'], early_stop=True, eval=True, grad_norm=inf, ignore_w=False, lr=0.01, n_hidden_layers=1, num_epochs=10000, num_univariate_tests=100, overwrite_reload='', patience=None, saveroot='save', seed=123, test_prop=0.4, test_size=None, train=True, train_prop=0.5, val_prop=0.1, w_transform='Standardize', y_transform='Normalize') 
2020-11-14 18:20:18.173280 / getting data: lbidd_log_50k 
2020-11-14 18:20:40.618271 / comet url: https://www.comet.ml/sunandr/causal-benchmark/4ea78252cd324afd9089e7b0766cdba2 
2020-11-14 18:20:40.619780 / ate: 0.054972401602067546 
2020-11-14 18:20:40.621440 / <models.distributions.distributions.SigmoidFlow object at 0x7fc3e2175610> ndim:16 base_distribution:normal 
2020-11-14 18:20:40.623329 / {'n_hidden_layers': 1, 'dim_h': 4, 'activation': ReLU()} 
2020-11-14 18:20:40.624918 / {'batch_size': 25000, 'lr': 0.01, 'num_epochs': 10000, 'verbose': True, 'print_every_iters': 100, 'optim': <class 'torch.optim.adam.Adam'>, 'eval_every': 100, 'plot_every': 100, 'p_every': 100, 'optim_args': {}} 
2020-11-14 18:21:34.147979 / Iteration 100: 0.6833627223968506 -0.7382907867431641 
2020-11-14 18:21:34.248625 / Iteration 100 valid loss -0.036185622215270996 
2020-11-14 18:21:34.249864 / saving best-val-loss model 
2020-11-14 18:22:36.823128 / Iteration 200: 0.6602382063865662 -2.0977790355682373 
2020-11-14 18:22:36.922347 / Iteration 200 valid loss -1.4269838333129883 
2020-11-14 18:22:36.923652 / saving best-val-loss model 
2020-11-14 18:23:44.940545 / Iteration 300: 0.6550683379173279 -2.6553869247436523 
2020-11-14 18:23:45.010875 / Iteration 300 valid loss -1.9788962602615356 
2020-11-14 18:23:45.012080 / saving best-val-loss model 
2020-11-14 18:24:53.431812 / Iteration 400: 0.6547490358352661 -2.8849587440490723 
2020-11-14 18:24:53.507345 / Iteration 400 valid loss -2.182323694229126 
2020-11-14 18:24:53.508609 / saving best-val-loss model 
2020-11-14 18:26:03.748148 / Iteration 500: 0.6550458073616028 -2.9753897190093994 
2020-11-14 18:26:03.822213 / Iteration 500 valid loss -2.271695137023926 
2020-11-14 18:26:03.823617 / saving best-val-loss model 
2020-11-14 18:27:14.628065 / Iteration 600: 0.655422031879425 -2.992403030395508 
2020-11-14 18:27:14.723796 / Iteration 600 valid loss -2.2838873863220215 
2020-11-14 18:27:14.725261 / saving best-val-loss model 
2020-11-14 18:28:24.243943 / Iteration 700: 0.6542784571647644 -3.0312540531158447 
2020-11-14 18:28:24.349310 / Iteration 700 valid loss -2.318556308746338 
2020-11-14 18:28:24.350445 / saving best-val-loss model 
2020-11-14 18:29:34.233313 / Iteration 800: 0.6543077230453491 -3.0515432357788086 
2020-11-14 18:29:34.320300 / Iteration 800 valid loss -2.3345961570739746 
2020-11-14 18:29:34.321816 / saving best-val-loss model 
2020-11-14 18:30:43.599118 / Iteration 900: 0.6542550921440125 -3.048232078552246 
2020-11-14 18:30:43.695432 / Iteration 900 valid loss -2.3115527629852295 
2020-11-14 18:31:54.733144 / Iteration 1000: 0.65423184633255 -3.0717735290527344 
2020-11-14 18:31:54.817533 / Iteration 1000 valid loss -2.338430881500244 
2020-11-14 18:31:54.818771 / saving best-val-loss model 
2020-11-14 18:33:06.894773 / Iteration 1100: 0.65425044298172 -3.0737156867980957 
2020-11-14 18:33:06.981208 / Iteration 1100 valid loss -2.340235710144043 
2020-11-14 18:33:06.982490 / saving best-val-loss model 
2020-11-14 18:34:18.218525 / Iteration 1200: 0.6542473435401917 -3.0812339782714844 
2020-11-14 18:34:18.294064 / Iteration 1200 valid loss -2.351884365081787 
2020-11-14 18:34:18.295259 / saving best-val-loss model 
2020-11-14 18:35:27.179494 / Iteration 1300: 0.6542379856109619 -3.0830190181732178 
2020-11-14 18:35:27.295431 / Iteration 1300 valid loss -2.3508214950561523 
2020-11-14 18:36:34.532440 / Iteration 1400: 0.6542472839355469 -3.098668336868286 
2020-11-14 18:36:34.619454 / Iteration 1400 valid loss -2.370887517929077 
2020-11-14 18:36:34.620783 / saving best-val-loss model 
2020-11-14 18:37:45.215353 / Iteration 1500: 0.6542479395866394 -3.0954930782318115 
2020-11-14 18:37:45.339878 / Iteration 1500 valid loss -2.361529588699341 
2020-11-14 18:38:56.489467 / Iteration 1600: 0.654129147529602 -3.107947587966919 
2020-11-14 18:38:56.587293 / Iteration 1600 valid loss -2.385443925857544 
2020-11-14 18:38:56.588805 / saving best-val-loss model 
2020-11-14 18:40:09.109883 / Iteration 1700: 0.6541426181793213 -3.0957608222961426 
2020-11-14 18:40:09.188926 / Iteration 1700 valid loss -2.377882480621338 
2020-11-14 18:41:19.116860 / Iteration 1800: 0.6541180610656738 -3.117767572402954 
2020-11-14 18:41:19.236002 / Iteration 1800 valid loss -2.3875012397766113 
2020-11-14 18:41:19.237472 / saving best-val-loss model 
2020-11-14 18:42:28.669349 / Iteration 1900: 0.6541019678115845 -3.1270172595977783 
2020-11-14 18:42:28.752126 / Iteration 1900 valid loss -2.400663137435913 
2020-11-14 18:42:28.753295 / saving best-val-loss model 
2020-11-14 18:43:37.562507 / Iteration 2000: 0.6540043354034424 -3.1274497509002686 
2020-11-14 18:43:37.641374 / Iteration 2000 valid loss -2.407021999359131 
2020-11-14 18:43:37.642574 / saving best-val-loss model 
2020-11-14 18:44:48.690635 / Iteration 2100: 0.6539540886878967 -3.1232805252075195 
2020-11-14 18:44:48.809863 / Iteration 2100 valid loss -2.407470703125 
2020-11-14 18:44:48.811096 / saving best-val-loss model 
2020-11-14 18:45:59.895239 / Iteration 2200: 0.6539232134819031 -3.127572536468506 
2020-11-14 18:45:59.984704 / Iteration 2200 valid loss -2.4065327644348145 
2020-11-14 18:47:06.803420 / Iteration 2300: 0.6539177894592285 -3.1313579082489014 
2020-11-14 18:47:06.881995 / Iteration 2300 valid loss -2.4115662574768066 
2020-11-14 18:47:06.883531 / saving best-val-loss model 
2020-11-14 18:48:16.810569 / Iteration 2400: 0.6538716554641724 -3.1393184661865234 
2020-11-14 18:48:16.894869 / Iteration 2400 valid loss -2.42117977142334 
2020-11-14 18:48:16.896089 / saving best-val-loss model 
2020-11-14 18:49:27.605622 / Iteration 2500: 0.6538568735122681 -3.1101553440093994 
2020-11-14 18:49:27.725833 / Iteration 2500 valid loss -2.396761655807495 
2020-11-14 18:50:37.861670 / Iteration 2600: 0.6538159251213074 -3.1337759494781494 
2020-11-14 18:50:37.942065 / Iteration 2600 valid loss -2.406643867492676 
2020-11-14 18:51:49.722321 / Iteration 2700: 0.6538066267967224 -3.1519219875335693 
2020-11-14 18:51:49.844056 / Iteration 2700 valid loss -2.4346237182617188 
2020-11-14 18:51:49.845397 / saving best-val-loss model 
2020-11-14 18:52:59.810176 / Iteration 2800: 0.6537935733795166 -3.158172607421875 
2020-11-14 18:52:59.929526 / Iteration 2800 valid loss -2.4405479431152344 
2020-11-14 18:52:59.930879 / saving best-val-loss model 
2020-11-14 18:54:10.513618 / Iteration 2900: 0.653756856918335 -3.1388869285583496 
2020-11-14 18:54:10.598203 / Iteration 2900 valid loss -2.423396110534668 
2020-11-14 18:55:20.393658 / Iteration 3000: 0.6537485122680664 -3.1628713607788086 
2020-11-14 18:55:20.478438 / Iteration 3000 valid loss -2.450479745864868 
2020-11-14 18:55:20.479745 / saving best-val-loss model 
2020-11-14 18:56:28.860794 / Iteration 3100: 0.6537554860115051 -3.139646530151367 
2020-11-14 18:56:28.996999 / Iteration 3100 valid loss -2.420537233352661 
2020-11-14 18:57:40.606490 / Iteration 3200: 0.6537529826164246 -3.1457836627960205 
2020-11-14 18:57:40.726789 / Iteration 3200 valid loss -2.444239616394043 
2020-11-14 18:58:49.676862 / Iteration 3300: 0.653795063495636 -3.1464481353759766 
2020-11-14 18:58:49.817659 / Iteration 3300 valid loss -2.4196295738220215 
2020-11-14 18:59:57.795532 / Iteration 3400: 0.6537484526634216 -3.17240571975708 
2020-11-14 18:59:57.874404 / Iteration 3400 valid loss -2.4572086334228516 
2020-11-14 18:59:57.875619 / saving best-val-loss model 
2020-11-14 19:05:48.156371 / Namespace(activation='ReLU', atoms=[], batch_size=25000, comet=True, data='lbidd_log_50k', dataroot='/home/mila/r/raghupas/causal-benchmark/datasets', dim_h=4, dist='SigmoidFlow', dist_args=['ndim=16', 'base_distribution=normal'], early_stop=True, eval=True, grad_norm=inf, ignore_w=False, lr=0.01, n_hidden_layers=1, num_epochs=3200, num_univariate_tests=100, overwrite_reload='', patience=None, saveroot='save', seed=123, test_prop=0.4, test_size=None, train=True, train_prop=0.5, val_prop=0.1, w_transform='Standardize', y_transform='Normalize') 
2020-11-14 19:05:48.159439 / getting data: lbidd_log_50k 
2020-11-14 19:06:09.297915 / comet url: https://www.comet.ml/sunandr/causal-benchmark/3d28929035e44a3db0486ef0c45341e2 
2020-11-14 19:06:09.299379 / ate: 0.054972401602067546 
2020-11-14 19:06:09.300838 / <models.distributions.distributions.SigmoidFlow object at 0x7f8c68db84d0> ndim:16 base_distribution:normal 
2020-11-14 19:06:09.302409 / {'n_hidden_layers': 1, 'dim_h': 4, 'activation': ReLU()} 
2020-11-14 19:06:09.304090 / {'batch_size': 25000, 'lr': 0.01, 'num_epochs': 3200, 'verbose': True, 'print_every_iters': 100, 'optim': <class 'torch.optim.adam.Adam'>, 'eval_every': 100, 'plot_every': 100, 'p_every': 100, 'optim_args': {}} 
2020-11-14 19:07:03.423694 / Iteration 100: 0.6833627223968506 -0.7382907867431641 
2020-11-14 19:07:03.492400 / Iteration 100 valid loss -0.036185622215270996 
2020-11-14 19:07:03.493666 / saving best-val-loss model 
2020-11-14 19:08:07.387678 / Iteration 200: 0.6602382063865662 -2.0977790355682373 
2020-11-14 19:08:07.506126 / Iteration 200 valid loss -1.4269838333129883 
2020-11-14 19:08:07.507396 / saving best-val-loss model 
2020-11-14 19:09:15.924049 / Iteration 300: 0.6550683379173279 -2.6553869247436523 
2020-11-14 19:09:16.005273 / Iteration 300 valid loss -1.9788962602615356 
2020-11-14 19:09:16.006633 / saving best-val-loss model 
2020-11-14 19:10:26.098554 / Iteration 400: 0.6547490358352661 -2.8849587440490723 
2020-11-14 19:10:26.210272 / Iteration 400 valid loss -2.182323694229126 
2020-11-14 19:10:26.211699 / saving best-val-loss model 
2020-11-14 19:11:35.639742 / Iteration 500: 0.6550458073616028 -2.9753897190093994 
2020-11-14 19:11:35.715592 / Iteration 500 valid loss -2.271695137023926 
2020-11-14 19:11:35.716804 / saving best-val-loss model 
2020-11-14 19:12:46.994440 / Iteration 600: 0.655422031879425 -2.992403030395508 
2020-11-14 19:12:47.119344 / Iteration 600 valid loss -2.2838873863220215 
2020-11-14 19:12:47.120713 / saving best-val-loss model 
2020-11-14 19:13:57.212933 / Iteration 700: 0.6542784571647644 -3.0312540531158447 
2020-11-14 19:13:57.347914 / Iteration 700 valid loss -2.318556308746338 
2020-11-14 19:13:57.349475 / saving best-val-loss model 
2020-11-14 19:15:11.990587 / Iteration 800: 0.6543077230453491 -3.0515432357788086 
2020-11-14 19:15:12.132460 / Iteration 800 valid loss -2.3345961570739746 
2020-11-14 19:15:12.133712 / saving best-val-loss model 
2020-11-14 19:16:22.746321 / Iteration 900: 0.6542550921440125 -3.048232078552246 
2020-11-14 19:16:22.825375 / Iteration 900 valid loss -2.3115527629852295 
2020-11-14 19:17:33.110057 / Iteration 1000: 0.65423184633255 -3.0717735290527344 
2020-11-14 19:17:33.208675 / Iteration 1000 valid loss -2.338430881500244 
2020-11-14 19:17:33.209905 / saving best-val-loss model 
2020-11-14 19:18:43.491803 / Iteration 1100: 0.65425044298172 -3.0737156867980957 
2020-11-14 19:18:43.571956 / Iteration 1100 valid loss -2.340235710144043 
2020-11-14 19:18:43.573367 / saving best-val-loss model 
2020-11-14 19:19:53.825691 / Iteration 1200: 0.6542473435401917 -3.0812339782714844 
2020-11-14 19:19:53.950187 / Iteration 1200 valid loss -2.351884365081787 
2020-11-14 19:19:53.951335 / saving best-val-loss model 
2020-11-14 19:21:06.985016 / Iteration 1300: 0.6542379856109619 -3.0830190181732178 
2020-11-14 19:21:07.094876 / Iteration 1300 valid loss -2.3508214950561523 
2020-11-14 19:22:19.127474 / Iteration 1400: 0.6542472839355469 -3.098668336868286 
2020-11-14 19:22:19.241590 / Iteration 1400 valid loss -2.370887517929077 
2020-11-14 19:22:19.243201 / saving best-val-loss model 
2020-11-14 19:23:30.301253 / Iteration 1500: 0.6542479395866394 -3.0954930782318115 
2020-11-14 19:23:30.379469 / Iteration 1500 valid loss -2.361529588699341 
2020-11-14 19:24:40.610541 / Iteration 1600: 0.654129147529602 -3.107947587966919 
2020-11-14 19:24:40.693555 / Iteration 1600 valid loss -2.385443925857544 
2020-11-14 19:24:40.695349 / saving best-val-loss model 
2020-11-14 19:25:50.580876 / Iteration 1700: 0.6541426181793213 -3.0957608222961426 
2020-11-14 19:25:50.669401 / Iteration 1700 valid loss -2.377882480621338 
2020-11-14 19:27:03.153288 / Iteration 1800: 0.6541180610656738 -3.117767572402954 
2020-11-14 19:27:03.279085 / Iteration 1800 valid loss -2.3875012397766113 
2020-11-14 19:27:03.280331 / saving best-val-loss model 
2020-11-14 19:28:17.630778 / Iteration 1900: 0.6541019678115845 -3.1270172595977783 
2020-11-14 19:28:17.711707 / Iteration 1900 valid loss -2.400663137435913 
2020-11-14 19:28:17.713035 / saving best-val-loss model 
2020-11-14 19:29:26.817529 / Iteration 2000: 0.6540043354034424 -3.1274497509002686 
2020-11-14 19:29:26.897783 / Iteration 2000 valid loss -2.407021999359131 
2020-11-14 19:29:26.899018 / saving best-val-loss model 
2020-11-14 19:30:38.439236 / Iteration 2100: 0.6539540886878967 -3.1232805252075195 
2020-11-14 19:30:38.521533 / Iteration 2100 valid loss -2.407470703125 
2020-11-14 19:30:38.522770 / saving best-val-loss model 
2020-11-14 19:31:50.043106 / Iteration 2200: 0.6539232134819031 -3.127572536468506 
2020-11-14 19:31:50.133629 / Iteration 2200 valid loss -2.4065327644348145 
2020-11-14 19:33:01.991688 / Iteration 2300: 0.6539177894592285 -3.1313579082489014 
2020-11-14 19:33:02.070559 / Iteration 2300 valid loss -2.4115662574768066 
2020-11-14 19:33:02.071844 / saving best-val-loss model 
2020-11-14 19:34:14.409012 / Iteration 2400: 0.6538716554641724 -3.1393184661865234 
2020-11-14 19:34:14.529033 / Iteration 2400 valid loss -2.42117977142334 
2020-11-14 19:34:14.530402 / saving best-val-loss model 
2020-11-14 19:35:27.085859 / Iteration 2500: 0.6538568735122681 -3.1101553440093994 
2020-11-14 19:35:27.209175 / Iteration 2500 valid loss -2.396761655807495 
2020-11-14 19:36:39.687741 / Iteration 2600: 0.6538159251213074 -3.1337759494781494 
2020-11-14 19:36:39.773182 / Iteration 2600 valid loss -2.406643867492676 
2020-11-14 19:37:48.777324 / Iteration 2700: 0.6538066267967224 -3.1519219875335693 
2020-11-14 19:37:48.913883 / Iteration 2700 valid loss -2.4346237182617188 
2020-11-14 19:37:48.915147 / saving best-val-loss model 
2020-11-14 19:39:01.106697 / Iteration 2800: 0.6537935733795166 -3.158172607421875 
2020-11-14 19:39:01.186390 / Iteration 2800 valid loss -2.4405479431152344 
2020-11-14 19:39:01.187701 / saving best-val-loss model 
2020-11-14 19:40:11.176116 / Iteration 2900: 0.653756856918335 -3.1388869285583496 
2020-11-14 19:40:11.254640 / Iteration 2900 valid loss -2.423396110534668 
2020-11-14 19:41:21.488768 / Iteration 3000: 0.6537485122680664 -3.1628713607788086 
2020-11-14 19:41:21.579515 / Iteration 3000 valid loss -2.450479745864868 
2020-11-14 19:41:21.580788 / saving best-val-loss model 
2020-11-14 19:42:32.366921 / Iteration 3100: 0.6537554860115051 -3.139646530151367 
2020-11-14 19:42:32.480876 / Iteration 3100 valid loss -2.420537233352661 
2020-11-14 19:43:44.264417 / Iteration 3200: 0.6537529826164246 -3.1457836627960205 
2020-11-14 19:43:44.359112 / Iteration 3200 valid loss -2.444239616394043 
2020-11-14 19:54:59.318335 / OrderedDict([('nll', -2.450479745864868), ('avg_t_pval', 0.8967337606104371), ('avg_y_pval', 0.07730257980042066), ('min_t_pval', 0.13064467513421296), ('min_y_pval', 0.020071466443027933), ('q30_t_pval', 0.9290835917025413), ('q30_y_pval', 0.05776257046427332), ('q50_t_pval', 0.9989384258511402), ('q50_y_pval', 0.0702819492517697), ('ate_exact', 0.0), ('ate_noisy', 0.04756111972033977)]) 
