2020-11-11 23:23:57.269846 / Namespace(activation='ReLU', atoms=[], batch_size=25000, comet=True, data='lbidd_quadratic_50k', dataroot='/home/mila/r/raghupas/causal-benchmark/datasets', dim_h=4, dist='SigmoidFlow', dist_args=['ndim=8', 'base_distribution=normal'], early_stop=True, eval=True, grad_norm=inf, ignore_w=False, lr=0.001, n_hidden_layers=1, num_epochs=3000, num_univariate_tests=100, overwrite_reload='', patience=None, saveroot='save', seed=123, test_prop=0.4, test_size=None, train=True, train_prop=0.5, val_prop=0.1, w_transform='Standardize', y_transform='Standardize') 
2020-11-11 23:23:57.272321 / getting data: lbidd_quadratic_50k 
2020-11-11 23:24:21.470521 / comet url: https://www.comet.ml/sunandr/causal-benchmark/8fb8a588d0dc41c0b4b1d56bebef6d2b 
2020-11-11 23:24:21.472486 / ate: 2.5437197090573935 
2020-11-11 23:24:21.474267 / <models.distributions.distributions.SigmoidFlow object at 0x7ff4ddfbf810> ndim:8 base_distribution:normal 
2020-11-11 23:24:21.476108 / {'n_hidden_layers': 1, 'dim_h': 4, 'activation': ReLU()} 
2020-11-11 23:24:21.478275 / {'batch_size': 25000, 'lr': 0.001, 'num_epochs': 3000, 'verbose': True, 'print_every_iters': 100, 'optim': <class 'torch.optim.adam.Adam'>, 'eval_every': 100, 'plot_every': 100, 'p_every': 100, 'optim_args': {}} 
2020-11-11 23:25:05.420259 / Iteration 100: 0.651818573474884 1.0465136766433716 
2020-11-11 23:25:05.495399 / Iteration 100 valid loss 1.7220834493637085 
2020-11-11 23:25:05.497140 / saving best-val-loss model 
2020-11-11 23:25:56.591889 / Iteration 200: 0.6592492461204529 0.2000340074300766 
2020-11-11 23:25:56.653870 / Iteration 200 valid loss 0.8878782391548157 
2020-11-11 23:25:56.655283 / saving best-val-loss model 
2020-11-11 23:26:48.639987 / Iteration 300: 0.6810579895973206 -0.3694908618927002 
2020-11-11 23:26:48.710584 / Iteration 300 valid loss 0.32342225313186646 
2020-11-11 23:26:48.712224 / saving best-val-loss model 
2020-11-11 23:27:40.380163 / Iteration 400: 0.6707234382629395 -0.7208735346794128 
2020-11-11 23:27:40.445324 / Iteration 400 valid loss -0.05114579200744629 
2020-11-11 23:27:40.446762 / saving best-val-loss model 
2020-11-11 23:28:33.196246 / Iteration 500: 0.6347158551216125 -0.8727176785469055 
2020-11-11 23:28:33.263312 / Iteration 500 valid loss -0.24309033155441284 
2020-11-11 23:28:33.265005 / saving best-val-loss model 
2020-11-11 23:29:26.663929 / Iteration 600: 0.5118567943572998 -1.0672584772109985 
2020-11-11 23:29:26.731551 / Iteration 600 valid loss -0.5600597262382507 
2020-11-11 23:29:26.733466 / saving best-val-loss model 
2020-11-11 23:30:20.086616 / Iteration 700: 0.46912625432014465 -1.1066385507583618 
2020-11-11 23:30:20.167795 / Iteration 700 valid loss -0.6447917222976685 
2020-11-11 23:30:20.169306 / saving best-val-loss model 
2020-11-11 23:31:13.699230 / Iteration 800: 0.4528231918811798 -1.1244590282440186 
2020-11-11 23:31:13.765748 / Iteration 800 valid loss -0.6838569045066833 
2020-11-11 23:31:13.767524 / saving best-val-loss model 
2020-11-11 23:32:06.905034 / Iteration 900: 0.4375460147857666 -1.1484243869781494 
2020-11-11 23:32:06.990912 / Iteration 900 valid loss -0.7243523001670837 
2020-11-11 23:32:06.992581 / saving best-val-loss model 
2020-11-11 23:33:01.044187 / Iteration 1000: 0.4334006607532501 -1.1655385494232178 
2020-11-11 23:33:01.117536 / Iteration 1000 valid loss -0.7407511472702026 
2020-11-11 23:33:01.119176 / saving best-val-loss model 
2020-11-11 23:33:56.267045 / Iteration 1100: 0.4306560456752777 -1.1836349964141846 
2020-11-11 23:33:56.336103 / Iteration 1100 valid loss -0.7587242722511292 
2020-11-11 23:33:56.337976 / saving best-val-loss model 
2020-11-11 23:34:50.251564 / Iteration 1200: 0.4288291931152344 -1.1976062059402466 
2020-11-11 23:34:50.318956 / Iteration 1200 valid loss -0.7694646120071411 
2020-11-11 23:34:50.320645 / saving best-val-loss model 
2020-11-11 23:35:44.013892 / Iteration 1300: 0.4276033937931061 -1.2044212818145752 
2020-11-11 23:35:44.096669 / Iteration 1300 valid loss -0.7727606296539307 
2020-11-11 23:35:44.098412 / saving best-val-loss model 
2020-11-11 23:36:38.004539 / Iteration 1400: 0.4266989529132843 -1.2103570699691772 
2020-11-11 23:36:38.075217 / Iteration 1400 valid loss -0.7790067791938782 
2020-11-11 23:36:38.076572 / saving best-val-loss model 
2020-11-11 23:37:31.880933 / Iteration 1500: 0.42702656984329224 -1.219813346862793 
2020-11-11 23:37:31.959002 / Iteration 1500 valid loss -0.788011372089386 
2020-11-11 23:37:31.960160 / saving best-val-loss model 
2020-11-11 23:38:25.363616 / Iteration 1600: 0.4267965257167816 -1.2278488874435425 
2020-11-11 23:38:25.433943 / Iteration 1600 valid loss -0.7967026233673096 
2020-11-11 23:38:25.435288 / saving best-val-loss model 
2020-11-11 23:39:19.165988 / Iteration 1700: 0.42629238963127136 -1.2318611145019531 
2020-11-11 23:39:19.235969 / Iteration 1700 valid loss -0.8002512454986572 
2020-11-11 23:39:19.238041 / saving best-val-loss model 
2020-11-11 23:40:13.478383 / Iteration 1800: 0.4257894456386566 -1.2351704835891724 
2020-11-11 23:40:13.566065 / Iteration 1800 valid loss -0.8022559881210327 
2020-11-11 23:40:13.567423 / saving best-val-loss model 
2020-11-11 23:41:08.463588 / Iteration 1900: 0.42520394921302795 -1.2380262613296509 
2020-11-11 23:41:08.552809 / Iteration 1900 valid loss -0.8045681715011597 
2020-11-11 23:41:08.554500 / saving best-val-loss model 
2020-11-11 23:42:02.956687 / Iteration 2000: 0.42466363310813904 -1.2406822443008423 
2020-11-11 23:42:03.028611 / Iteration 2000 valid loss -0.8060771226882935 
2020-11-11 23:42:03.030105 / saving best-val-loss model 
2020-11-11 23:42:57.502924 / Iteration 2100: 0.4242347776889801 -1.2427982091903687 
2020-11-11 23:42:57.577962 / Iteration 2100 valid loss -0.8071844577789307 
2020-11-11 23:42:57.579818 / saving best-val-loss model 
2020-11-11 23:43:51.354200 / Iteration 2200: 0.4239048957824707 -1.244659185409546 
2020-11-11 23:43:51.421208 / Iteration 2200 valid loss -0.8086493015289307 
2020-11-11 23:43:51.422743 / saving best-val-loss model 
2020-11-11 23:44:45.732868 / Iteration 2300: 0.42353543639183044 -1.246119499206543 
2020-11-11 23:44:45.815174 / Iteration 2300 valid loss -0.8091498613357544 
2020-11-11 23:44:45.816600 / saving best-val-loss model 
2020-11-11 23:45:39.827348 / Iteration 2400: 0.4232787787914276 -1.2474274635314941 
2020-11-11 23:45:39.901042 / Iteration 2400 valid loss -0.809442400932312 
2020-11-11 23:45:39.902499 / saving best-val-loss model 
2020-11-11 23:46:33.827062 / Iteration 2500: 0.4229780435562134 -1.248457908630371 
2020-11-11 23:46:33.898152 / Iteration 2500 valid loss -0.81036376953125 
2020-11-11 23:46:33.899413 / saving best-val-loss model 
2020-11-11 23:47:28.188861 / Iteration 2600: 0.42265671491622925 -1.249254584312439 
2020-11-11 23:47:28.259491 / Iteration 2600 valid loss -0.811046838760376 
2020-11-11 23:47:28.261269 / saving best-val-loss model 
2020-11-11 23:48:22.776967 / Iteration 2700: 0.42238500714302063 -1.2498939037322998 
2020-11-11 23:48:22.857058 / Iteration 2700 valid loss -0.8111485242843628 
2020-11-11 23:48:22.858442 / saving best-val-loss model 
2020-11-11 23:49:17.053548 / Iteration 2800: 0.42201635241508484 -1.2504785060882568 
2020-11-11 23:49:17.119891 / Iteration 2800 valid loss -0.8110995292663574 
2020-11-11 23:50:11.777030 / Iteration 2900: 0.4218103885650635 -1.2509952783584595 
2020-11-11 23:50:11.844768 / Iteration 2900 valid loss -0.8101191520690918 
2020-11-11 23:51:06.916558 / Iteration 3000: 0.42155805230140686 -1.2515064477920532 
2020-11-11 23:51:06.987530 / Iteration 3000 valid loss -0.8078356385231018 
2020-11-11 23:57:20.089608 / OrderedDict([('nll', -0.8111485242843628), ('avg_t_pval', 0.9486634612538665), ('avg_y_pval', 0.4080388156953206), ('min_t_pval', 0.14333060976330916), ('min_y_pval', 0.014118131391049301), ('q30_t_pval', 0.9741217681370615), ('q30_y_pval', 0.29016215632083725), ('q50_t_pval', 0.9976330351851795), ('q50_y_pval', 0.4071986254335911), ('ate_exact', 0.0), ('ate_noisy', 4.049595463275909)]) 
