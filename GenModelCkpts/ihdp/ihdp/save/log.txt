2020-11-12 01:47:02.182349 / Namespace(activation='ReLU', atoms=[0.0], batch_size=25000, comet=True, data='lalonde', dataroot='/home/mila/r/raghupas/causal-benchmark/datasets', dim_h=4, dist='SigmoidFlow', dist_args=['ndim=64', 'base_distribution=uniform'], early_stop=True, eval=True, grad_norm=inf, ignore_w=False, lr=0.01, n_hidden_layers=1, num_epochs=10000, num_univariate_tests=100, overwrite_reload='', patience=None, saveroot='save', seed=123, test_prop=0.4, test_size=None, train=True, train_prop=0.5, val_prop=0.1, w_transform='Normalize', y_transform='Normalize') 
2020-11-12 01:47:02.184852 / getting data: lalonde 
2020-11-12 01:47:24.918989 / comet url: https://www.comet.ml/sunandr/causal-benchmark/9684262eadd5427592d6f64bf752c987 
2020-11-12 01:47:24.920754 / ate: None 
2020-11-12 01:47:24.922415 / <models.distributions.distributions.MixedDistribution object at 0x7fe3cd55cb50> 
2020-11-12 01:47:24.924303 / {'n_hidden_layers': 1, 'dim_h': 4, 'activation': ReLU()} 
2020-11-12 01:47:24.926180 / {'batch_size': 25000, 'lr': 0.01, 'num_epochs': 10000, 'verbose': True, 'print_every_iters': 100, 'optim': <class 'torch.optim.adam.Adam'>, 'eval_every': 100, 'plot_every': 100, 'p_every': 100, 'optim_args': {}} 
2020-11-12 01:47:27.805694 / Iteration 100: 0.2641369700431824 -0.006096299737691879 
2020-11-12 01:47:27.815373 / Iteration 100 valid loss 0.26809659600257874 
2020-11-12 01:47:27.816767 / saving best-val-loss model 
2020-11-12 01:47:34.943163 / Iteration 200: 0.24058277904987335 -0.30659085512161255 
2020-11-12 01:47:34.951493 / Iteration 200 valid loss -0.04429924488067627 
2020-11-12 01:47:34.953010 / saving best-val-loss model 
2020-11-12 01:47:42.279669 / Iteration 300: 0.24056892096996307 -0.31790006160736084 
2020-11-12 01:47:42.288145 / Iteration 300 valid loss -0.05994302034378052 
2020-11-12 01:47:42.289852 / saving best-val-loss model 
2020-11-12 01:47:49.517997 / Iteration 400: 0.24056881666183472 -0.3205437660217285 
2020-11-12 01:47:49.527145 / Iteration 400 valid loss -0.0627129077911377 
2020-11-12 01:47:49.528852 / saving best-val-loss model 
2020-11-12 01:47:56.917867 / Iteration 500: 0.2405688315629959 -0.32217440009117126 
2020-11-12 01:47:56.926949 / Iteration 500 valid loss -0.06348264217376709 
2020-11-12 01:47:56.928633 / saving best-val-loss model 
2020-11-12 01:48:04.287887 / Iteration 600: 0.2405688315629959 -0.32410672307014465 
2020-11-12 01:48:04.296989 / Iteration 600 valid loss -0.06249624118208885 
2020-11-12 01:48:11.621727 / Iteration 700: 0.2405688315629959 -0.32593727111816406 
2020-11-12 01:48:11.630374 / Iteration 700 valid loss -0.06215432286262512 
2020-11-12 01:48:19.028779 / Iteration 800: 0.2405688315629959 -0.3292508125305176 
2020-11-12 01:48:19.038076 / Iteration 800 valid loss -0.06350940465927124 
2020-11-12 01:48:19.039809 / saving best-val-loss model 
2020-11-12 01:48:26.561385 / Iteration 900: 0.2405688613653183 -0.3327648341655731 
2020-11-12 01:48:26.569591 / Iteration 900 valid loss -0.06718289852142334 
2020-11-12 01:48:26.571196 / saving best-val-loss model 
2020-11-12 01:48:34.097240 / Iteration 1000: 0.2405688315629959 -0.3342893421649933 
2020-11-12 01:48:34.105213 / Iteration 1000 valid loss -0.06892621517181396 
2020-11-12 01:48:34.106375 / saving best-val-loss model 
2020-11-12 01:48:41.672194 / Iteration 1100: 0.2405688613653183 -0.3349481225013733 
2020-11-12 01:48:41.680524 / Iteration 1100 valid loss -0.06926891207695007 
2020-11-12 01:48:41.681917 / saving best-val-loss model 
2020-11-12 01:48:49.091383 / Iteration 1200: 0.2405688762664795 -0.33760958909988403 
2020-11-12 01:48:49.100556 / Iteration 1200 valid loss -0.06971925497055054 
2020-11-12 01:48:49.102110 / saving best-val-loss model 
2020-11-12 01:48:56.523760 / Iteration 1300: 0.2405688613653183 -0.3400939404964447 
2020-11-12 01:48:56.532506 / Iteration 1300 valid loss -0.06889227032661438 
2020-11-12 01:49:04.052016 / Iteration 1400: 0.2405688315629959 -0.3411712348461151 
2020-11-12 01:49:04.060629 / Iteration 1400 valid loss -0.06889459490776062 
2020-11-12 01:49:11.528820 / Iteration 1500: 0.2405688613653183 -0.3420872390270233 
2020-11-12 01:49:11.537388 / Iteration 1500 valid loss -0.0694015622138977 
2020-11-12 01:49:18.892080 / Iteration 1600: 0.24056890606880188 -0.3428969383239746 
2020-11-12 01:49:18.900935 / Iteration 1600 valid loss -0.07023334503173828 
2020-11-12 01:49:18.903506 / saving best-val-loss model 
2020-11-12 01:49:26.655986 / Iteration 1700: 0.2405688613653183 -0.3434025049209595 
2020-11-12 01:49:26.664768 / Iteration 1700 valid loss -0.07066568732261658 
2020-11-12 01:49:26.666281 / saving best-val-loss model 
2020-11-12 01:49:34.287690 / Iteration 1800: 0.2405688613653183 -0.343932181596756 
2020-11-12 01:49:34.295670 / Iteration 1800 valid loss -0.06958544254302979 
2020-11-12 01:49:41.744386 / Iteration 1900: 0.2405688315629959 -0.34430867433547974 
2020-11-12 01:49:41.752337 / Iteration 1900 valid loss -0.06938022375106812 
2020-11-12 01:49:49.165567 / Iteration 2000: 0.2405688315629959 -0.34459608793258667 
2020-11-12 01:49:49.173778 / Iteration 2000 valid loss -0.069163978099823 
2020-11-12 01:49:56.499915 / Iteration 2100: 0.2405688315629959 -0.3448152542114258 
2020-11-12 01:49:56.508241 / Iteration 2100 valid loss -0.06893375515937805 
2020-11-12 01:50:03.761263 / Iteration 2200: 0.2405688315629959 -0.34499892592430115 
2020-11-12 01:50:03.770654 / Iteration 2200 valid loss -0.06878188252449036 
2020-11-12 01:50:11.495725 / Iteration 2300: 0.2405688315629959 -0.3451509475708008 
2020-11-12 01:50:11.504376 / Iteration 2300 valid loss -0.06860312819480896 
2020-11-12 01:50:18.957128 / Iteration 2400: 0.2405688315629959 -0.3453041613101959 
2020-11-12 01:50:18.965235 / Iteration 2400 valid loss -0.0686216652393341 
2020-11-12 01:50:26.329625 / Iteration 2500: 0.2405688613653183 -0.3455873131752014 
2020-11-12 01:50:26.337333 / Iteration 2500 valid loss -0.06891709566116333 
2020-11-12 01:50:33.760538 / Iteration 2600: 0.2405688315629959 -0.3459676206111908 
2020-11-12 01:50:33.769109 / Iteration 2600 valid loss -0.06892746686935425 
2020-11-12 01:50:41.175476 / Iteration 2700: 0.2405688613653183 -0.3463667333126068 
2020-11-12 01:50:41.184258 / Iteration 2700 valid loss -0.06880462169647217 
2020-11-12 01:50:48.985259 / Iteration 2800: 0.2405688613653183 -0.34672170877456665 
2020-11-12 01:50:48.994231 / Iteration 2800 valid loss -0.06857481598854065 
2020-11-12 01:50:56.755674 / Iteration 2900: 0.2405688315629959 -0.34700584411621094 
2020-11-12 01:50:56.764575 / Iteration 2900 valid loss -0.06834721565246582 
2020-11-12 01:51:04.393967 / Iteration 3000: 0.2405688613653183 -0.34721383452415466 
2020-11-12 01:51:04.401771 / Iteration 3000 valid loss -0.06813046336174011 
2020-11-12 01:51:11.872492 / Iteration 3100: 0.2405688315629959 -0.34734711050987244 
2020-11-12 01:51:11.881119 / Iteration 3100 valid loss -0.06777864694595337 
2020-11-12 01:51:19.457567 / Iteration 3200: 0.24056881666183472 -0.3474564850330353 
2020-11-12 01:51:19.466119 / Iteration 3200 valid loss -0.0678015649318695 
2020-11-12 01:51:26.752189 / Iteration 3300: 0.24056878685951233 -0.3475417196750641 
2020-11-12 01:51:26.759966 / Iteration 3300 valid loss -0.06774002313613892 
2020-11-12 01:51:34.122659 / Iteration 3400: 0.24056881666183472 -0.3476000726222992 
2020-11-12 01:51:34.130309 / Iteration 3400 valid loss -0.06769302487373352 
2020-11-12 01:51:41.567980 / Iteration 3500: 0.2405688315629959 -0.34764808416366577 
2020-11-12 01:51:41.576719 / Iteration 3500 valid loss -0.0676824152469635 
2020-11-12 01:51:49.075184 / Iteration 3600: 0.2405688762664795 -0.3476581573486328 
2020-11-12 01:51:49.083960 / Iteration 3600 valid loss -0.06766310334205627 
2020-11-12 01:51:56.495341 / Iteration 3700: 0.2405688762664795 -0.34772610664367676 
2020-11-12 01:51:56.503832 / Iteration 3700 valid loss -0.06766626238822937 
2020-11-12 01:52:04.088872 / Iteration 3800: 0.24056890606880188 -0.3477637767791748 
2020-11-12 01:52:04.096713 / Iteration 3800 valid loss -0.06773144006729126 
2020-11-12 01:52:11.632437 / Iteration 3900: 0.24056890606880188 -0.34780076146125793 
2020-11-12 01:52:11.639886 / Iteration 3900 valid loss -0.06774282455444336 
2020-11-12 01:52:19.158667 / Iteration 4000: 0.24056890606880188 -0.34783878922462463 
2020-11-12 01:52:19.167358 / Iteration 4000 valid loss -0.06775188446044922 
2020-11-12 01:52:26.567722 / Iteration 4100: 0.2405688613653183 -0.3479323089122772 
2020-11-12 01:52:26.576032 / Iteration 4100 valid loss -0.06781831383705139 
2020-11-12 01:52:33.991686 / Iteration 4200: 0.2405688762664795 -0.34823596477508545 
2020-11-12 01:52:33.999427 / Iteration 4200 valid loss -0.06739276647567749 
2020-11-12 01:52:41.389712 / Iteration 4300: 0.2405688762664795 -0.348438560962677 
2020-11-12 01:52:41.398360 / Iteration 4300 valid loss -0.0673866868019104 
2020-11-12 01:52:48.834038 / Iteration 4400: 0.2405688613653183 -0.34886306524276733 
2020-11-12 01:52:48.843182 / Iteration 4400 valid loss -0.06785345077514648 
2020-11-12 01:52:56.290508 / Iteration 4500: 0.2405688613653183 -0.35018301010131836 
2020-11-12 01:52:56.298858 / Iteration 4500 valid loss -0.06739440560340881 
2020-11-12 01:53:03.844002 / Iteration 4600: 0.24056890606880188 -0.3508189618587494 
2020-11-12 01:53:03.852583 / Iteration 4600 valid loss -0.06654107570648193 
2020-11-12 01:53:11.259195 / Iteration 4700: 0.2405688613653183 -0.3510447144508362 
2020-11-12 01:53:11.267889 / Iteration 4700 valid loss -0.0657360851764679 
2020-11-12 01:53:18.608087 / Iteration 4800: 0.2405688315629959 -0.35113775730133057 
2020-11-12 01:53:18.616037 / Iteration 4800 valid loss -0.0653870701789856 
2020-11-12 01:53:25.941033 / Iteration 4900: 0.2405688762664795 -0.35120341181755066 
2020-11-12 01:53:25.950049 / Iteration 4900 valid loss -0.06521493196487427 
2020-11-12 01:53:33.266402 / Iteration 5000: 0.2405688762664795 -0.3512652516365051 
2020-11-12 01:53:33.274284 / Iteration 5000 valid loss -0.06513276696205139 
2020-11-12 01:53:40.649671 / Iteration 5100: 0.24056881666183472 -0.35132890939712524 
2020-11-12 01:53:40.659118 / Iteration 5100 valid loss -0.06508809328079224 
2020-11-12 01:53:48.100136 / Iteration 5200: 0.2405688762664795 -0.35217440128326416 
2020-11-12 01:53:48.108041 / Iteration 5200 valid loss -0.06601572036743164 
2020-11-12 01:53:55.479946 / Iteration 5300: 0.2405688315629959 -0.35273441672325134 
2020-11-12 01:53:55.487991 / Iteration 5300 valid loss -0.06517601013183594 
2020-11-12 01:54:03.011937 / Iteration 5400: 0.24056890606880188 -0.3529788851737976 
2020-11-12 01:54:03.020829 / Iteration 5400 valid loss -0.0658525824546814 
2020-11-12 01:54:10.392115 / Iteration 5500: 0.2405688613653183 -0.35325801372528076 
2020-11-12 01:54:10.399696 / Iteration 5500 valid loss -0.06727761030197144 
2020-11-12 01:54:17.896336 / Iteration 5600: 0.24056890606880188 -0.35362499952316284 
2020-11-12 01:54:17.905014 / Iteration 5600 valid loss -0.06885778903961182 
2020-11-12 01:54:25.355976 / Iteration 5700: 0.2405688762664795 -0.35406675934791565 
2020-11-12 01:54:25.364985 / Iteration 5700 valid loss -0.07071980834007263 
2020-11-12 01:54:25.366534 / saving best-val-loss model 
2020-11-12 02:00:23.589859 / Namespace(activation='ReLU', atoms=[0.0], batch_size=25000, comet=True, data='lalonde', dataroot='/home/mila/r/raghupas/causal-benchmark/datasets', dim_h=4, dist='SigmoidFlow', dist_args=['ndim=2', 'base_distribution=uniform'], early_stop=True, eval=True, grad_norm=inf, ignore_w=False, lr=0.01, n_hidden_layers=1, num_epochs=10000, num_univariate_tests=100, overwrite_reload='', patience=None, saveroot='save', seed=123, test_prop=0.4, test_size=None, train=True, train_prop=0.5, val_prop=0.1, w_transform='Normalize', y_transform='Normalize') 
2020-11-12 02:00:23.592431 / getting data: lalonde 
2020-11-12 02:00:44.567838 / comet url: https://www.comet.ml/sunandr/causal-benchmark/8b45ee0f545d459fb5deef319a6bf5bd 
2020-11-12 02:00:44.571155 / ate: None 
2020-11-12 02:00:44.573008 / <models.distributions.distributions.MixedDistribution object at 0x7f93a3884390> 
2020-11-12 02:00:44.574920 / {'n_hidden_layers': 1, 'dim_h': 4, 'activation': ReLU()} 
2020-11-12 02:00:44.576819 / {'batch_size': 25000, 'lr': 0.01, 'num_epochs': 10000, 'verbose': True, 'print_every_iters': 100, 'optim': <class 'torch.optim.adam.Adam'>, 'eval_every': 100, 'plot_every': 100, 'p_every': 100, 'optim_args': {}} 
2020-11-12 02:00:46.965673 / Iteration 100: 0.21214598417282104 -0.24547943472862244 
2020-11-12 02:00:46.973073 / Iteration 100 valid loss 0.031138896942138672 
2020-11-12 02:00:46.974559 / saving best-val-loss model 
2020-11-12 02:00:53.474902 / Iteration 200: 0.1593914031982422 -0.37020111083984375 
2020-11-12 02:00:53.483254 / Iteration 200 valid loss -0.14869700372219086 
2020-11-12 02:00:53.484552 / saving best-val-loss model 
2020-11-12 02:01:00.042791 / Iteration 300: 0.14040641486644745 -0.6114034652709961 
2020-11-12 02:01:00.049480 / Iteration 300 valid loss -0.3839937150478363 
2020-11-12 02:01:00.051022 / saving best-val-loss model 
2020-11-12 02:01:06.771522 / Iteration 400: 0.1420125961303711 -0.663002073764801 
2020-11-12 02:01:06.778972 / Iteration 400 valid loss -0.44898903369903564 
2020-11-12 02:01:06.780518 / saving best-val-loss model 
2020-11-12 02:01:13.480428 / Iteration 500: 0.14215920865535736 -0.6895809173583984 
2020-11-12 02:01:13.486627 / Iteration 500 valid loss -0.4851379990577698 
2020-11-12 02:01:13.488070 / saving best-val-loss model 
2020-11-12 02:01:20.329176 / Iteration 600: 0.14242959022521973 -0.7053882479667664 
2020-11-12 02:01:20.338600 / Iteration 600 valid loss -0.508552074432373 
2020-11-12 02:01:20.340058 / saving best-val-loss model 
2020-11-12 02:01:27.144615 / Iteration 700: 0.14223381876945496 -0.7201803922653198 
2020-11-12 02:01:27.151397 / Iteration 700 valid loss -0.5221400260925293 
2020-11-12 02:01:27.152918 / saving best-val-loss model 
2020-11-12 02:01:33.838502 / Iteration 800: 0.14197483658790588 -0.7290751934051514 
2020-11-12 02:01:33.845408 / Iteration 800 valid loss -0.5260872840881348 
2020-11-12 02:01:33.846727 / saving best-val-loss model 
2020-11-12 02:01:40.567749 / Iteration 900: 0.1417095810174942 -0.7350112795829773 
2020-11-12 02:01:40.574952 / Iteration 900 valid loss -0.5389328002929688 
2020-11-12 02:01:40.576216 / saving best-val-loss model 
2020-11-12 02:01:47.205049 / Iteration 1000: 0.14123061299324036 -0.7397300004959106 
2020-11-12 02:01:47.212571 / Iteration 1000 valid loss -0.5417658686637878 
2020-11-12 02:01:47.214154 / saving best-val-loss model 
2020-11-12 02:01:54.051136 / Iteration 1100: 0.14114555716514587 -0.7436162829399109 
2020-11-12 02:01:54.057764 / Iteration 1100 valid loss -0.5456726551055908 
2020-11-12 02:01:54.059237 / saving best-val-loss model 
2020-11-12 02:02:00.672277 / Iteration 1200: 0.14103779196739197 -0.7469072937965393 
2020-11-12 02:02:00.678735 / Iteration 1200 valid loss -0.5484230518341064 
2020-11-12 02:02:00.680089 / saving best-val-loss model 
2020-11-12 02:02:07.531886 / Iteration 1300: 0.14097252488136292 -0.7496273517608643 
2020-11-12 02:02:07.538356 / Iteration 1300 valid loss -0.5513681173324585 
2020-11-12 02:02:07.539764 / saving best-val-loss model 
2020-11-12 02:02:14.312838 / Iteration 1400: 0.14091487228870392 -0.7523443102836609 
2020-11-12 02:02:14.319808 / Iteration 1400 valid loss -0.5522858500480652 
2020-11-12 02:02:14.321523 / saving best-val-loss model 
2020-11-12 02:02:21.100886 / Iteration 1500: 0.14088374376296997 -0.7546818256378174 
2020-11-12 02:02:21.109423 / Iteration 1500 valid loss -0.554113507270813 
2020-11-12 02:02:21.111146 / saving best-val-loss model 
2020-11-12 02:02:27.730222 / Iteration 1600: 0.14083096385002136 -0.7568109631538391 
2020-11-12 02:02:27.737275 / Iteration 1600 valid loss -0.5553895831108093 
2020-11-12 02:02:27.738984 / saving best-val-loss model 
2020-11-12 02:02:34.537164 / Iteration 1700: 0.14080104231834412 -0.7587233185768127 
2020-11-12 02:02:34.543903 / Iteration 1700 valid loss -0.5546818375587463 
2020-11-12 02:02:41.426074 / Iteration 1800: 0.140981525182724 -0.7604048252105713 
2020-11-12 02:02:41.432631 / Iteration 1800 valid loss -0.5552094578742981 
2020-11-12 02:02:48.325594 / Iteration 1900: 0.140809565782547 -0.7623312473297119 
2020-11-12 02:02:48.332182 / Iteration 1900 valid loss -0.5543376803398132 
2020-11-12 02:02:55.483756 / Iteration 2000: 0.14081580936908722 -0.765618085861206 
2020-11-12 02:02:55.490427 / Iteration 2000 valid loss -0.5574741363525391 
2020-11-12 02:02:55.492115 / saving best-val-loss model 
2020-11-12 02:03:02.189486 / Iteration 2100: 0.1408301442861557 -0.767383337020874 
2020-11-12 02:03:02.196470 / Iteration 2100 valid loss -0.5556979179382324 
2020-11-12 02:03:08.909658 / Iteration 2200: 0.14088159799575806 -0.7687749266624451 
2020-11-12 02:03:08.916253 / Iteration 2200 valid loss -0.5554313659667969 
2020-11-12 02:03:15.670346 / Iteration 2300: 0.1409304141998291 -0.770341157913208 
2020-11-12 02:03:15.677188 / Iteration 2300 valid loss -0.5543860197067261 
2020-11-12 02:03:22.511999 / Iteration 2400: 0.1409461945295334 -0.7717654705047607 
2020-11-12 02:03:22.518406 / Iteration 2400 valid loss -0.555299699306488 
2020-11-12 02:03:29.122005 / Iteration 2500: 0.14096227288246155 -0.7727068066596985 
2020-11-12 02:03:29.128485 / Iteration 2500 valid loss -0.5511634349822998 
2020-11-12 02:03:35.909407 / Iteration 2600: 0.14099235832691193 -0.7741580605506897 
2020-11-12 02:03:35.916364 / Iteration 2600 valid loss -0.5541039109230042 
2020-11-12 02:03:42.725522 / Iteration 2700: 0.14104938507080078 -0.7753261923789978 
2020-11-12 02:03:42.733309 / Iteration 2700 valid loss -0.5515936613082886 
2020-11-12 02:03:49.411307 / Iteration 2800: 0.1410834789276123 -0.7763752937316895 
2020-11-12 02:03:49.418069 / Iteration 2800 valid loss -0.5501818060874939 
2020-11-12 02:03:56.084606 / Iteration 2900: 0.14113086462020874 -0.7773913741111755 
2020-11-12 02:03:56.091484 / Iteration 2900 valid loss -0.5475683808326721 
2020-11-12 02:04:02.870277 / Iteration 3000: 0.14117436110973358 -0.7782827019691467 
2020-11-12 02:04:02.876670 / Iteration 3000 valid loss -0.5458963513374329 
2020-11-14 15:10:39.215982 / Namespace(activation='ReLU', atoms=[], batch_size=8192, comet=True, data='lbidd_log_50k', dataroot='/home/mila/r/raghupas/causal-benchmark/datasets', dim_h=8, dist='LogNormal', dist_args=[], early_stop=True, eval=True, grad_norm=inf, ignore_w=False, lr=0.001, n_hidden_layers=1, num_epochs=10000, num_univariate_tests=100, overwrite_reload='', patience=None, saveroot='save', seed=123, test_prop=0.4, test_size=None, train=True, train_prop=0.5, val_prop=0.1, w_transform='Standardize', y_transform='Standardize') 
2020-11-14 15:10:39.219094 / getting data: lbidd_log_50k 
2020-11-14 15:11:00.686783 / comet url: https://www.comet.ml/sunandr/causal-benchmark/105ab65716f9409a94ce52802dceea14 
2020-11-14 15:11:00.688277 / ate: 0.054972401602067546 
2020-11-14 15:11:00.689603 / <models.distributions.distributions.LogNormal object at 0x7efdf1d5b990> 
2020-11-14 15:11:00.691443 / {'n_hidden_layers': 1, 'dim_h': 8, 'activation': ReLU()} 
2020-11-14 15:11:00.693414 / {'batch_size': 8192, 'lr': 0.001, 'num_epochs': 10000, 'verbose': True, 'print_every_iters': 100, 'optim': <class 'torch.optim.adam.Adam'>, 'eval_every': 100, 'plot_every': 100, 'p_every': 100, 'optim_args': {}} 
2020-11-14 15:11:13.203861 / Iteration 100: nan nan 
2020-11-14 15:11:13.312247 / Iteration 100 valid loss nan 
2020-11-14 15:12:41.576851 / Namespace(activation='ReLU', atoms=[], batch_size=8192, comet=True, data='lbidd_log_50k', dataroot='/home/mila/r/raghupas/causal-benchmark/datasets', dim_h=8, dist='LogNormal', dist_args=[], early_stop=True, eval=True, grad_norm=inf, ignore_w=False, lr=0.001, n_hidden_layers=1, num_epochs=10000, num_univariate_tests=100, overwrite_reload='', patience=None, saveroot='save', seed=123, test_prop=0.4, test_size=None, train=True, train_prop=0.5, val_prop=0.1, w_transform='Standardize', y_transform='Standardize') 
2020-11-14 15:12:41.579426 / getting data: lbidd_log_50k 
2020-11-14 15:13:03.076995 / comet url: https://www.comet.ml/sunandr/causal-benchmark/ffc8094e0c4b4cedbadec5a94634c9a4 
2020-11-14 15:13:03.078473 / ate: 0.054972401602067546 
2020-11-14 15:13:03.079807 / <models.distributions.distributions.LogNormal object at 0x7f6c0c594e90> 
2020-11-14 15:13:03.081322 / {'n_hidden_layers': 1, 'dim_h': 8, 'activation': ReLU()} 
2020-11-14 15:13:03.082855 / {'batch_size': 8192, 'lr': 0.001, 'num_epochs': 10000, 'verbose': True, 'print_every_iters': 100, 'optim': <class 'torch.optim.adam.Adam'>, 'eval_every': 100, 'plot_every': 1000, 'p_every': 100, 'optim_args': {}} 
2020-11-14 15:13:16.368290 / Iteration 100: nan nan 
2020-11-14 15:13:16.447295 / Iteration 100 valid loss nan 
2020-11-14 15:14:30.801658 / Namespace(activation='ReLU', atoms=[], batch_size=25000, comet=True, data='lbidd_log_50k', dataroot='/home/mila/r/raghupas/causal-benchmark/datasets', dim_h=4, dist='SigmoidFlow', dist_args=['ndim=8', 'base_distribution=normal'], early_stop=True, eval=True, grad_norm=inf, ignore_w=False, lr=0.001, n_hidden_layers=1, num_epochs=10000, num_univariate_tests=100, overwrite_reload='', patience=None, saveroot='save', seed=123, test_prop=0.4, test_size=None, train=True, train_prop=0.5, val_prop=0.1, w_transform='Standardize', y_transform='Standardize') 
2020-11-14 15:14:30.804429 / getting data: lbidd_log_50k 
2020-11-14 15:14:53.536231 / comet url: https://www.comet.ml/sunandr/causal-benchmark/9fa27de74b1842c8b857b4cca833eefc 
2020-11-14 15:14:53.538111 / ate: 0.054972401602067546 
2020-11-14 15:14:53.539942 / <models.distributions.distributions.SigmoidFlow object at 0x7fa593a96a10> ndim:8 base_distribution:normal 
2020-11-14 15:14:53.541896 / {'n_hidden_layers': 1, 'dim_h': 4, 'activation': ReLU()} 
2020-11-14 15:14:53.543711 / {'batch_size': 25000, 'lr': 0.001, 'num_epochs': 10000, 'verbose': True, 'print_every_iters': 100, 'optim': <class 'torch.optim.adam.Adam'>, 'eval_every': 100, 'plot_every': 100, 'p_every': 100, 'optim_args': {}} 
2020-11-14 15:15:47.233835 / Iteration 100: 0.6880795359611511 1.3990732431411743 
2020-11-14 15:15:47.309156 / Iteration 100 valid loss 2.0957210063934326 
2020-11-14 15:15:47.310821 / saving best-val-loss model 
2020-11-14 15:16:47.140181 / Iteration 200: 0.6807884573936462 1.2338511943817139 
2020-11-14 15:16:47.216538 / Iteration 200 valid loss 1.9346822500228882 
2020-11-14 15:16:47.217948 / saving best-val-loss model 
2020-11-14 15:17:51.458611 / Iteration 300: 0.6681150197982788 1.0236101150512695 
2020-11-14 15:17:51.528653 / Iteration 300 valid loss 1.7160701751708984 
2020-11-14 15:17:51.529900 / saving best-val-loss model 
2020-11-14 15:18:52.126658 / Iteration 400: 0.6650285720825195 0.7060832977294922 
2020-11-14 15:18:52.237886 / Iteration 400 valid loss 1.3931047916412354 
2020-11-14 15:18:52.239179 / saving best-val-loss model 
2020-11-14 15:19:54.397866 / Iteration 500: 0.6602915525436401 0.4295165538787842 
2020-11-14 15:19:54.490403 / Iteration 500 valid loss 1.119264841079712 
2020-11-14 15:19:54.491873 / saving best-val-loss model 
2020-11-14 15:20:57.890134 / Iteration 600: 0.657677173614502 0.22315296530723572 
2020-11-14 15:20:57.999821 / Iteration 600 valid loss 0.9113163352012634 
2020-11-14 15:20:58.001374 / saving best-val-loss model 
2020-11-14 15:22:02.375028 / Iteration 700: 0.656191885471344 0.04692131280899048 
2020-11-14 15:22:02.493746 / Iteration 700 valid loss 0.7378658056259155 
2020-11-14 15:22:02.495094 / saving best-val-loss model 
2020-11-14 15:23:08.786099 / Iteration 800: 0.6547956466674805 -0.15374387800693512 
2020-11-14 15:23:08.866070 / Iteration 800 valid loss 0.551584780216217 
2020-11-14 15:23:08.867239 / saving best-val-loss model 
2020-11-14 15:24:12.922025 / Iteration 900: 0.6542791724205017 -0.30614638328552246 
2020-11-14 15:24:12.993294 / Iteration 900 valid loss 0.41426748037338257 
2020-11-14 15:24:12.994472 / saving best-val-loss model 
2020-11-14 15:25:18.467092 / Iteration 1000: 0.65416419506073 -0.41067004203796387 
2020-11-14 15:25:18.574167 / Iteration 1000 valid loss 0.31905218958854675 
2020-11-14 15:25:18.575699 / saving best-val-loss model 
2020-11-14 15:26:21.506212 / Iteration 1100: 0.6541557908058167 -0.48200613260269165 
2020-11-14 15:26:21.617247 / Iteration 1100 valid loss 0.2567119598388672 
2020-11-14 15:26:21.618955 / saving best-val-loss model 
2020-11-14 15:27:25.629362 / Iteration 1200: 0.6541206240653992 -0.5279372334480286 
2020-11-14 15:27:25.747928 / Iteration 1200 valid loss 0.20931376516819 
2020-11-14 15:27:25.749589 / saving best-val-loss model 
2020-11-14 15:28:29.810832 / Iteration 1300: 0.6541434526443481 -0.5644550919532776 
2020-11-14 15:28:29.886996 / Iteration 1300 valid loss 0.17156898975372314 
2020-11-14 15:28:29.888572 / saving best-val-loss model 
2020-11-14 15:29:32.970086 / Iteration 1400: 0.6541122794151306 -0.6038742661476135 
2020-11-14 15:29:33.043929 / Iteration 1400 valid loss 0.13377058506011963 
2020-11-14 15:29:33.045197 / saving best-val-loss model 
2020-11-14 15:30:39.252246 / Iteration 1500: 0.6541144251823425 -0.6267601847648621 
2020-11-14 15:30:39.370328 / Iteration 1500 valid loss 0.11531294137239456 
2020-11-14 15:30:39.371798 / saving best-val-loss model 
2020-11-14 15:31:45.967021 / Iteration 1600: 0.6541014313697815 -0.6444047689437866 
2020-11-14 15:31:46.055521 / Iteration 1600 valid loss 0.10062527656555176 
2020-11-14 15:31:46.057247 / saving best-val-loss model 
2020-11-14 15:32:50.643278 / Iteration 1700: 0.654086709022522 -0.6593258380889893 
2020-11-14 15:32:50.766671 / Iteration 1700 valid loss 0.0874820351600647 
2020-11-14 15:32:50.768225 / saving best-val-loss model 
2020-11-14 15:33:57.046126 / Iteration 1800: 0.6540884971618652 -0.6722884178161621 
2020-11-14 15:33:57.153747 / Iteration 1800 valid loss 0.07756829261779785 
2020-11-14 15:33:57.155478 / saving best-val-loss model 
2020-11-14 15:35:04.280178 / Iteration 1900: 0.6540687084197998 -0.6875910758972168 
2020-11-14 15:35:04.359325 / Iteration 1900 valid loss 0.06580168008804321 
2020-11-14 15:35:04.360886 / saving best-val-loss model 
2020-11-14 15:36:10.901105 / Iteration 2000: 0.6540402770042419 -0.7036415338516235 
2020-11-14 15:36:10.975800 / Iteration 2000 valid loss 0.051509320735931396 
2020-11-14 15:36:10.977071 / saving best-val-loss model 
2020-11-14 15:37:16.857670 / Iteration 2100: 0.6540184617042542 -0.7210811972618103 
2020-11-14 15:37:16.986550 / Iteration 2100 valid loss 0.036132216453552246 
2020-11-14 15:37:16.987747 / saving best-val-loss model 
2020-11-14 15:38:22.829216 / Iteration 2200: 0.6540240049362183 -0.738368809223175 
2020-11-14 15:38:22.911992 / Iteration 2200 valid loss 0.020527780055999756 
2020-11-14 15:38:22.913643 / saving best-val-loss model 
2020-11-14 15:39:24.198874 / Iteration 2300: 0.653982400894165 -0.7637043595314026 
2020-11-14 15:39:24.313436 / Iteration 2300 valid loss -0.00699615478515625 
2020-11-14 15:39:24.314840 / saving best-val-loss model 
2020-11-14 15:40:29.280761 / Iteration 2400: 0.6539552211761475 -0.7884806990623474 
2020-11-14 15:40:29.404166 / Iteration 2400 valid loss -0.03277558088302612 
2020-11-14 15:40:29.405558 / saving best-val-loss model 
2020-11-14 15:41:36.545901 / Iteration 2500: 0.653938889503479 -0.8086128830909729 
2020-11-14 15:41:36.636054 / Iteration 2500 valid loss -0.052647173404693604 
2020-11-14 15:41:36.637719 / saving best-val-loss model 
2020-11-14 15:42:42.921561 / Iteration 2600: 0.6539263725280762 -0.8285902142524719 
2020-11-14 15:42:42.994188 / Iteration 2600 valid loss -0.06992757320404053 
2020-11-14 15:42:42.995478 / saving best-val-loss model 
2020-11-14 15:43:47.211236 / Iteration 2700: 0.6539080739021301 -0.8426215052604675 
2020-11-14 15:43:47.289780 / Iteration 2700 valid loss -0.08216303586959839 
2020-11-14 15:43:47.291682 / saving best-val-loss model 
2020-11-14 15:44:52.692860 / Iteration 2800: 0.6538889408111572 -0.8555469512939453 
2020-11-14 15:44:52.802755 / Iteration 2800 valid loss -0.0924978256225586 
2020-11-14 15:44:52.804217 / saving best-val-loss model 
2020-11-14 15:56:24.247536 / Namespace(activation='ReLU', atoms=[], batch_size=25000, comet=True, data='lbidd_log_50k', dataroot='/home/mila/r/raghupas/causal-benchmark/datasets', dim_h=4, dist='SigmoidFlow', dist_args=['ndim=8', 'base_distribution=normal'], early_stop=True, eval=True, grad_norm=inf, ignore_w=False, lr=0.001, n_hidden_layers=1, num_epochs=10000, num_univariate_tests=100, overwrite_reload='', patience=None, saveroot='save', seed=123, test_prop=0.4, test_size=None, train=True, train_prop=0.5, val_prop=0.1, w_transform='Standardize', y_transform='Normalize') 
2020-11-14 15:56:24.250069 / getting data: lbidd_log_50k 
2020-11-14 15:56:46.156664 / comet url: https://www.comet.ml/sunandr/causal-benchmark/a1117ea982264fcfbcf46240f8c29a7f 
2020-11-14 15:56:46.158225 / ate: 0.054972401602067546 
2020-11-14 15:56:46.159895 / <models.distributions.distributions.SigmoidFlow object at 0x7f7055669dd0> ndim:8 base_distribution:normal 
2020-11-14 15:56:46.161496 / {'n_hidden_layers': 1, 'dim_h': 4, 'activation': ReLU()} 
2020-11-14 15:56:46.163112 / {'batch_size': 25000, 'lr': 0.001, 'num_epochs': 10000, 'verbose': True, 'print_every_iters': 100, 'optim': <class 'torch.optim.adam.Adam'>, 'eval_every': 100, 'plot_every': 100, 'p_every': 100, 'optim_args': {}} 
2020-11-14 15:57:41.417139 / Iteration 100: 0.6865424513816833 0.9565437436103821 
2020-11-14 15:57:41.529150 / Iteration 100 valid loss 1.6206625699996948 
2020-11-14 15:57:41.530869 / saving best-val-loss model 
2020-11-14 15:58:43.661861 / Iteration 200: 0.6842060685157776 -0.2910490930080414 
2020-11-14 15:58:43.768636 / Iteration 200 valid loss 0.3893337845802307 
2020-11-14 15:58:43.770165 / saving best-val-loss model 
2020-11-14 15:59:48.187741 / Iteration 300: 0.6864205598831177 -0.6401333212852478 
2020-11-14 15:59:48.265260 / Iteration 300 valid loss 0.06452131271362305 
2020-11-14 15:59:48.266581 / saving best-val-loss model 
2020-11-14 16:00:49.028499 / Iteration 400: 0.6874001026153564 -0.7165974378585815 
2020-11-14 16:00:49.102805 / Iteration 400 valid loss -0.006505608558654785 
2020-11-14 16:00:49.104013 / saving best-val-loss model 
2020-11-14 16:01:50.973651 / Iteration 500: 0.6864555478096008 -0.749725341796875 
2020-11-14 16:01:51.080263 / Iteration 500 valid loss -0.037424683570861816 
2020-11-14 16:01:51.081401 / saving best-val-loss model 
2020-11-14 16:02:51.674951 / Iteration 600: 0.6816722750663757 -0.7739017009735107 
2020-11-14 16:02:51.748459 / Iteration 600 valid loss -0.0651618242263794 
2020-11-14 16:02:51.749763 / saving best-val-loss model 
2020-11-14 16:03:53.371516 / Iteration 700: 0.6738700866699219 -0.8381928205490112 
2020-11-14 16:03:53.448626 / Iteration 700 valid loss -0.13438791036605835 
2020-11-14 16:03:53.450146 / saving best-val-loss model 
2020-11-14 16:04:56.268321 / Iteration 800: 0.6650480628013611 -0.8659639954566956 
2020-11-14 16:04:56.345682 / Iteration 800 valid loss -0.1660008430480957 
2020-11-14 16:04:56.347097 / saving best-val-loss model 
2020-11-14 16:05:57.396287 / Iteration 900: 0.65950608253479 -0.8844906091690063 
2020-11-14 16:05:57.480649 / Iteration 900 valid loss -0.18676602840423584 
2020-11-14 16:05:57.481852 / saving best-val-loss model 
2020-11-14 16:07:00.373756 / Iteration 1000: 0.6568979620933533 -0.9068665504455566 
2020-11-14 16:07:00.492002 / Iteration 1000 valid loss -0.2107866406440735 
2020-11-14 16:07:00.493415 / saving best-val-loss model 
2020-11-14 16:08:02.393743 / Iteration 1100: 0.6555507779121399 -0.9485698342323303 
2020-11-14 16:08:02.471039 / Iteration 1100 valid loss -0.25562238693237305 
2020-11-14 16:08:02.472480 / saving best-val-loss model 
2020-11-14 16:09:03.638367 / Iteration 1200: 0.6550307869911194 -1.367793321609497 
2020-11-14 16:09:03.763723 / Iteration 1200 valid loss -0.6815341711044312 
2020-11-14 16:09:03.765004 / saving best-val-loss model 
2020-11-14 16:10:05.634589 / Iteration 1300: 0.656157910823822 -1.824810266494751 
2020-11-14 16:10:05.725237 / Iteration 1300 valid loss -1.1314706802368164 
2020-11-14 16:10:05.726424 / saving best-val-loss model 
2020-11-14 16:11:07.637790 / Iteration 1400: 0.6577275991439819 -1.9692786931991577 
2020-11-14 16:11:07.725184 / Iteration 1400 valid loss -1.2824273109436035 
2020-11-14 16:11:07.726457 / saving best-val-loss model 
2020-11-14 16:12:12.167777 / Iteration 1500: 0.6579746007919312 -2.0642409324645996 
2020-11-14 16:12:12.243278 / Iteration 1500 valid loss -1.3787108659744263 
2020-11-14 16:12:12.244757 / saving best-val-loss model 
2020-11-14 16:13:18.809494 / Iteration 1600: 0.6582684516906738 -2.2340810298919678 
2020-11-14 16:13:18.920616 / Iteration 1600 valid loss -1.5481696128845215 
2020-11-14 16:13:18.922545 / saving best-val-loss model 
2020-11-14 16:14:23.576798 / Iteration 1700: 0.6590375900268555 -2.3890442848205566 
2020-11-14 16:14:23.660332 / Iteration 1700 valid loss -1.694410800933838 
2020-11-14 16:14:23.661662 / saving best-val-loss model 
2020-11-14 16:15:28.690531 / Iteration 1800: 0.6614454984664917 -2.492446184158325 
2020-11-14 16:15:28.762556 / Iteration 1800 valid loss -1.7922176122665405 
2020-11-14 16:15:28.763927 / saving best-val-loss model 
2020-11-14 16:16:30.002005 / Iteration 1900: 0.6623651385307312 -2.5795986652374268 
2020-11-14 16:16:30.113377 / Iteration 1900 valid loss -1.878466248512268 
2020-11-14 16:16:30.114549 / saving best-val-loss model 
2020-11-14 16:17:35.099187 / Iteration 2000: 0.6578553318977356 -2.6595568656921387 
2020-11-14 16:17:35.188304 / Iteration 2000 valid loss -1.9602500200271606 
2020-11-14 16:17:35.189735 / saving best-val-loss model 
2020-11-14 16:18:44.343450 / Iteration 2100: 0.6555418968200684 -2.6956796646118164 
2020-11-14 16:18:44.460620 / Iteration 2100 valid loss -1.9942657947540283 
2020-11-14 16:18:44.462049 / saving best-val-loss model 
2020-11-14 16:19:51.819565 / Iteration 2200: 0.6550924181938171 -2.719311237335205 
2020-11-14 16:19:51.938196 / Iteration 2200 valid loss -2.0173816680908203 
2020-11-14 16:19:51.939502 / saving best-val-loss model 
2020-11-14 16:20:57.229090 / Iteration 2300: 0.6549338102340698 -2.7354323863983154 
2020-11-14 16:20:57.348559 / Iteration 2300 valid loss -2.032637357711792 
2020-11-14 16:20:57.350409 / saving best-val-loss model 
2020-11-14 16:22:03.194260 / Iteration 2400: 0.654826819896698 -2.7471091747283936 
2020-11-14 16:22:03.314092 / Iteration 2400 valid loss -2.0440869331359863 
2020-11-14 16:22:03.315422 / saving best-val-loss model 
2020-11-14 16:23:10.251905 / Iteration 2500: 0.6547669768333435 -2.7570927143096924 
2020-11-14 16:23:10.385036 / Iteration 2500 valid loss -2.0531368255615234 
2020-11-14 16:23:10.386356 / saving best-val-loss model 
2020-11-14 16:24:19.692956 / Iteration 2600: 0.6547130346298218 -2.76461124420166 
2020-11-14 16:24:19.823981 / Iteration 2600 valid loss -2.061168670654297 
2020-11-14 16:24:19.825507 / saving best-val-loss model 
2020-11-14 16:25:30.158245 / Iteration 2700: 0.6546786427497864 -2.7714765071868896 
2020-11-14 16:25:30.275529 / Iteration 2700 valid loss -2.0676703453063965 
2020-11-14 16:25:30.276744 / saving best-val-loss model 
2020-11-14 16:26:37.389043 / Iteration 2800: 0.6546531915664673 -2.7774064540863037 
2020-11-14 16:26:37.502010 / Iteration 2800 valid loss -2.072829008102417 
2020-11-14 16:26:37.503696 / saving best-val-loss model 
2020-11-14 16:27:44.234777 / Iteration 2900: 0.6546797156333923 -2.7864019870758057 
2020-11-14 16:27:44.366005 / Iteration 2900 valid loss -2.0808308124542236 
2020-11-14 16:27:44.367170 / saving best-val-loss model 
2020-11-14 16:28:51.139134 / Iteration 3000: 0.6547424793243408 -2.7933034896850586 
2020-11-14 16:28:51.227684 / Iteration 3000 valid loss -2.0869696140289307 
2020-11-14 16:28:51.228974 / saving best-val-loss model 
2020-11-14 16:30:00.952145 / Iteration 3100: 0.654742419719696 -2.7990763187408447 
2020-11-14 16:30:01.065579 / Iteration 3100 valid loss -2.0921287536621094 
2020-11-14 16:30:01.066900 / saving best-val-loss model 
2020-11-14 16:31:09.259656 / Iteration 3200: 0.6547532081604004 -2.8039114475250244 
2020-11-14 16:31:09.346499 / Iteration 3200 valid loss -2.0963385105133057 
2020-11-14 16:31:09.347875 / saving best-val-loss model 
2020-11-14 16:32:16.329180 / Iteration 3300: 0.6547577977180481 -2.80879807472229 
2020-11-14 16:32:16.413015 / Iteration 3300 valid loss -2.1011459827423096 
2020-11-14 16:32:16.414237 / saving best-val-loss model 
2020-11-14 16:33:24.533777 / Iteration 3400: 0.6547788381576538 -2.813558340072632 
2020-11-14 16:33:24.662597 / Iteration 3400 valid loss -2.106031656265259 
2020-11-14 16:33:24.664148 / saving best-val-loss model 
2020-11-14 16:34:32.873468 / Iteration 3500: 0.6548094749450684 -2.8183634281158447 
2020-11-14 16:34:32.960386 / Iteration 3500 valid loss -2.1105659008026123 
2020-11-14 16:34:32.961534 / saving best-val-loss model 
2020-11-14 16:35:40.920073 / Iteration 3600: 0.654837965965271 -2.823776960372925 
2020-11-14 16:35:41.026464 / Iteration 3600 valid loss -2.116442918777466 
2020-11-14 16:35:41.027892 / saving best-val-loss model 
2020-11-14 16:36:49.795386 / Iteration 3700: 0.6548774838447571 -2.829887866973877 
2020-11-14 16:36:49.922434 / Iteration 3700 valid loss -2.122910976409912 
2020-11-14 16:36:49.923838 / saving best-val-loss model 
2020-11-14 16:37:58.548418 / Iteration 3800: 0.6549171805381775 -2.836291790008545 
2020-11-14 16:37:58.631343 / Iteration 3800 valid loss -2.1301286220550537 
2020-11-14 16:37:58.632585 / saving best-val-loss model 
2020-11-14 16:39:05.908567 / Iteration 3900: 0.6549916863441467 -2.8446197509765625 
2020-11-14 16:39:06.030189 / Iteration 3900 valid loss -2.1391990184783936 
2020-11-14 16:39:06.031543 / saving best-val-loss model 
2020-11-14 16:40:13.870614 / Iteration 4000: 0.6550596952438354 -2.8529090881347656 
2020-11-14 16:40:13.994248 / Iteration 4000 valid loss -2.1477034091949463 
2020-11-14 16:40:13.995921 / saving best-val-loss model 
2020-11-14 16:41:21.979349 / Iteration 4100: 0.6551029682159424 -2.8595969676971436 
2020-11-14 16:41:22.068121 / Iteration 4100 valid loss -2.1544206142425537 
2020-11-14 16:41:22.069814 / saving best-val-loss model 
2020-11-14 16:42:28.206432 / Iteration 4200: 0.6551678776741028 -2.867173671722412 
2020-11-14 16:42:28.293092 / Iteration 4200 valid loss -2.160306453704834 
2020-11-14 16:42:28.294409 / saving best-val-loss model 
2020-11-14 16:43:33.960087 / Iteration 4300: 0.6552127599716187 -2.873749256134033 
2020-11-14 16:43:34.040891 / Iteration 4300 valid loss -2.1675775051116943 
2020-11-14 16:43:34.042148 / saving best-val-loss model 
2020-11-14 16:44:43.653046 / Iteration 4400: 0.6552402377128601 -2.8799893856048584 
2020-11-14 16:44:43.792457 / Iteration 4400 valid loss -2.1735079288482666 
2020-11-14 16:44:43.793868 / saving best-val-loss model 
2020-11-14 16:45:51.080441 / Iteration 4500: 0.655081033706665 -2.885316848754883 
2020-11-14 16:45:51.204133 / Iteration 4500 valid loss -2.179006814956665 
2020-11-14 16:45:51.205735 / saving best-val-loss model 
2020-11-14 16:46:58.103056 / Iteration 4600: 0.6536926627159119 -2.8925108909606934 
2020-11-14 16:46:58.185846 / Iteration 4600 valid loss -2.187525749206543 
2020-11-14 16:46:58.187173 / saving best-val-loss model 
2020-11-14 16:48:35.206698 / Namespace(activation='ReLU', atoms=[], batch_size=25000, comet=True, data='lbidd_log_50k', dataroot='/home/mila/r/raghupas/causal-benchmark/datasets', dim_h=4, dist='SigmoidFlow', dist_args=['ndim=8', 'base_distribution=normal'], early_stop=True, eval=True, grad_norm=inf, ignore_w=False, lr=0.01, n_hidden_layers=1, num_epochs=10000, num_univariate_tests=100, overwrite_reload='', patience=None, saveroot='save', seed=123, test_prop=0.4, test_size=None, train=True, train_prop=0.5, val_prop=0.1, w_transform='Normalize', y_transform='Normalize') 
2020-11-14 16:48:35.209146 / getting data: lbidd_log_50k 
2020-11-14 16:48:56.761583 / comet url: https://www.comet.ml/sunandr/causal-benchmark/c3e94d94f6614f298042d798451f9aa7 
2020-11-14 16:48:56.762934 / ate: 0.054972401602067546 
2020-11-14 16:48:56.764292 / <models.distributions.distributions.SigmoidFlow object at 0x7f937f49b090> ndim:8 base_distribution:normal 
2020-11-14 16:48:56.765705 / {'n_hidden_layers': 1, 'dim_h': 4, 'activation': ReLU()} 
2020-11-14 16:48:56.767354 / {'batch_size': 25000, 'lr': 0.01, 'num_epochs': 10000, 'verbose': True, 'print_every_iters': 100, 'optim': <class 'torch.optim.adam.Adam'>, 'eval_every': 100, 'plot_every': 100, 'p_every': 100, 'optim_args': {}} 
2020-11-14 16:49:49.737127 / Iteration 100: 0.6850168704986572 -0.6360487341880798 
2020-11-14 16:49:49.809511 / Iteration 100 valid loss 0.06551748514175415 
2020-11-14 16:49:49.810828 / saving best-val-loss model 
2020-11-14 16:50:50.605493 / Iteration 200: 0.6848717331886292 -0.6956305503845215 
2020-11-14 16:50:50.688807 / Iteration 200 valid loss 0.00736004114151001 
2020-11-14 16:50:50.690410 / saving best-val-loss model 
2020-11-14 16:51:52.092865 / Iteration 300: 0.6846945881843567 -0.702520489692688 
2020-11-14 16:51:52.173799 / Iteration 300 valid loss 0.0002307295799255371 
2020-11-14 16:51:52.175344 / saving best-val-loss model 
2020-11-14 16:52:52.009771 / Iteration 400: 0.684557318687439 -0.7096278071403503 
2020-11-14 16:52:52.080587 / Iteration 400 valid loss -0.005274415016174316 
2020-11-14 16:52:52.081863 / saving best-val-loss model 
2020-11-14 16:53:53.304465 / Iteration 500: 0.6841626763343811 -0.7137575745582581 
2020-11-14 16:53:53.422104 / Iteration 500 valid loss -0.010799288749694824 
2020-11-14 16:53:53.423339 / saving best-val-loss model 
2020-11-14 16:54:54.596711 / Iteration 600: 0.6822858452796936 -0.7197214961051941 
2020-11-14 16:54:54.669514 / Iteration 600 valid loss -0.019646108150482178 
2020-11-14 16:54:54.670749 / saving best-val-loss model 
2020-11-14 16:55:57.137392 / Iteration 700: 0.6719580292701721 -0.737514078617096 
2020-11-14 16:55:57.211547 / Iteration 700 valid loss -0.04813331365585327 
2020-11-14 16:55:57.212683 / saving best-val-loss model 
2020-11-14 16:56:59.206721 / Iteration 800: 0.658992350101471 -0.7604231238365173 
2020-11-14 16:56:59.316091 / Iteration 800 valid loss -0.08229511976242065 
2020-11-14 16:56:59.317529 / saving best-val-loss model 
2020-11-14 16:58:01.412360 / Iteration 900: 0.65462327003479 -0.8196248412132263 
2020-11-14 16:58:01.501381 / Iteration 900 valid loss -0.14056819677352905 
2020-11-14 16:58:01.502913 / saving best-val-loss model 
2020-11-14 16:59:04.633550 / Iteration 1000: 0.6542341113090515 -0.8342338800430298 
2020-11-14 16:59:04.716172 / Iteration 1000 valid loss -0.15200519561767578 
2020-11-14 16:59:04.718101 / saving best-val-loss model 
2020-11-14 17:00:07.058155 / Iteration 1100: 0.6543689370155334 -0.8622817397117615 
2020-11-14 17:00:07.130938 / Iteration 1100 valid loss -0.1857997179031372 
2020-11-14 17:00:07.132146 / saving best-val-loss model 
2020-11-14 17:01:09.297380 / Iteration 1200: 0.6551734209060669 -0.94996178150177 
2020-11-14 17:01:09.377377 / Iteration 1200 valid loss -0.2820321321487427 
2020-11-14 17:01:09.378518 / saving best-val-loss model 
2020-11-14 17:02:14.571551 / Iteration 1300: 0.6552425622940063 -1.0210522413253784 
2020-11-14 17:02:14.665489 / Iteration 1300 valid loss -0.3524329662322998 
2020-11-14 17:02:14.666831 / saving best-val-loss model 
2020-11-14 17:03:18.259491 / Iteration 1400: 0.6552113890647888 -1.0584580898284912 
2020-11-14 17:03:18.336795 / Iteration 1400 valid loss -0.38845890760421753 
2020-11-14 17:03:18.338093 / saving best-val-loss model 
2020-11-14 17:04:20.458404 / Iteration 1500: 0.655028223991394 -1.0873730182647705 
2020-11-14 17:04:20.537573 / Iteration 1500 valid loss -0.42015355825424194 
2020-11-14 17:04:20.539006 / saving best-val-loss model 
2020-11-14 17:05:22.820402 / Iteration 1600: 0.6550441384315491 -1.1048197746276855 
2020-11-14 17:05:22.931196 / Iteration 1600 valid loss -0.43765467405319214 
2020-11-14 17:05:22.932725 / saving best-val-loss model 
2020-11-14 17:06:26.052971 / Iteration 1700: 0.655098557472229 -1.1172609329223633 
2020-11-14 17:06:26.163403 / Iteration 1700 valid loss -0.4476040005683899 
2020-11-14 17:06:26.164642 / saving best-val-loss model 
2020-11-14 17:07:30.532228 / Iteration 1800: 0.6551659107208252 -1.1267200708389282 
2020-11-14 17:07:30.647241 / Iteration 1800 valid loss -0.45821136236190796 
2020-11-14 17:07:30.648392 / saving best-val-loss model 
2020-11-14 17:08:32.333082 / Iteration 1900: 0.6552221775054932 -1.136128544807434 
2020-11-14 17:08:32.459677 / Iteration 1900 valid loss -0.46772003173828125 
2020-11-14 17:08:32.461594 / saving best-val-loss model 
2020-11-14 17:09:33.719765 / Iteration 2000: 0.6554240584373474 -1.131409764289856 
2020-11-14 17:09:33.794554 / Iteration 2000 valid loss -0.45957332849502563 
2020-11-14 17:10:37.363335 / Iteration 2100: 0.6553743481636047 -1.1494543552398682 
2020-11-14 17:10:37.444759 / Iteration 2100 valid loss -0.47946420311927795 
2020-11-14 17:10:37.446618 / saving best-val-loss model 
2020-11-14 17:11:40.145452 / Iteration 2200: 0.655427873134613 -1.1564054489135742 
2020-11-14 17:11:40.233230 / Iteration 2200 valid loss -0.4869762659072876 
2020-11-14 17:11:40.234975 / saving best-val-loss model 
2020-11-14 17:12:41.948320 / Iteration 2300: 0.6554957032203674 -1.1621450185775757 
2020-11-14 17:12:42.028119 / Iteration 2300 valid loss -0.4923853576183319 
2020-11-14 17:12:42.029386 / saving best-val-loss model 
2020-11-14 17:13:42.048243 / Iteration 2400: 0.6555647850036621 -1.166995644569397 
2020-11-14 17:13:42.164743 / Iteration 2400 valid loss -0.49662813544273376 
2020-11-14 17:13:42.165990 / saving best-val-loss model 
2020-11-14 17:14:47.394201 / Iteration 2500: 0.6556936502456665 -1.1659353971481323 
2020-11-14 17:14:47.521607 / Iteration 2500 valid loss -0.4927298426628113 
2020-11-14 17:15:49.777045 / Iteration 2600: 0.6559088826179504 -1.1559364795684814 
2020-11-14 17:15:49.891589 / Iteration 2600 valid loss -0.4863246977329254 
2020-11-14 17:16:55.468778 / Iteration 2700: 0.6558260917663574 -1.1719553470611572 
2020-11-14 17:16:55.546983 / Iteration 2700 valid loss -0.4966552257537842 
2020-11-14 17:16:55.548819 / saving best-val-loss model 
2020-11-14 17:18:00.640706 / Iteration 2800: 0.6557838916778564 -1.1859874725341797 
2020-11-14 17:18:00.759529 / Iteration 2800 valid loss -0.5154896378517151 
2020-11-14 17:18:00.760831 / saving best-val-loss model 
2020-11-14 17:19:03.147491 / Iteration 2900: 0.6558358073234558 -1.1899902820587158 
2020-11-14 17:19:03.224816 / Iteration 2900 valid loss -0.5194055438041687 
2020-11-14 17:19:03.226061 / saving best-val-loss model 
2020-11-14 17:20:04.881723 / Iteration 3000: 0.6558853387832642 -1.1937248706817627 
2020-11-14 17:20:04.992705 / Iteration 3000 valid loss -0.5231829285621643 
2020-11-14 17:20:04.994091 / saving best-val-loss model 
2020-11-14 17:21:09.532997 / Iteration 3100: 0.6560409665107727 -1.1864428520202637 
2020-11-14 17:21:09.602675 / Iteration 3100 valid loss -0.5125873684883118 
2020-11-14 17:22:10.830151 / Iteration 3200: 0.6559799313545227 -1.2014240026474 
2020-11-14 17:22:10.949386 / Iteration 3200 valid loss -0.530488908290863 
2020-11-14 17:22:10.951064 / saving best-val-loss model 
2020-11-14 17:23:12.410397 / Iteration 3300: 0.656024158000946 -1.205397605895996 
2020-11-14 17:23:12.498186 / Iteration 3300 valid loss -0.5343371033668518 
2020-11-14 17:23:12.499630 / saving best-val-loss model 
2020-11-14 17:24:17.427076 / Iteration 3400: 0.6560677289962769 -1.2095285654067993 
2020-11-14 17:24:17.506747 / Iteration 3400 valid loss -0.5382978916168213 
2020-11-14 17:24:17.508169 / saving best-val-loss model 
2020-11-14 17:25:20.476601 / Iteration 3500: 0.6561102271080017 -1.2141505479812622 
2020-11-14 17:25:20.572708 / Iteration 3500 valid loss -0.5427186489105225 
2020-11-14 17:25:20.574044 / saving best-val-loss model 
2020-11-14 17:26:26.889601 / Iteration 3600: 0.6561524271965027 -1.2200993299484253 
2020-11-14 17:26:26.970342 / Iteration 3600 valid loss -0.5475907921791077 
2020-11-14 17:26:26.971826 / saving best-val-loss model 
2020-11-14 17:27:28.930370 / Iteration 3700: 0.6561900973320007 -1.2285280227661133 
2020-11-14 17:27:29.009626 / Iteration 3700 valid loss -0.5563535094261169 
2020-11-14 17:27:29.010866 / saving best-val-loss model 
2020-11-14 17:28:31.807117 / Iteration 3800: 0.656225860118866 -1.2573418617248535 
2020-11-14 17:28:31.917196 / Iteration 3800 valid loss -0.5865573287010193 
2020-11-14 17:28:31.919210 / saving best-val-loss model 
2020-11-14 17:29:38.534989 / Iteration 3900: 0.6566389203071594 -1.3304237127304077 
2020-11-14 17:29:38.663483 / Iteration 3900 valid loss -0.6600701808929443 
2020-11-14 17:29:38.665540 / saving best-val-loss model 
2020-11-14 17:30:44.781816 / Iteration 4000: 0.6562909483909607 -1.4454541206359863 
2020-11-14 17:30:44.862477 / Iteration 4000 valid loss -0.7711523771286011 
2020-11-14 17:30:44.863909 / saving best-val-loss model 
2020-11-14 17:31:47.849517 / Iteration 4100: 0.6563214063644409 -1.5039551258087158 
2020-11-14 17:31:47.954591 / Iteration 4100 valid loss -0.827900767326355 
2020-11-14 17:31:47.955918 / saving best-val-loss model 
2020-11-14 17:32:48.752694 / Iteration 4200: 0.6566517353057861 -1.5071123838424683 
2020-11-14 17:32:48.831384 / Iteration 4200 valid loss -0.8037748336791992 
2020-11-14 17:33:51.400937 / Iteration 4300: 0.6563649773597717 -1.5870507955551147 
2020-11-14 17:33:51.475968 / Iteration 4300 valid loss -0.9106457829475403 
2020-11-14 17:33:51.477812 / saving best-val-loss model 
2020-11-14 17:34:56.028650 / Iteration 4400: 0.6563768982887268 -1.6202011108398438 
2020-11-14 17:34:56.108471 / Iteration 4400 valid loss -0.9426577091217041 
2020-11-14 17:34:56.110087 / saving best-val-loss model 
2020-11-14 17:35:58.095897 / Iteration 4500: 0.6563974022865295 -1.6479815244674683 
2020-11-14 17:35:58.180925 / Iteration 4500 valid loss -0.970238208770752 
2020-11-14 17:35:58.182220 / saving best-val-loss model 
2020-11-14 17:37:05.011377 / Iteration 4600: 0.6564147472381592 -1.672476053237915 
2020-11-14 17:37:05.141715 / Iteration 4600 valid loss -0.9947003126144409 
2020-11-14 17:37:05.143148 / saving best-val-loss model 
2020-11-14 17:38:10.900852 / Iteration 4700: 0.656441330909729 -1.6934709548950195 
2020-11-14 17:38:11.018635 / Iteration 4700 valid loss -1.016054630279541 
2020-11-14 17:38:11.020007 / saving best-val-loss model 
2020-11-14 17:39:14.156933 / Iteration 4800: 0.6565403938293457 -1.7077696323394775 
2020-11-14 17:39:14.246745 / Iteration 4800 valid loss -1.0272014141082764 
2020-11-14 17:39:14.247899 / saving best-val-loss model 
2020-11-14 17:40:16.585657 / Iteration 4900: 0.6564896106719971 -1.737180471420288 
2020-11-14 17:40:16.670165 / Iteration 4900 valid loss -1.062757968902588 
2020-11-14 17:40:16.671363 / saving best-val-loss model 
2020-11-14 17:41:18.391291 / Iteration 5000: 0.6564961671829224 -1.7609803676605225 
2020-11-14 17:41:18.468324 / Iteration 5000 valid loss -1.0855340957641602 
2020-11-14 17:41:18.469835 / saving best-val-loss model 
2020-11-14 17:42:20.841940 / Iteration 5100: 0.6566110849380493 -1.7631921768188477 
2020-11-14 17:42:20.966125 / Iteration 5100 valid loss -1.083897352218628 
2020-11-14 17:43:27.574770 / Iteration 5200: 0.656531810760498 -1.7979507446289062 
2020-11-14 17:43:27.701292 / Iteration 5200 valid loss -1.1228816509246826 
2020-11-14 17:43:27.702713 / saving best-val-loss model 
2020-11-14 17:44:29.994773 / Iteration 5300: 0.6565374732017517 -1.8492176532745361 
2020-11-14 17:44:30.108479 / Iteration 5300 valid loss -1.169234037399292 
2020-11-14 17:44:30.109671 / saving best-val-loss model 
2020-11-14 17:45:32.759704 / Iteration 5400: 0.656552791595459 -1.8698506355285645 
2020-11-14 17:45:32.838776 / Iteration 5400 valid loss -1.1894696950912476 
2020-11-14 17:45:32.840022 / saving best-val-loss model 
2020-11-14 17:46:36.752060 / Iteration 5500: 0.656581461429596 -1.8845912218093872 
2020-11-14 17:46:36.875462 / Iteration 5500 valid loss -1.2101819515228271 
2020-11-14 17:46:36.877020 / saving best-val-loss model 
2020-11-14 17:47:40.452607 / Iteration 5600: 0.6565753221511841 -1.908984899520874 
2020-11-14 17:47:40.577432 / Iteration 5600 valid loss -1.22862708568573 
2020-11-14 17:47:40.578730 / saving best-val-loss model 
2020-11-14 17:48:45.689762 / Iteration 5700: 0.656589150428772 -1.9267417192459106 
2020-11-14 17:48:45.767320 / Iteration 5700 valid loss -1.246354103088379 
2020-11-14 17:48:45.768493 / saving best-val-loss model 
2020-11-14 17:49:50.568496 / Iteration 5800: 0.6566371321678162 -1.938665747642517 
2020-11-14 17:49:50.690977 / Iteration 5800 valid loss -1.2555829286575317 
2020-11-14 17:49:50.692198 / saving best-val-loss model 
2020-11-14 17:50:53.470015 / Iteration 5900: 0.6566165685653687 -1.9652546644210815 
2020-11-14 17:50:53.546996 / Iteration 5900 valid loss -1.2845649719238281 
2020-11-14 17:50:53.548223 / saving best-val-loss model 
2020-11-14 17:51:56.134970 / Iteration 6000: 0.6566293239593506 -1.9844554662704468 
2020-11-14 17:51:56.236822 / Iteration 6000 valid loss -1.3035061359405518 
2020-11-14 17:51:56.238348 / saving best-val-loss model 
2020-11-14 17:53:01.332449 / Iteration 6100: 0.6566593647003174 -1.9955097436904907 
2020-11-14 17:53:01.414065 / Iteration 6100 valid loss -1.3090957403182983 
2020-11-14 17:53:01.415284 / saving best-val-loss model 
2020-11-14 17:54:05.362941 / Iteration 6200: 0.6566532850265503 -2.0195460319519043 
2020-11-14 17:54:05.477171 / Iteration 6200 valid loss -1.337059736251831 
2020-11-14 17:54:05.478338 / saving best-val-loss model 
2020-11-14 17:55:10.555131 / Iteration 6300: 0.6566690802574158 -2.0337328910827637 
2020-11-14 17:55:10.636784 / Iteration 6300 valid loss -1.3501579761505127 
2020-11-14 17:55:10.638042 / saving best-val-loss model 
2020-11-14 17:56:17.061768 / Iteration 6400: 0.6566907167434692 -2.043212890625 
2020-11-14 17:56:17.133509 / Iteration 6400 valid loss -1.3417539596557617 
2020-11-14 17:57:20.542512 / Iteration 6500: 0.6566967368125916 -2.058900833129883 
2020-11-14 17:57:20.666555 / Iteration 6500 valid loss -1.3739075660705566 
2020-11-14 17:57:20.668367 / saving best-val-loss model 
2020-11-14 17:58:45.857519 / Namespace(activation='ReLU', atoms=[], batch_size=25000, comet=True, data='lbidd_log_50k', dataroot='/home/mila/r/raghupas/causal-benchmark/datasets', dim_h=4, dist='SigmoidFlow', dist_args=['ndim=8', 'base_distribution=normal'], early_stop=True, eval=True, grad_norm=inf, ignore_w=False, lr=0.001, n_hidden_layers=1, num_epochs=10000, num_univariate_tests=100, overwrite_reload='', patience=None, saveroot='save', seed=123, test_prop=0.4, test_size=None, train=True, train_prop=0.5, val_prop=0.1, w_transform='Normalize', y_transform='Normalize') 
2020-11-14 17:58:45.860169 / getting data: lbidd_log_50k 
2020-11-14 17:59:07.049523 / comet url: https://www.comet.ml/sunandr/causal-benchmark/f5d92d25c7f844af9adb5cc48a500522 
2020-11-14 17:59:07.050884 / ate: 0.054972401602067546 
2020-11-14 17:59:07.052503 / <models.distributions.distributions.SigmoidFlow object at 0x7f1d51e9b850> ndim:8 base_distribution:normal 
2020-11-14 17:59:07.053984 / {'n_hidden_layers': 1, 'dim_h': 4, 'activation': ReLU()} 
2020-11-14 17:59:07.055434 / {'batch_size': 25000, 'lr': 0.001, 'num_epochs': 10000, 'verbose': True, 'print_every_iters': 100, 'optim': <class 'torch.optim.adam.Adam'>, 'eval_every': 100, 'plot_every': 100, 'p_every': 100, 'optim_args': {}} 
2020-11-14 18:00:01.186697 / Iteration 100: 0.69489586353302 1.317285180091858 
2020-11-14 18:00:01.256408 / Iteration 100 valid loss 2.0092668533325195 
2020-11-14 18:00:01.257702 / saving best-val-loss model 
2020-11-14 18:01:03.448852 / Iteration 200: 0.6851512789726257 0.8443019390106201 
2020-11-14 18:01:03.546239 / Iteration 200 valid loss 1.5215859413146973 
2020-11-14 18:01:03.547743 / saving best-val-loss model 
2020-11-14 18:02:04.452091 / Iteration 300: 0.6850573420524597 0.11164572089910507 
2020-11-14 18:02:04.559106 / Iteration 300 valid loss 0.7897313833236694 
2020-11-14 18:02:04.560325 / saving best-val-loss model 
2020-11-14 18:03:07.523603 / Iteration 400: 0.6850211024284363 -0.34793031215667725 
2020-11-14 18:03:07.602236 / Iteration 400 valid loss 0.33640772104263306 
2020-11-14 18:03:07.603685 / saving best-val-loss model 
2020-11-14 18:04:12.705265 / Iteration 500: 0.6850103735923767 -0.5387563705444336 
2020-11-14 18:04:12.791998 / Iteration 500 valid loss 0.15124166011810303 
2020-11-14 18:04:12.794355 / saving best-val-loss model 
2020-11-14 18:20:18.170576 / Namespace(activation='ReLU', atoms=[], batch_size=25000, comet=True, data='lbidd_log_50k', dataroot='/home/mila/r/raghupas/causal-benchmark/datasets', dim_h=4, dist='SigmoidFlow', dist_args=['ndim=16', 'base_distribution=normal'], early_stop=True, eval=True, grad_norm=inf, ignore_w=False, lr=0.01, n_hidden_layers=1, num_epochs=10000, num_univariate_tests=100, overwrite_reload='', patience=None, saveroot='save', seed=123, test_prop=0.4, test_size=None, train=True, train_prop=0.5, val_prop=0.1, w_transform='Standardize', y_transform='Normalize') 
2020-11-14 18:20:18.173280 / getting data: lbidd_log_50k 
2020-11-14 18:20:40.618271 / comet url: https://www.comet.ml/sunandr/causal-benchmark/4ea78252cd324afd9089e7b0766cdba2 
2020-11-14 18:20:40.619780 / ate: 0.054972401602067546 
2020-11-14 18:20:40.621440 / <models.distributions.distributions.SigmoidFlow object at 0x7fc3e2175610> ndim:16 base_distribution:normal 
2020-11-14 18:20:40.623329 / {'n_hidden_layers': 1, 'dim_h': 4, 'activation': ReLU()} 
2020-11-14 18:20:40.624918 / {'batch_size': 25000, 'lr': 0.01, 'num_epochs': 10000, 'verbose': True, 'print_every_iters': 100, 'optim': <class 'torch.optim.adam.Adam'>, 'eval_every': 100, 'plot_every': 100, 'p_every': 100, 'optim_args': {}} 
2020-11-14 18:21:34.147979 / Iteration 100: 0.6833627223968506 -0.7382907867431641 
2020-11-14 18:21:34.248625 / Iteration 100 valid loss -0.036185622215270996 
2020-11-14 18:21:34.249864 / saving best-val-loss model 
2020-11-14 18:22:36.823128 / Iteration 200: 0.6602382063865662 -2.0977790355682373 
2020-11-14 18:22:36.922347 / Iteration 200 valid loss -1.4269838333129883 
2020-11-14 18:22:36.923652 / saving best-val-loss model 
2020-11-14 18:23:44.940545 / Iteration 300: 0.6550683379173279 -2.6553869247436523 
2020-11-14 18:23:45.010875 / Iteration 300 valid loss -1.9788962602615356 
2020-11-14 18:23:45.012080 / saving best-val-loss model 
2020-11-14 18:24:53.431812 / Iteration 400: 0.6547490358352661 -2.8849587440490723 
2020-11-14 18:24:53.507345 / Iteration 400 valid loss -2.182323694229126 
2020-11-14 18:24:53.508609 / saving best-val-loss model 
2020-11-14 18:26:03.748148 / Iteration 500: 0.6550458073616028 -2.9753897190093994 
2020-11-14 18:26:03.822213 / Iteration 500 valid loss -2.271695137023926 
2020-11-14 18:26:03.823617 / saving best-val-loss model 
2020-11-14 18:27:14.628065 / Iteration 600: 0.655422031879425 -2.992403030395508 
2020-11-14 18:27:14.723796 / Iteration 600 valid loss -2.2838873863220215 
2020-11-14 18:27:14.725261 / saving best-val-loss model 
2020-11-14 18:28:24.243943 / Iteration 700: 0.6542784571647644 -3.0312540531158447 
2020-11-14 18:28:24.349310 / Iteration 700 valid loss -2.318556308746338 
2020-11-14 18:28:24.350445 / saving best-val-loss model 
2020-11-14 18:29:34.233313 / Iteration 800: 0.6543077230453491 -3.0515432357788086 
2020-11-14 18:29:34.320300 / Iteration 800 valid loss -2.3345961570739746 
2020-11-14 18:29:34.321816 / saving best-val-loss model 
2020-11-14 18:30:43.599118 / Iteration 900: 0.6542550921440125 -3.048232078552246 
2020-11-14 18:30:43.695432 / Iteration 900 valid loss -2.3115527629852295 
2020-11-14 18:31:54.733144 / Iteration 1000: 0.65423184633255 -3.0717735290527344 
2020-11-14 18:31:54.817533 / Iteration 1000 valid loss -2.338430881500244 
2020-11-14 18:31:54.818771 / saving best-val-loss model 
2020-11-14 18:33:06.894773 / Iteration 1100: 0.65425044298172 -3.0737156867980957 
2020-11-14 18:33:06.981208 / Iteration 1100 valid loss -2.340235710144043 
2020-11-14 18:33:06.982490 / saving best-val-loss model 
2020-11-14 18:34:18.218525 / Iteration 1200: 0.6542473435401917 -3.0812339782714844 
2020-11-14 18:34:18.294064 / Iteration 1200 valid loss -2.351884365081787 
2020-11-14 18:34:18.295259 / saving best-val-loss model 
2020-11-14 18:35:27.179494 / Iteration 1300: 0.6542379856109619 -3.0830190181732178 
2020-11-14 18:35:27.295431 / Iteration 1300 valid loss -2.3508214950561523 
2020-11-14 18:36:34.532440 / Iteration 1400: 0.6542472839355469 -3.098668336868286 
2020-11-14 18:36:34.619454 / Iteration 1400 valid loss -2.370887517929077 
2020-11-14 18:36:34.620783 / saving best-val-loss model 
2020-11-14 18:37:45.215353 / Iteration 1500: 0.6542479395866394 -3.0954930782318115 
2020-11-14 18:37:45.339878 / Iteration 1500 valid loss -2.361529588699341 
2020-11-14 18:38:56.489467 / Iteration 1600: 0.654129147529602 -3.107947587966919 
2020-11-14 18:38:56.587293 / Iteration 1600 valid loss -2.385443925857544 
2020-11-14 18:38:56.588805 / saving best-val-loss model 
2020-11-14 18:40:09.109883 / Iteration 1700: 0.6541426181793213 -3.0957608222961426 
2020-11-14 18:40:09.188926 / Iteration 1700 valid loss -2.377882480621338 
2020-11-14 18:41:19.116860 / Iteration 1800: 0.6541180610656738 -3.117767572402954 
2020-11-14 18:41:19.236002 / Iteration 1800 valid loss -2.3875012397766113 
2020-11-14 18:41:19.237472 / saving best-val-loss model 
2020-11-14 18:42:28.669349 / Iteration 1900: 0.6541019678115845 -3.1270172595977783 
2020-11-14 18:42:28.752126 / Iteration 1900 valid loss -2.400663137435913 
2020-11-14 18:42:28.753295 / saving best-val-loss model 
2020-11-14 18:43:37.562507 / Iteration 2000: 0.6540043354034424 -3.1274497509002686 
2020-11-14 18:43:37.641374 / Iteration 2000 valid loss -2.407021999359131 
2020-11-14 18:43:37.642574 / saving best-val-loss model 
2020-11-14 18:44:48.690635 / Iteration 2100: 0.6539540886878967 -3.1232805252075195 
2020-11-14 18:44:48.809863 / Iteration 2100 valid loss -2.407470703125 
2020-11-14 18:44:48.811096 / saving best-val-loss model 
2020-11-14 18:45:59.895239 / Iteration 2200: 0.6539232134819031 -3.127572536468506 
2020-11-14 18:45:59.984704 / Iteration 2200 valid loss -2.4065327644348145 
2020-11-14 18:47:06.803420 / Iteration 2300: 0.6539177894592285 -3.1313579082489014 
2020-11-14 18:47:06.881995 / Iteration 2300 valid loss -2.4115662574768066 
2020-11-14 18:47:06.883531 / saving best-val-loss model 
2020-11-14 18:48:16.810569 / Iteration 2400: 0.6538716554641724 -3.1393184661865234 
2020-11-14 18:48:16.894869 / Iteration 2400 valid loss -2.42117977142334 
2020-11-14 18:48:16.896089 / saving best-val-loss model 
2020-11-14 18:49:27.605622 / Iteration 2500: 0.6538568735122681 -3.1101553440093994 
2020-11-14 18:49:27.725833 / Iteration 2500 valid loss -2.396761655807495 
2020-11-14 18:50:37.861670 / Iteration 2600: 0.6538159251213074 -3.1337759494781494 
2020-11-14 18:50:37.942065 / Iteration 2600 valid loss -2.406643867492676 
2020-11-14 18:51:49.722321 / Iteration 2700: 0.6538066267967224 -3.1519219875335693 
2020-11-14 18:51:49.844056 / Iteration 2700 valid loss -2.4346237182617188 
2020-11-14 18:51:49.845397 / saving best-val-loss model 
2020-11-14 18:52:59.810176 / Iteration 2800: 0.6537935733795166 -3.158172607421875 
2020-11-14 18:52:59.929526 / Iteration 2800 valid loss -2.4405479431152344 
2020-11-14 18:52:59.930879 / saving best-val-loss model 
2020-11-14 18:54:10.513618 / Iteration 2900: 0.653756856918335 -3.1388869285583496 
2020-11-14 18:54:10.598203 / Iteration 2900 valid loss -2.423396110534668 
2020-11-14 18:55:20.393658 / Iteration 3000: 0.6537485122680664 -3.1628713607788086 
2020-11-14 18:55:20.478438 / Iteration 3000 valid loss -2.450479745864868 
2020-11-14 18:55:20.479745 / saving best-val-loss model 
2020-11-14 18:56:28.860794 / Iteration 3100: 0.6537554860115051 -3.139646530151367 
2020-11-14 18:56:28.996999 / Iteration 3100 valid loss -2.420537233352661 
2020-11-14 18:57:40.606490 / Iteration 3200: 0.6537529826164246 -3.1457836627960205 
2020-11-14 18:57:40.726789 / Iteration 3200 valid loss -2.444239616394043 
2020-11-14 18:58:49.676862 / Iteration 3300: 0.653795063495636 -3.1464481353759766 
2020-11-14 18:58:49.817659 / Iteration 3300 valid loss -2.4196295738220215 
2020-11-14 18:59:57.795532 / Iteration 3400: 0.6537484526634216 -3.17240571975708 
2020-11-14 18:59:57.874404 / Iteration 3400 valid loss -2.4572086334228516 
2020-11-14 18:59:57.875619 / saving best-val-loss model 
2020-11-14 19:05:48.156371 / Namespace(activation='ReLU', atoms=[], batch_size=25000, comet=True, data='lbidd_log_50k', dataroot='/home/mila/r/raghupas/causal-benchmark/datasets', dim_h=4, dist='SigmoidFlow', dist_args=['ndim=16', 'base_distribution=normal'], early_stop=True, eval=True, grad_norm=inf, ignore_w=False, lr=0.01, n_hidden_layers=1, num_epochs=3200, num_univariate_tests=100, overwrite_reload='', patience=None, saveroot='save', seed=123, test_prop=0.4, test_size=None, train=True, train_prop=0.5, val_prop=0.1, w_transform='Standardize', y_transform='Normalize') 
2020-11-14 19:05:48.159439 / getting data: lbidd_log_50k 
2020-11-14 19:06:09.297915 / comet url: https://www.comet.ml/sunandr/causal-benchmark/3d28929035e44a3db0486ef0c45341e2 
2020-11-14 19:06:09.299379 / ate: 0.054972401602067546 
2020-11-14 19:06:09.300838 / <models.distributions.distributions.SigmoidFlow object at 0x7f8c68db84d0> ndim:16 base_distribution:normal 
2020-11-14 19:06:09.302409 / {'n_hidden_layers': 1, 'dim_h': 4, 'activation': ReLU()} 
2020-11-14 19:06:09.304090 / {'batch_size': 25000, 'lr': 0.01, 'num_epochs': 3200, 'verbose': True, 'print_every_iters': 100, 'optim': <class 'torch.optim.adam.Adam'>, 'eval_every': 100, 'plot_every': 100, 'p_every': 100, 'optim_args': {}} 
2020-11-14 19:07:03.423694 / Iteration 100: 0.6833627223968506 -0.7382907867431641 
2020-11-14 19:07:03.492400 / Iteration 100 valid loss -0.036185622215270996 
2020-11-14 19:07:03.493666 / saving best-val-loss model 
2020-11-14 19:08:07.387678 / Iteration 200: 0.6602382063865662 -2.0977790355682373 
2020-11-14 19:08:07.506126 / Iteration 200 valid loss -1.4269838333129883 
2020-11-14 19:08:07.507396 / saving best-val-loss model 
2020-11-14 19:09:15.924049 / Iteration 300: 0.6550683379173279 -2.6553869247436523 
2020-11-14 19:09:16.005273 / Iteration 300 valid loss -1.9788962602615356 
2020-11-14 19:09:16.006633 / saving best-val-loss model 
2020-11-14 19:10:26.098554 / Iteration 400: 0.6547490358352661 -2.8849587440490723 
2020-11-14 19:10:26.210272 / Iteration 400 valid loss -2.182323694229126 
2020-11-14 19:10:26.211699 / saving best-val-loss model 
2020-11-14 19:11:35.639742 / Iteration 500: 0.6550458073616028 -2.9753897190093994 
2020-11-14 19:11:35.715592 / Iteration 500 valid loss -2.271695137023926 
2020-11-14 19:11:35.716804 / saving best-val-loss model 
2020-11-14 19:12:46.994440 / Iteration 600: 0.655422031879425 -2.992403030395508 
2020-11-14 19:12:47.119344 / Iteration 600 valid loss -2.2838873863220215 
2020-11-14 19:12:47.120713 / saving best-val-loss model 
2020-11-14 19:13:57.212933 / Iteration 700: 0.6542784571647644 -3.0312540531158447 
2020-11-14 19:13:57.347914 / Iteration 700 valid loss -2.318556308746338 
2020-11-14 19:13:57.349475 / saving best-val-loss model 
2020-11-14 19:15:11.990587 / Iteration 800: 0.6543077230453491 -3.0515432357788086 
2020-11-14 19:15:12.132460 / Iteration 800 valid loss -2.3345961570739746 
2020-11-14 19:15:12.133712 / saving best-val-loss model 
2020-11-14 19:16:22.746321 / Iteration 900: 0.6542550921440125 -3.048232078552246 
2020-11-14 19:16:22.825375 / Iteration 900 valid loss -2.3115527629852295 
2020-11-14 19:17:33.110057 / Iteration 1000: 0.65423184633255 -3.0717735290527344 
2020-11-14 19:17:33.208675 / Iteration 1000 valid loss -2.338430881500244 
2020-11-14 19:17:33.209905 / saving best-val-loss model 
2020-11-14 19:18:43.491803 / Iteration 1100: 0.65425044298172 -3.0737156867980957 
2020-11-14 19:18:43.571956 / Iteration 1100 valid loss -2.340235710144043 
2020-11-14 19:18:43.573367 / saving best-val-loss model 
2020-11-14 19:19:53.825691 / Iteration 1200: 0.6542473435401917 -3.0812339782714844 
2020-11-14 19:19:53.950187 / Iteration 1200 valid loss -2.351884365081787 
2020-11-14 19:19:53.951335 / saving best-val-loss model 
2020-11-14 19:21:06.985016 / Iteration 1300: 0.6542379856109619 -3.0830190181732178 
2020-11-14 19:21:07.094876 / Iteration 1300 valid loss -2.3508214950561523 
2020-11-14 19:22:19.127474 / Iteration 1400: 0.6542472839355469 -3.098668336868286 
2020-11-14 19:22:19.241590 / Iteration 1400 valid loss -2.370887517929077 
2020-11-14 19:22:19.243201 / saving best-val-loss model 
2020-11-14 19:23:30.301253 / Iteration 1500: 0.6542479395866394 -3.0954930782318115 
2020-11-14 19:23:30.379469 / Iteration 1500 valid loss -2.361529588699341 
2020-11-14 19:24:40.610541 / Iteration 1600: 0.654129147529602 -3.107947587966919 
2020-11-14 19:24:40.693555 / Iteration 1600 valid loss -2.385443925857544 
2020-11-14 19:24:40.695349 / saving best-val-loss model 
2020-11-14 19:25:50.580876 / Iteration 1700: 0.6541426181793213 -3.0957608222961426 
2020-11-14 19:25:50.669401 / Iteration 1700 valid loss -2.377882480621338 
2020-11-14 19:27:03.153288 / Iteration 1800: 0.6541180610656738 -3.117767572402954 
2020-11-14 19:27:03.279085 / Iteration 1800 valid loss -2.3875012397766113 
2020-11-14 19:27:03.280331 / saving best-val-loss model 
2020-11-14 19:28:17.630778 / Iteration 1900: 0.6541019678115845 -3.1270172595977783 
2020-11-14 19:28:17.711707 / Iteration 1900 valid loss -2.400663137435913 
2020-11-14 19:28:17.713035 / saving best-val-loss model 
2020-11-14 19:29:26.817529 / Iteration 2000: 0.6540043354034424 -3.1274497509002686 
2020-11-14 19:29:26.897783 / Iteration 2000 valid loss -2.407021999359131 
2020-11-14 19:29:26.899018 / saving best-val-loss model 
2020-11-14 19:30:38.439236 / Iteration 2100: 0.6539540886878967 -3.1232805252075195 
2020-11-14 19:30:38.521533 / Iteration 2100 valid loss -2.407470703125 
2020-11-14 19:30:38.522770 / saving best-val-loss model 
2020-11-14 19:31:50.043106 / Iteration 2200: 0.6539232134819031 -3.127572536468506 
2020-11-14 19:31:50.133629 / Iteration 2200 valid loss -2.4065327644348145 
2020-11-14 19:33:01.991688 / Iteration 2300: 0.6539177894592285 -3.1313579082489014 
2020-11-14 19:33:02.070559 / Iteration 2300 valid loss -2.4115662574768066 
2020-11-14 19:33:02.071844 / saving best-val-loss model 
2020-11-14 19:34:14.409012 / Iteration 2400: 0.6538716554641724 -3.1393184661865234 
2020-11-14 19:34:14.529033 / Iteration 2400 valid loss -2.42117977142334 
2020-11-14 19:34:14.530402 / saving best-val-loss model 
2020-11-14 19:35:27.085859 / Iteration 2500: 0.6538568735122681 -3.1101553440093994 
2020-11-14 19:35:27.209175 / Iteration 2500 valid loss -2.396761655807495 
2020-11-14 19:36:39.687741 / Iteration 2600: 0.6538159251213074 -3.1337759494781494 
2020-11-14 19:36:39.773182 / Iteration 2600 valid loss -2.406643867492676 
2020-11-14 19:37:48.777324 / Iteration 2700: 0.6538066267967224 -3.1519219875335693 
2020-11-14 19:37:48.913883 / Iteration 2700 valid loss -2.4346237182617188 
2020-11-14 19:37:48.915147 / saving best-val-loss model 
2020-11-14 19:39:01.106697 / Iteration 2800: 0.6537935733795166 -3.158172607421875 
2020-11-14 19:39:01.186390 / Iteration 2800 valid loss -2.4405479431152344 
2020-11-14 19:39:01.187701 / saving best-val-loss model 
2020-11-14 19:40:11.176116 / Iteration 2900: 0.653756856918335 -3.1388869285583496 
2020-11-14 19:40:11.254640 / Iteration 2900 valid loss -2.423396110534668 
2020-11-14 19:41:21.488768 / Iteration 3000: 0.6537485122680664 -3.1628713607788086 
2020-11-14 19:41:21.579515 / Iteration 3000 valid loss -2.450479745864868 
2020-11-14 19:41:21.580788 / saving best-val-loss model 
2020-11-14 19:42:32.366921 / Iteration 3100: 0.6537554860115051 -3.139646530151367 
2020-11-14 19:42:32.480876 / Iteration 3100 valid loss -2.420537233352661 
2020-11-14 19:43:44.264417 / Iteration 3200: 0.6537529826164246 -3.1457836627960205 
2020-11-14 19:43:44.359112 / Iteration 3200 valid loss -2.444239616394043 
2020-11-14 19:54:59.318335 / OrderedDict([('nll', -2.450479745864868), ('avg_t_pval', 0.8967337606104371), ('avg_y_pval', 0.07730257980042066), ('min_t_pval', 0.13064467513421296), ('min_y_pval', 0.020071466443027933), ('q30_t_pval', 0.9290835917025413), ('q30_y_pval', 0.05776257046427332), ('q50_t_pval', 0.9989384258511402), ('q50_y_pval', 0.0702819492517697), ('ate_exact', 0.0), ('ate_noisy', 0.04756111972033977)]) 
2020-11-15 00:03:41.452609 / Namespace(activation='ReLU', atoms=[], batch_size=25000, comet=True, data='ihdp', dataroot='/home/mila/r/raghupas/causal-benchmark/datasets', dim_h=4, dist='SigmoidFlow', dist_args=['ndim=16', 'base_distribution=normal'], early_stop=True, eval=True, grad_norm=inf, ignore_w=False, lr=0.01, n_hidden_layers=1, num_epochs=10000, num_univariate_tests=100, overwrite_reload='', patience=None, saveroot='save', seed=123, test_prop=0.4, test_size=None, train=True, train_prop=0.5, val_prop=0.1, w_transform='Standardize', y_transform='Normalize') 
2020-11-15 00:03:41.455089 / getting data: ihdp 
2020-11-15 00:04:02.616200 / comet url: https://www.comet.ml/sunandr/causal-benchmark/382e7ac8368a47b9bde49033592ce2b4 
2020-11-15 00:04:02.617982 / ate: None 
2020-11-15 00:04:02.619575 / <models.distributions.distributions.SigmoidFlow object at 0x7f976edb74d0> ndim:16 base_distribution:normal 
2020-11-15 00:04:02.621153 / {'n_hidden_layers': 1, 'dim_h': 4, 'activation': ReLU()} 
2020-11-15 00:04:02.622993 / {'batch_size': 25000, 'lr': 0.01, 'num_epochs': 10000, 'verbose': True, 'print_every_iters': 100, 'optim': <class 'torch.optim.adam.Adam'>, 'eval_every': 100, 'plot_every': 100, 'p_every': 100, 'optim_args': {}} 
2020-11-15 00:04:03.763131 / Iteration 100: 0.4369907081127167 -0.9971350431442261 
2020-11-15 00:04:03.767876 / Iteration 100 valid loss -0.5161275267601013 
2020-11-15 00:04:03.769167 / saving best-val-loss model 
2020-11-15 00:04:08.509566 / Iteration 200: 0.39314568042755127 -1.321988821029663 
2020-11-15 00:04:08.513904 / Iteration 200 valid loss -0.0016829371452331543 
2020-11-15 00:04:13.203497 / Iteration 300: 0.37220942974090576 -1.4247316122055054 
2020-11-15 00:04:13.208608 / Iteration 300 valid loss 0.5880650877952576 
2020-11-15 00:04:17.984031 / Iteration 400: 0.36297428607940674 -1.4983700513839722 
2020-11-15 00:04:17.989578 / Iteration 400 valid loss 1.3971505165100098 
2020-11-15 00:04:22.828107 / Iteration 500: 0.3689684271812439 -1.5963255167007446 
2020-11-15 00:04:22.832687 / Iteration 500 valid loss 2.0572361946105957 
2020-11-15 00:04:27.671387 / Iteration 600: 0.3617664575576782 -1.6343871355056763 
2020-11-15 00:04:27.675856 / Iteration 600 valid loss 2.7966456413269043 
2020-11-15 00:04:32.509212 / Iteration 700: 0.3593953847885132 -1.6765751838684082 
2020-11-15 00:04:32.514094 / Iteration 700 valid loss 3.480128526687622 
2020-11-15 00:04:37.381626 / Iteration 800: 0.3581704795360565 -1.6965793371200562 
2020-11-15 00:04:37.386355 / Iteration 800 valid loss 3.885195255279541 
2020-11-15 00:04:42.265292 / Iteration 900: 0.3568861186504364 -1.7077738046646118 
2020-11-15 00:04:42.269739 / Iteration 900 valid loss 4.150309085845947 
2020-11-15 00:04:47.355376 / Iteration 1000: 0.3565744459629059 -1.715969443321228 
2020-11-15 00:04:47.361212 / Iteration 1000 valid loss 4.231881141662598 
2020-11-15 00:04:52.348887 / Iteration 1100: 0.35650214552879333 -1.7181998491287231 
2020-11-15 00:04:52.353601 / Iteration 1100 valid loss 4.370965003967285 
2020-11-15 00:04:57.249630 / Iteration 1200: 0.35599595308303833 -1.745538353919983 
2020-11-15 00:04:57.254055 / Iteration 1200 valid loss 5.089215278625488 
2020-11-15 00:05:02.129290 / Iteration 1300: 0.35293853282928467 -1.7851046323776245 
2020-11-15 00:05:02.133698 / Iteration 1300 valid loss 5.725137233734131 
2020-11-15 00:05:07.151506 / Iteration 1400: 0.3524738550186157 -1.804695963859558 
2020-11-15 00:05:07.156303 / Iteration 1400 valid loss 6.050782680511475 
2020-11-15 00:05:12.212124 / Iteration 1500: 0.35167616605758667 -1.8283849954605103 
2020-11-15 00:05:12.216647 / Iteration 1500 valid loss 6.381978511810303 
2020-11-15 00:05:17.186650 / Iteration 1600: 0.35161012411117554 -1.8296014070510864 
2020-11-15 00:05:17.191797 / Iteration 1600 valid loss 6.654743671417236 
2020-11-15 00:05:22.257208 / Iteration 1700: 0.3507249355316162 -1.8571680784225464 
2020-11-15 00:05:22.261731 / Iteration 1700 valid loss 6.794826030731201 
2020-11-15 00:05:27.442903 / Iteration 1800: 0.3514852225780487 -1.8229831457138062 
2020-11-15 00:05:27.448212 / Iteration 1800 valid loss 6.862727165222168 
2020-11-15 00:05:32.565366 / Iteration 1900: 0.349176824092865 -1.8719062805175781 
2020-11-15 00:05:32.569836 / Iteration 1900 valid loss 7.092513084411621 
2020-11-15 00:05:37.668203 / Iteration 2000: 0.3471185266971588 -1.8748903274536133 
2020-11-15 00:05:37.672785 / Iteration 2000 valid loss 7.123509883880615 
2020-11-15 00:05:42.824353 / Iteration 2100: 0.3465093970298767 -1.8822126388549805 
2020-11-15 00:05:42.829182 / Iteration 2100 valid loss 7.248600482940674 
2020-11-15 00:05:47.889683 / Iteration 2200: 0.3445100486278534 -1.892098307609558 
2020-11-15 00:05:47.894458 / Iteration 2200 valid loss 7.268270015716553 
2020-11-15 00:05:53.076117 / Iteration 2300: 0.34510180354118347 -1.862383484840393 
2020-11-15 00:05:53.080769 / Iteration 2300 valid loss 7.304297924041748 
2020-11-15 00:05:58.161691 / Iteration 2400: 0.34345823526382446 -1.855514645576477 
2020-11-15 00:05:58.167307 / Iteration 2400 valid loss 7.361223220825195 
2020-11-15 00:06:03.384956 / Iteration 2500: 0.3414987325668335 -1.9091242551803589 
2020-11-15 00:06:03.390128 / Iteration 2500 valid loss 7.468088150024414 
2020-11-15 00:06:08.389106 / Iteration 2600: 0.3428236246109009 -1.9179021120071411 
2020-11-15 00:06:08.394475 / Iteration 2600 valid loss 7.581822872161865 
2020-11-15 00:07:52.415306 / Namespace(activation='ReLU', atoms=[], batch_size=25000, comet=True, data='ihdp', dataroot='/home/mila/r/raghupas/causal-benchmark/datasets', dim_h=4, dist='SigmoidFlow', dist_args=['ndim=2', 'base_distribution=normal'], early_stop=True, eval=True, grad_norm=inf, ignore_w=False, lr=0.0001, n_hidden_layers=1, num_epochs=10000, num_univariate_tests=100, overwrite_reload='', patience=None, saveroot='save', seed=123, test_prop=0.4, test_size=None, train=True, train_prop=0.5, val_prop=0.1, w_transform='Standardize', y_transform='Normalize') 
2020-11-15 00:07:52.418314 / getting data: ihdp 
2020-11-15 00:08:15.152706 / comet url: https://www.comet.ml/sunandr/causal-benchmark/06541da35aa94650ac81f74d046f5ed3 
2020-11-15 00:08:15.154238 / ate: None 
2020-11-15 00:08:15.155739 / <models.distributions.distributions.SigmoidFlow object at 0x7fa96f34ac10> ndim:2 base_distribution:normal 
2020-11-15 00:08:15.157455 / {'n_hidden_layers': 1, 'dim_h': 4, 'activation': ReLU()} 
2020-11-15 00:08:15.159090 / {'batch_size': 25000, 'lr': 0.0001, 'num_epochs': 10000, 'verbose': True, 'print_every_iters': 100, 'optim': <class 'torch.optim.adam.Adam'>, 'eval_every': 100, 'plot_every': 100, 'p_every': 100, 'optim_args': {}} 
2020-11-15 00:08:16.261831 / Iteration 100: 0.5914884209632874 1.3913500308990479 
2020-11-15 00:08:16.266817 / Iteration 100 valid loss 1.9714837074279785 
2020-11-15 00:08:16.268588 / saving best-val-loss model 
2020-11-15 00:08:20.924794 / Iteration 200: 0.5864614248275757 1.3538824319839478 
2020-11-15 00:08:20.930363 / Iteration 200 valid loss 1.9268044233322144 
2020-11-15 00:08:20.932036 / saving best-val-loss model 
2020-11-15 00:08:25.508452 / Iteration 300: 0.5806732773780823 1.3186016082763672 
2020-11-15 00:08:25.513964 / Iteration 300 valid loss 1.8832852840423584 
2020-11-15 00:08:25.515228 / saving best-val-loss model 
2020-11-15 00:08:29.991687 / Iteration 400: 0.5745289325714111 1.2847356796264648 
2020-11-15 00:08:29.996154 / Iteration 400 valid loss 1.8410667181015015 
2020-11-15 00:08:29.997588 / saving best-val-loss model 
2020-11-15 00:08:34.453946 / Iteration 500: 0.568427324295044 1.2518808841705322 
2020-11-15 00:08:34.459231 / Iteration 500 valid loss 1.800032377243042 
2020-11-15 00:08:34.460767 / saving best-val-loss model 
2020-11-15 00:08:38.956062 / Iteration 600: 0.562635600566864 1.219946026802063 
2020-11-15 00:08:38.960598 / Iteration 600 valid loss 1.7600675821304321 
2020-11-15 00:08:38.962021 / saving best-val-loss model 
2020-11-15 00:08:43.453635 / Iteration 700: 0.5571784973144531 1.1887141466140747 
2020-11-15 00:08:43.458538 / Iteration 700 valid loss 1.7209810018539429 
2020-11-15 00:08:43.459876 / saving best-val-loss model 
2020-11-15 00:08:47.979386 / Iteration 800: 0.5520955920219421 1.1582560539245605 
2020-11-15 00:08:47.983581 / Iteration 800 valid loss 1.683061122894287 
2020-11-15 00:08:47.984888 / saving best-val-loss model 
2020-11-15 00:08:52.467837 / Iteration 900: 0.5472590327262878 1.12834632396698 
2020-11-15 00:08:52.473073 / Iteration 900 valid loss 1.6461615562438965 
2020-11-15 00:08:52.474495 / saving best-val-loss model 
2020-11-15 00:08:57.005549 / Iteration 1000: 0.5426201224327087 1.0986213684082031 
2020-11-15 00:08:57.010105 / Iteration 1000 valid loss 1.6095545291900635 
2020-11-15 00:08:57.011226 / saving best-val-loss model 
2020-11-15 00:09:01.516083 / Iteration 1100: 0.5381239056587219 1.0685715675354004 
2020-11-15 00:09:01.520265 / Iteration 1100 valid loss 1.5727980136871338 
2020-11-15 00:09:01.521445 / saving best-val-loss model 
2020-11-15 00:09:06.034886 / Iteration 1200: 0.533730685710907 1.0376044511795044 
2020-11-15 00:09:06.039781 / Iteration 1200 valid loss 1.5350391864776611 
2020-11-15 00:09:06.041215 / saving best-val-loss model 
2020-11-15 00:09:10.490989 / Iteration 1300: 0.5294315814971924 1.0050344467163086 
2020-11-15 00:09:10.495714 / Iteration 1300 valid loss 1.4959686994552612 
2020-11-15 00:09:10.497125 / saving best-val-loss model 
2020-11-15 00:09:14.962287 / Iteration 1400: 0.5254318714141846 0.9696251153945923 
2020-11-15 00:09:14.967901 / Iteration 1400 valid loss 1.4544386863708496 
2020-11-15 00:09:14.969934 / saving best-val-loss model 
2020-11-15 00:09:19.543826 / Iteration 1500: 0.521908164024353 0.9272328615188599 
2020-11-15 00:09:19.548479 / Iteration 1500 valid loss 1.40717351436615 
2020-11-15 00:09:19.549836 / saving best-val-loss model 
2020-11-15 00:09:23.971570 / Iteration 1600: 0.5181166529655457 0.8743882775306702 
2020-11-15 00:09:23.976000 / Iteration 1600 valid loss 1.3502329587936401 
2020-11-15 00:09:23.977185 / saving best-val-loss model 
2020-11-15 00:09:28.390676 / Iteration 1700: 0.5135602355003357 0.8114310503005981 
2020-11-15 00:09:28.395080 / Iteration 1700 valid loss 1.2835899591445923 
2020-11-15 00:09:28.396538 / saving best-val-loss model 
2020-11-15 00:09:32.760344 / Iteration 1800: 0.5084169507026672 0.7383245825767517 
2020-11-15 00:09:32.764378 / Iteration 1800 valid loss 1.2067968845367432 
2020-11-15 00:09:32.765748 / saving best-val-loss model 
2020-11-15 00:09:37.172349 / Iteration 1900: 0.5024882555007935 0.6550284624099731 
2020-11-15 00:09:37.176774 / Iteration 1900 valid loss 1.1198315620422363 
2020-11-15 00:09:37.177953 / saving best-val-loss model 
2020-11-15 00:09:41.597701 / Iteration 2000: 0.4962898790836334 0.563056468963623 
2020-11-15 00:09:41.601869 / Iteration 2000 valid loss 1.0230287313461304 
2020-11-15 00:09:41.603063 / saving best-val-loss model 
2020-11-15 00:09:46.105507 / Iteration 2100: 0.4905696213245392 0.46537503600120544 
2020-11-15 00:09:46.109656 / Iteration 2100 valid loss 0.919407069683075 
2020-11-15 00:09:46.110855 / saving best-val-loss model 
2020-11-15 00:09:50.595542 / Iteration 2200: 0.48590102791786194 0.36658477783203125 
2020-11-15 00:09:50.600302 / Iteration 2200 valid loss 0.8132516741752625 
2020-11-15 00:09:50.601441 / saving best-val-loss model 
2020-11-15 00:09:55.003578 / Iteration 2300: 0.4822428822517395 0.2699505686759949 
2020-11-15 00:09:55.007917 / Iteration 2300 valid loss 0.7089593410491943 
2020-11-15 00:09:55.009274 / saving best-val-loss model 
2020-11-15 00:09:59.486669 / Iteration 2400: 0.4792584478855133 0.17635025084018707 
2020-11-15 00:09:59.491162 / Iteration 2400 valid loss 0.6073211431503296 
2020-11-15 00:09:59.492318 / saving best-val-loss model 
2020-11-15 00:10:03.955517 / Iteration 2500: 0.4765978753566742 0.08611652255058289 
2020-11-15 00:10:03.960650 / Iteration 2500 valid loss 0.509894609451294 
2020-11-15 00:10:03.962009 / saving best-val-loss model 
2020-11-15 00:10:08.424057 / Iteration 2600: 0.4740840792655945 -0.0001886573008960113 
2020-11-15 00:10:08.428793 / Iteration 2600 valid loss 0.41660791635513306 
2020-11-15 00:10:08.429937 / saving best-val-loss model 
2020-11-15 00:10:12.974498 / Iteration 2700: 0.4708218574523926 -0.08835519105195999 
2020-11-15 00:10:12.979220 / Iteration 2700 valid loss 0.3174169957637787 
2020-11-15 00:10:12.980447 / saving best-val-loss model 
2020-11-15 00:10:17.803987 / Iteration 2800: 0.46801549196243286 -0.17094163596630096 
2020-11-15 00:10:17.808638 / Iteration 2800 valid loss 0.2245493084192276 
2020-11-15 00:10:17.809731 / saving best-val-loss model 
2020-11-15 00:10:22.467972 / Iteration 2900: 0.46571165323257446 -0.2451189160346985 
2020-11-15 00:10:22.472339 / Iteration 2900 valid loss 0.14198878407478333 
2020-11-15 00:10:22.473530 / saving best-val-loss model 
2020-11-15 00:10:27.135733 / Iteration 3000: 0.4638043940067291 -0.31116780638694763 
2020-11-15 00:10:27.140235 / Iteration 3000 valid loss 0.06906455755233765 
2020-11-15 00:10:27.141352 / saving best-val-loss model 
2020-11-15 00:10:31.836645 / Iteration 3100: 0.46217361092567444 -0.3698108196258545 
2020-11-15 00:10:31.841000 / Iteration 3100 valid loss 0.004541546106338501 
2020-11-15 00:10:31.842158 / saving best-val-loss model 
2020-11-15 00:10:36.576766 / Iteration 3200: 0.4607456624507904 -0.4220670461654663 
2020-11-15 00:10:36.581136 / Iteration 3200 valid loss -0.05257749557495117 
2020-11-15 00:10:36.582383 / saving best-val-loss model 
2020-11-15 00:10:41.232864 / Iteration 3300: 0.459522545337677 -0.4683528244495392 
2020-11-15 00:10:41.238373 / Iteration 3300 valid loss -0.10304772853851318 
2020-11-15 00:10:41.239878 / saving best-val-loss model 
2020-11-15 00:10:45.962932 / Iteration 3400: 0.45846477150917053 -0.5096117854118347 
2020-11-15 00:10:45.967620 / Iteration 3400 valid loss -0.1474752426147461 
2020-11-15 00:10:45.968871 / saving best-val-loss model 
2020-11-15 00:10:50.697603 / Iteration 3500: 0.4575508236885071 -0.5465843677520752 
2020-11-15 00:10:50.702067 / Iteration 3500 valid loss -0.186894029378891 
2020-11-15 00:10:50.703437 / saving best-val-loss model 
2020-11-15 00:10:55.365454 / Iteration 3600: 0.4568295478820801 -0.580049991607666 
2020-11-15 00:10:55.370148 / Iteration 3600 valid loss -0.22152680158615112 
2020-11-15 00:10:55.371520 / saving best-val-loss model 
2020-11-15 00:11:00.006262 / Iteration 3700: 0.4562840461730957 -0.6105071306228638 
2020-11-15 00:11:00.011712 / Iteration 3700 valid loss -0.25191670656204224 
2020-11-15 00:11:00.013331 / saving best-val-loss model 
2020-11-15 00:11:04.654637 / Iteration 3800: 0.45584893226623535 -0.6383081674575806 
2020-11-15 00:11:04.658808 / Iteration 3800 valid loss -0.27811023592948914 
2020-11-15 00:11:04.659855 / saving best-val-loss model 
2020-11-15 00:11:09.447520 / Iteration 3900: 0.4555005431175232 -0.6625476479530334 
2020-11-15 00:11:09.451990 / Iteration 3900 valid loss -0.3002338409423828 
2020-11-15 00:11:09.453448 / saving best-val-loss model 
2020-11-15 00:11:14.223236 / Iteration 4000: 0.4552296996116638 -0.6838257908821106 
2020-11-15 00:11:14.228368 / Iteration 4000 valid loss -0.319003164768219 
2020-11-15 00:11:14.229598 / saving best-val-loss model 
2020-11-15 00:11:18.961738 / Iteration 4100: 0.4549298882484436 -0.7032516598701477 
2020-11-15 00:11:18.966010 / Iteration 4100 valid loss -0.33543381094932556 
2020-11-15 00:11:18.967181 / saving best-val-loss model 
2020-11-15 00:11:23.714941 / Iteration 4200: 0.45486727356910706 -0.7212955355644226 
2020-11-15 00:11:23.719312 / Iteration 4200 valid loss -0.3501099646091461 
2020-11-15 00:11:23.720612 / saving best-val-loss model 
2020-11-15 00:11:28.334697 / Iteration 4300: 0.4548802077770233 -0.7383774518966675 
2020-11-15 00:11:28.339668 / Iteration 4300 valid loss -0.3623811602592468 
2020-11-15 00:11:28.340831 / saving best-val-loss model 
2020-11-15 00:11:32.938909 / Iteration 4400: 0.4549323618412018 -0.7543421387672424 
2020-11-15 00:11:32.943546 / Iteration 4400 valid loss -0.37376803159713745 
2020-11-15 00:11:32.944815 / saving best-val-loss model 
2020-11-15 00:11:37.615641 / Iteration 4500: 0.45498037338256836 -0.7693319916725159 
2020-11-15 00:11:37.620802 / Iteration 4500 valid loss -0.3841715157032013 
2020-11-15 00:11:37.622090 / saving best-val-loss model 
2020-11-15 00:11:42.242249 / Iteration 4600: 0.45503464341163635 -0.7837126851081848 
2020-11-15 00:11:42.246455 / Iteration 4600 valid loss -0.3937629759311676 
2020-11-15 00:11:42.247834 / saving best-val-loss model 
2020-11-15 00:11:46.925576 / Iteration 4700: 0.4551333785057068 -0.7973524928092957 
2020-11-15 00:11:46.930257 / Iteration 4700 valid loss -0.40321141481399536 
2020-11-15 00:11:46.931542 / saving best-val-loss model 
2020-11-15 00:11:51.545401 / Iteration 4800: 0.45527538657188416 -0.810290515422821 
2020-11-15 00:11:51.550086 / Iteration 4800 valid loss -0.41131898760795593 
2020-11-15 00:11:51.551716 / saving best-val-loss model 
2020-11-15 00:11:56.216489 / Iteration 4900: 0.4554181396961212 -0.8225564956665039 
2020-11-15 00:11:56.221418 / Iteration 4900 valid loss -0.41845905780792236 
2020-11-15 00:11:56.222607 / saving best-val-loss model 
2020-11-15 00:12:00.832326 / Iteration 5000: 0.4555085599422455 -0.8342892527580261 
2020-11-15 00:12:00.836662 / Iteration 5000 valid loss -0.42457956075668335 
2020-11-15 00:12:00.837900 / saving best-val-loss model 
2020-11-15 00:12:05.457732 / Iteration 5100: 0.4555831849575043 -0.8452533483505249 
2020-11-15 00:12:05.462207 / Iteration 5100 valid loss -0.4297274351119995 
2020-11-15 00:12:05.463462 / saving best-val-loss model 
2020-11-15 00:12:10.053024 / Iteration 5200: 0.4556420147418976 -0.8555293083190918 
2020-11-15 00:12:10.057369 / Iteration 5200 valid loss -0.4336746633052826 
2020-11-15 00:12:10.058592 / saving best-val-loss model 
2020-11-15 00:12:14.724761 / Iteration 5300: 0.45579394698143005 -0.8656693696975708 
2020-11-15 00:12:14.729606 / Iteration 5300 valid loss -0.4375189244747162 
2020-11-15 00:12:14.730839 / saving best-val-loss model 
2020-11-15 00:12:19.399948 / Iteration 5400: 0.45602425932884216 -0.8756421208381653 
2020-11-15 00:12:19.405108 / Iteration 5400 valid loss -0.4409645199775696 
2020-11-15 00:12:19.406343 / saving best-val-loss model 
2020-11-15 00:12:24.046568 / Iteration 5500: 0.456208199262619 -0.8850367069244385 
2020-11-15 00:12:24.051885 / Iteration 5500 valid loss -0.4440074563026428 
2020-11-15 00:12:24.053174 / saving best-val-loss model 
2020-11-15 00:12:28.672502 / Iteration 5600: 0.45638152956962585 -0.8938708901405334 
2020-11-15 00:12:28.676776 / Iteration 5600 valid loss -0.4461185336112976 
2020-11-15 00:12:28.678037 / saving best-val-loss model 
2020-11-15 00:12:33.467324 / Iteration 5700: 0.45649126172065735 -0.9024295806884766 
2020-11-15 00:12:33.472052 / Iteration 5700 valid loss -0.4472864866256714 
2020-11-15 00:12:33.473471 / saving best-val-loss model 
2020-11-15 00:12:38.243860 / Iteration 5800: 0.45652925968170166 -0.910865843296051 
2020-11-15 00:12:38.249096 / Iteration 5800 valid loss -0.4479043185710907 
2020-11-15 00:12:38.250728 / saving best-val-loss model 
2020-11-15 00:12:42.968285 / Iteration 5900: 0.45650047063827515 -0.9188544750213623 
2020-11-15 00:12:42.972817 / Iteration 5900 valid loss -0.44846126437187195 
2020-11-15 00:12:42.974424 / saving best-val-loss model 
2020-11-15 00:12:47.846565 / Iteration 6000: 0.4563639760017395 -0.92658931016922 
2020-11-15 00:12:47.851305 / Iteration 6000 valid loss -0.44778817892074585 
2020-11-15 00:12:52.758438 / Iteration 6100: 0.4561319649219513 -0.9337892532348633 
2020-11-15 00:12:52.763215 / Iteration 6100 valid loss -0.44714346528053284 
2020-11-15 00:12:57.412826 / Iteration 6200: 0.45588597655296326 -0.9405145645141602 
2020-11-15 00:12:57.417696 / Iteration 6200 valid loss -0.44710150361061096 
2020-11-15 00:13:02.209844 / Iteration 6300: 0.45557349920272827 -0.946805477142334 
2020-11-15 00:13:02.214248 / Iteration 6300 valid loss -0.4465017318725586 
2020-11-15 00:13:06.963837 / Iteration 6400: 0.4551679790019989 -0.9527729749679565 
2020-11-15 00:13:06.969088 / Iteration 6400 valid loss -0.4447805881500244 
2020-11-15 00:13:11.643076 / Iteration 6500: 0.45478203892707825 -0.9584060311317444 
2020-11-15 00:13:11.647497 / Iteration 6500 valid loss -0.4422609508037567 
2020-11-15 00:13:16.504800 / Iteration 6600: 0.45436209440231323 -0.9636659026145935 
2020-11-15 00:13:16.509635 / Iteration 6600 valid loss -0.4391483664512634 
2020-11-15 00:13:21.198801 / Iteration 6700: 0.45385026931762695 -0.9687499403953552 
2020-11-15 00:13:21.203551 / Iteration 6700 valid loss -0.43619614839553833 
2020-11-15 00:13:25.987709 / Iteration 6800: 0.4532754719257355 -0.9739236235618591 
2020-11-15 00:13:25.992354 / Iteration 6800 valid loss -0.4339180886745453 
2020-11-15 00:13:30.687551 / Iteration 6900: 0.45271724462509155 -0.9789820313453674 
2020-11-15 00:13:30.692511 / Iteration 6900 valid loss -0.430470734834671 
2020-11-15 00:13:35.390285 / Iteration 7000: 0.4522606432437897 -0.9845989942550659 
2020-11-15 00:13:35.394901 / Iteration 7000 valid loss -0.4270457327365875 
2020-11-15 00:13:40.017682 / Iteration 7100: 0.4517613649368286 -0.9900351762771606 
2020-11-15 00:13:40.022473 / Iteration 7100 valid loss -0.4238218367099762 
2020-11-15 00:13:44.771321 / Iteration 7200: 0.45109522342681885 -0.9956677556037903 
2020-11-15 00:13:44.775986 / Iteration 7200 valid loss -0.42174673080444336 
2020-11-15 00:13:49.422406 / Iteration 7300: 0.4503704607486725 -1.0011916160583496 
2020-11-15 00:13:49.426869 / Iteration 7300 valid loss -0.4202060103416443 
2020-11-15 00:13:54.167777 / Iteration 7400: 0.44965890049934387 -1.0067040920257568 
2020-11-15 00:13:54.172647 / Iteration 7400 valid loss -0.4191662669181824 
2020-11-15 00:13:58.949896 / Iteration 7500: 0.44888219237327576 -1.0120593309402466 
2020-11-15 00:13:58.954986 / Iteration 7500 valid loss -0.41782718896865845 
2020-11-15 00:14:03.687048 / Iteration 7600: 0.4479922652244568 -1.017526626586914 
2020-11-15 00:14:03.691782 / Iteration 7600 valid loss -0.41749072074890137 
2020-11-15 00:14:08.398414 / Iteration 7700: 0.4470965266227722 -1.023118257522583 
2020-11-15 00:14:08.403151 / Iteration 7700 valid loss -0.41770604252815247 
2020-11-15 00:14:13.063976 / Iteration 7800: 0.44621166586875916 -1.0286917686462402 
2020-11-15 00:14:13.069875 / Iteration 7800 valid loss -0.4177681803703308 
2020-11-15 00:14:17.738876 / Iteration 7900: 0.44537538290023804 -1.0341932773590088 
2020-11-15 00:14:17.743589 / Iteration 7900 valid loss -0.41712215542793274 
2020-11-15 00:14:22.505292 / Iteration 8000: 0.4445136487483978 -1.0397619009017944 
2020-11-15 00:14:22.510096 / Iteration 8000 valid loss -0.41545265913009644 
2020-11-15 00:14:27.234497 / Iteration 8100: 0.4435691237449646 -1.0456619262695312 
2020-11-15 00:14:27.239835 / Iteration 8100 valid loss -0.41456666588783264 
2020-11-15 00:14:31.936548 / Iteration 8200: 0.4423457980155945 -1.0517386198043823 
2020-11-15 00:14:31.941332 / Iteration 8200 valid loss -0.41262370347976685 
2020-11-15 00:14:36.663306 / Iteration 8300: 0.44120070338249207 -1.0578444004058838 
2020-11-15 00:14:36.668157 / Iteration 8300 valid loss -0.41322454810142517 
2020-11-15 00:14:41.492593 / Iteration 8400: 0.4400733411312103 -1.0642321109771729 
2020-11-15 00:14:41.497204 / Iteration 8400 valid loss -0.41466495394706726 
2020-11-15 00:14:46.255205 / Iteration 8500: 0.4388562738895416 -1.0704727172851562 
2020-11-15 00:14:46.259806 / Iteration 8500 valid loss -0.4165746569633484 
2020-11-15 00:14:50.900911 / Iteration 8600: 0.4374242424964905 -1.0763752460479736 
2020-11-15 00:14:50.905688 / Iteration 8600 valid loss -0.41606420278549194 
2020-11-15 00:14:55.673220 / Iteration 8700: 0.43614959716796875 -1.0825152397155762 
2020-11-15 00:14:55.677838 / Iteration 8700 valid loss -0.4198700487613678 
2020-11-15 00:15:00.323973 / Iteration 8800: 0.4349915683269501 -1.0884062051773071 
2020-11-15 00:15:00.328714 / Iteration 8800 valid loss -0.42311573028564453 
2020-11-15 00:15:05.063554 / Iteration 8900: 0.4338376820087433 -1.0942764282226562 
2020-11-15 00:15:05.068660 / Iteration 8900 valid loss -0.4294852614402771 
2020-11-15 00:15:09.841194 / Iteration 9000: 0.4326186180114746 -1.1003202199935913 
2020-11-15 00:15:09.846335 / Iteration 9000 valid loss -0.4383934438228607 
2020-11-15 00:15:14.640935 / Iteration 9100: 0.4314800500869751 -1.1061524152755737 
2020-11-15 00:15:14.646226 / Iteration 9100 valid loss -0.44366219639778137 
2020-11-15 00:15:19.358728 / Iteration 9200: 0.4304029643535614 -1.112047791481018 
2020-11-15 00:15:19.363635 / Iteration 9200 valid loss -0.44674956798553467 
2020-11-15 00:15:24.029251 / Iteration 9300: 0.4293806850910187 -1.1180071830749512 
2020-11-15 00:15:24.033973 / Iteration 9300 valid loss -0.4461601674556732 
2020-11-15 00:15:28.738243 / Iteration 9400: 0.4281683564186096 -1.1231430768966675 
2020-11-15 00:15:28.744327 / Iteration 9400 valid loss -0.44803258776664734 
2020-11-15 00:15:33.333667 / Iteration 9500: 0.4261230230331421 -1.128755807876587 
2020-11-15 00:15:33.338616 / Iteration 9500 valid loss -0.4520540237426758 
2020-11-15 00:15:33.339788 / saving best-val-loss model 
2020-11-15 00:15:37.967326 / Iteration 9600: 0.42387136816978455 -1.1335701942443848 
2020-11-15 00:15:37.972273 / Iteration 9600 valid loss -0.4640839695930481 
2020-11-15 00:15:37.973487 / saving best-val-loss model 
2020-11-15 00:15:42.602412 / Iteration 9700: 0.4225076735019684 -1.1383955478668213 
2020-11-15 00:15:42.606978 / Iteration 9700 valid loss -0.4754403829574585 
2020-11-15 00:15:42.608530 / saving best-val-loss model 
2020-11-15 00:15:47.319917 / Iteration 9800: 0.4212998151779175 -1.1427748203277588 
2020-11-15 00:15:47.323960 / Iteration 9800 valid loss -0.48277872800827026 
2020-11-15 00:15:47.325183 / saving best-val-loss model 
2020-11-15 00:15:51.877016 / Iteration 9900: 0.420205295085907 -1.1466898918151855 
2020-11-15 00:15:51.881344 / Iteration 9900 valid loss -0.489164263010025 
2020-11-15 00:15:51.882466 / saving best-val-loss model 
2020-11-15 00:15:56.562488 / Iteration 10000: 0.4193386435508728 -1.151172161102295 
2020-11-15 00:15:56.567681 / Iteration 10000 valid loss -0.49841317534446716 
2020-11-15 00:15:56.569331 / saving best-val-loss model 
2020-11-15 00:16:39.538901 / OrderedDict([('nll', -0.49841317534446716), ('avg_t_pval', 0.8905007708960969), ('avg_y_pval', 0.27700404332283773), ('min_t_pval', 0.2517934969860438), ('min_y_pval', 0.013120642873374222), ('q30_t_pval', 0.9020597663523344), ('q30_y_pval', 0.10829048667083316), ('q50_t_pval', 0.9870057637636653), ('q50_y_pval', 0.21601959807748727), ('ate_exact', 0.0), ('ate_noisy', 4.105119199752807)]) 
2020-11-15 00:19:00.273605 / Namespace(activation='ReLU', atoms=[], batch_size=25000, comet=True, data='ihdp', dataroot='/home/mila/r/raghupas/causal-benchmark/datasets', dim_h=2, dist='SigmoidFlow', dist_args=['ndim=4', 'base_distribution=normal'], early_stop=True, eval=True, grad_norm=inf, ignore_w=False, lr=0.001, n_hidden_layers=1, num_epochs=10000, num_univariate_tests=100, overwrite_reload='', patience=None, saveroot='save', seed=123, test_prop=0.4, test_size=None, train=True, train_prop=0.5, val_prop=0.1, w_transform='Standardize', y_transform='Normalize') 
2020-11-15 00:19:00.276364 / getting data: ihdp 
2020-11-15 00:19:21.136388 / comet url: https://www.comet.ml/sunandr/causal-benchmark/00feddfaceab469ca5ff4622f512a399 
2020-11-15 00:19:21.137718 / ate: None 
2020-11-15 00:19:21.139148 / <models.distributions.distributions.SigmoidFlow object at 0x7efffa5ab590> ndim:4 base_distribution:normal 
2020-11-15 00:19:21.140623 / {'n_hidden_layers': 1, 'dim_h': 2, 'activation': ReLU()} 
2020-11-15 00:19:21.142042 / {'batch_size': 25000, 'lr': 0.001, 'num_epochs': 10000, 'verbose': True, 'print_every_iters': 100, 'optim': <class 'torch.optim.adam.Adam'>, 'eval_every': 100, 'plot_every': 100, 'p_every': 100, 'optim_args': {}} 
2020-11-15 00:19:22.253569 / Iteration 100: 0.6175324320793152 1.0498906373977661 
2020-11-15 00:19:22.258195 / Iteration 100 valid loss 1.6333729028701782 
2020-11-15 00:19:22.259305 / saving best-val-loss model 
2020-11-15 00:19:26.833238 / Iteration 200: 0.5945466756820679 0.9182737469673157 
2020-11-15 00:19:26.838089 / Iteration 200 valid loss 1.4908077716827393 
2020-11-15 00:19:26.839204 / saving best-val-loss model 
2020-11-15 00:19:31.309737 / Iteration 300: 0.5503764152526855 0.49135223031044006 
2020-11-15 00:19:31.314585 / Iteration 300 valid loss 1.0102217197418213 
2020-11-15 00:19:31.315849 / saving best-val-loss model 
2020-11-15 00:19:35.743329 / Iteration 400: 0.45544368028640747 -0.1616709977388382 
2020-11-15 00:19:35.748011 / Iteration 400 valid loss 0.192501500248909 
2020-11-15 00:19:35.749229 / saving best-val-loss model 
2020-11-15 00:19:40.496220 / Iteration 500: 0.4483746290206909 -0.5178323984146118 
2020-11-15 00:19:40.501219 / Iteration 500 valid loss -0.19795268774032593 
2020-11-15 00:19:40.502389 / saving best-val-loss model 
2020-11-15 00:19:45.186457 / Iteration 600: 0.4453151524066925 -0.6770035028457642 
2020-11-15 00:19:45.191911 / Iteration 600 valid loss -0.35356056690216064 
2020-11-15 00:19:45.193116 / saving best-val-loss model 
2020-11-15 00:19:49.850642 / Iteration 700: 0.44371265172958374 -0.7537192106246948 
2020-11-15 00:19:49.855140 / Iteration 700 valid loss -0.42344462871551514 
2020-11-15 00:19:49.856265 / saving best-val-loss model 
2020-11-15 00:19:54.531170 / Iteration 800: 0.44246411323547363 -0.7986478209495544 
2020-11-15 00:19:54.536169 / Iteration 800 valid loss -0.46082040667533875 
2020-11-15 00:19:54.537474 / saving best-val-loss model 
2020-11-15 00:19:59.196255 / Iteration 900: 0.44149312376976013 -0.829684317111969 
2020-11-15 00:19:59.201111 / Iteration 900 valid loss -0.48111090064048767 
2020-11-15 00:19:59.202524 / saving best-val-loss model 
2020-11-15 00:20:03.890154 / Iteration 1000: 0.44047555327415466 -0.8610653281211853 
2020-11-15 00:20:03.894862 / Iteration 1000 valid loss -0.48822352290153503 
2020-11-15 00:20:03.896010 / saving best-val-loss model 
2020-11-15 00:20:08.483401 / Iteration 1100: 0.43908536434173584 -0.8890342712402344 
2020-11-15 00:20:08.488603 / Iteration 1100 valid loss -0.4833217263221741 
2020-11-15 00:20:13.125137 / Iteration 1200: 0.43916112184524536 -0.9065202474594116 
2020-11-15 00:20:13.129945 / Iteration 1200 valid loss -0.48497506976127625 
2020-11-15 00:20:17.711593 / Iteration 1300: 0.43993955850601196 -0.9165317416191101 
2020-11-15 00:20:17.716073 / Iteration 1300 valid loss -0.4966006875038147 
2020-11-15 00:20:17.717298 / saving best-val-loss model 
2020-11-15 00:20:22.358621 / Iteration 1400: 0.44108903408050537 -0.9236254096031189 
2020-11-15 00:20:22.363916 / Iteration 1400 valid loss -0.5074974298477173 
2020-11-15 00:20:22.365181 / saving best-val-loss model 
2020-11-15 00:20:27.140659 / Iteration 1500: 0.44212785363197327 -0.9291567802429199 
2020-11-15 00:20:27.144937 / Iteration 1500 valid loss -0.5127812623977661 
2020-11-15 00:20:27.146205 / saving best-val-loss model 
2020-11-15 00:20:31.856641 / Iteration 1600: 0.44424670934677124 -0.9356966018676758 
2020-11-15 00:20:31.861253 / Iteration 1600 valid loss -0.5215305089950562 
2020-11-15 00:20:31.862594 / saving best-val-loss model 
2020-11-15 00:20:36.450233 / Iteration 1700: 0.4471912980079651 -0.9444620609283447 
2020-11-15 00:20:36.454806 / Iteration 1700 valid loss -0.537409782409668 
2020-11-15 00:20:36.456149 / saving best-val-loss model 
2020-11-15 00:20:41.057571 / Iteration 1800: 0.4504934251308441 -0.9558390378952026 
2020-11-15 00:20:41.063094 / Iteration 1800 valid loss -0.5592529773712158 
2020-11-15 00:20:41.064636 / saving best-val-loss model 
2020-11-15 00:20:45.817744 / Iteration 1900: 0.45401930809020996 -0.9660552144050598 
2020-11-15 00:20:45.822264 / Iteration 1900 valid loss -0.5862597227096558 
2020-11-15 00:20:45.823613 / saving best-val-loss model 
2020-11-15 00:20:50.551897 / Iteration 2000: 0.4598774313926697 -0.981813371181488 
2020-11-15 00:20:50.556357 / Iteration 2000 valid loss -0.6220943927764893 
2020-11-15 00:20:50.557766 / saving best-val-loss model 
2020-11-15 00:20:55.156118 / Iteration 2100: 0.46789300441741943 -1.0059013366699219 
2020-11-15 00:20:55.162018 / Iteration 2100 valid loss -0.6636765003204346 
2020-11-15 00:20:55.163363 / saving best-val-loss model 
2020-11-15 00:20:59.836819 / Iteration 2200: 0.4720894694328308 -1.0284978151321411 
2020-11-15 00:20:59.841261 / Iteration 2200 valid loss -0.691912055015564 
2020-11-15 00:20:59.842660 / saving best-val-loss model 
2020-11-15 00:21:04.416253 / Iteration 2300: 0.4734332263469696 -1.044918179512024 
2020-11-15 00:21:04.420682 / Iteration 2300 valid loss -0.6865336298942566 
2020-11-15 00:21:09.135082 / Iteration 2400: 0.4742279350757599 -1.058424711227417 
2020-11-15 00:21:09.139794 / Iteration 2400 valid loss -0.6889135837554932 
2020-11-15 00:21:13.785216 / Iteration 2500: 0.4738965332508087 -1.0684337615966797 
2020-11-15 00:21:13.789870 / Iteration 2500 valid loss -0.690805196762085 
2020-11-15 00:21:18.448452 / Iteration 2600: 0.47361382842063904 -1.0763951539993286 
2020-11-15 00:21:18.453450 / Iteration 2600 valid loss -0.6931453943252563 
2020-11-15 00:21:18.454783 / saving best-val-loss model 
2020-11-15 00:21:23.316824 / Iteration 2700: 0.47312459349632263 -1.0827864408493042 
2020-11-15 00:21:23.321796 / Iteration 2700 valid loss -0.6953288316726685 
2020-11-15 00:21:23.323663 / saving best-val-loss model 
2020-11-15 00:21:28.107407 / Iteration 2800: 0.47261935472488403 -1.0880709886550903 
2020-11-15 00:21:28.113634 / Iteration 2800 valid loss -0.6970375776290894 
2020-11-15 00:21:28.115601 / saving best-val-loss model 
2020-11-15 00:21:32.888072 / Iteration 2900: 0.4721011519432068 -1.0925242900848389 
2020-11-15 00:21:32.893531 / Iteration 2900 valid loss -0.697758674621582 
2020-11-15 00:21:32.895071 / saving best-val-loss model 
2020-11-15 00:21:37.592274 / Iteration 3000: 0.47155675292015076 -1.0962847471237183 
2020-11-15 00:21:37.597313 / Iteration 3000 valid loss -0.6985048055648804 
2020-11-15 00:21:37.598437 / saving best-val-loss model 
2020-11-15 00:21:42.261078 / Iteration 3100: 0.47102898359298706 -1.0994250774383545 
2020-11-15 00:21:42.265244 / Iteration 3100 valid loss -0.6987521648406982 
2020-11-15 00:21:42.266323 / saving best-val-loss model 
2020-11-15 00:21:46.828009 / Iteration 3200: 0.47055816650390625 -1.1021229028701782 
2020-11-15 00:21:46.832897 / Iteration 3200 valid loss -0.6988897919654846 
2020-11-15 00:21:46.834062 / saving best-val-loss model 
2020-11-15 00:21:51.469566 / Iteration 3300: 0.4702136218547821 -1.1044385433197021 
2020-11-15 00:21:51.474006 / Iteration 3300 valid loss -0.6997534036636353 
2020-11-15 00:21:51.475251 / saving best-val-loss model 
2020-11-15 00:21:56.055402 / Iteration 3400: 0.46999379992485046 -1.1064696311950684 
2020-11-15 00:21:56.059989 / Iteration 3400 valid loss -0.6997160911560059 
2020-11-15 00:22:00.679669 / Iteration 3500: 0.4698701798915863 -1.108284831047058 
2020-11-15 00:22:00.684834 / Iteration 3500 valid loss -0.6998457908630371 
2020-11-15 00:22:00.686198 / saving best-val-loss model 
2020-11-15 00:22:05.368183 / Iteration 3600: 0.46981167793273926 -1.1099175214767456 
2020-11-15 00:22:05.373528 / Iteration 3600 valid loss -0.7006731629371643 
2020-11-15 00:22:05.374918 / saving best-val-loss model 
2020-11-15 00:22:09.918154 / Iteration 3700: 0.46979954838752747 -1.1113497018814087 
2020-11-15 00:22:09.923057 / Iteration 3700 valid loss -0.7028177976608276 
2020-11-15 00:22:09.924229 / saving best-val-loss model 
2020-11-15 00:22:14.576177 / Iteration 3800: 0.4698042869567871 -1.1125890016555786 
2020-11-15 00:22:14.580439 / Iteration 3800 valid loss -0.7048448324203491 
2020-11-15 00:22:14.581509 / saving best-val-loss model 
2020-11-15 00:22:19.237799 / Iteration 3900: 0.46980735659599304 -1.1136962175369263 
2020-11-15 00:22:19.242177 / Iteration 3900 valid loss -0.7064030170440674 
2020-11-15 00:22:19.243409 / saving best-val-loss model 
2020-11-15 00:22:23.841287 / Iteration 4000: 0.46981218457221985 -1.1147032976150513 
2020-11-15 00:22:23.846310 / Iteration 4000 valid loss -0.707435131072998 
2020-11-15 00:22:23.847688 / saving best-val-loss model 
2020-11-15 00:22:28.461741 / Iteration 4100: 0.46981775760650635 -1.1156301498413086 
2020-11-15 00:22:28.466918 / Iteration 4100 valid loss -0.7084535360336304 
2020-11-15 00:22:28.468183 / saving best-val-loss model 
2020-11-15 00:22:33.224540 / Iteration 4200: 0.4698237180709839 -1.116483449935913 
2020-11-15 00:22:33.228680 / Iteration 4200 valid loss -0.709424614906311 
2020-11-15 00:22:33.229964 / saving best-val-loss model 
2020-11-15 00:22:37.914231 / Iteration 4300: 0.469829797744751 -1.1172711849212646 
2020-11-15 00:22:37.918964 / Iteration 4300 valid loss -0.7103447914123535 
2020-11-15 00:22:37.920179 / saving best-val-loss model 
2020-11-15 00:22:42.600888 / Iteration 4400: 0.4698357880115509 -1.1180005073547363 
2020-11-15 00:22:42.605710 / Iteration 4400 valid loss -0.7112148404121399 
2020-11-15 00:22:42.607303 / saving best-val-loss model 
2020-11-15 00:22:47.319959 / Iteration 4500: 0.4698418080806732 -1.118678092956543 
2020-11-15 00:22:47.324880 / Iteration 4500 valid loss -0.7120363712310791 
2020-11-15 00:22:47.326252 / saving best-val-loss model 
2020-11-15 00:22:51.896675 / Iteration 4600: 0.4698476493358612 -1.1193101406097412 
2020-11-15 00:22:51.900941 / Iteration 4600 valid loss -0.7128146886825562 
2020-11-15 00:22:51.902179 / saving best-val-loss model 
2020-11-15 00:22:56.546247 / Iteration 4700: 0.4698532521724701 -1.1199016571044922 
2020-11-15 00:22:56.551482 / Iteration 4700 valid loss -0.7135534286499023 
2020-11-15 00:22:56.552667 / saving best-val-loss model 
2020-11-15 00:23:01.194825 / Iteration 4800: 0.46985888481140137 -1.1204580068588257 
2020-11-15 00:23:01.199324 / Iteration 4800 valid loss -0.7142578363418579 
2020-11-15 00:23:01.200587 / saving best-val-loss model 
2020-11-15 00:23:05.783726 / Iteration 4900: 0.4698643088340759 -1.1209830045700073 
2020-11-15 00:23:05.788349 / Iteration 4900 valid loss -0.7149304151535034 
2020-11-15 00:23:05.789560 / saving best-val-loss model 
2020-11-15 00:23:10.431440 / Iteration 5000: 0.46986955404281616 -1.1214799880981445 
2020-11-15 00:23:10.436102 / Iteration 5000 valid loss -0.7155768871307373 
2020-11-15 00:23:10.437456 / saving best-val-loss model 
2020-11-15 00:23:15.220604 / Iteration 5100: 0.46987462043762207 -1.1219533681869507 
2020-11-15 00:23:15.226131 / Iteration 5100 valid loss -0.7162004709243774 
2020-11-15 00:23:15.227393 / saving best-val-loss model 
2020-11-15 00:23:19.859305 / Iteration 5200: 0.4698795676231384 -1.1224079132080078 
2020-11-15 00:23:19.864271 / Iteration 5200 valid loss -0.7168089747428894 
2020-11-15 00:23:19.865552 / saving best-val-loss model 
2020-11-15 00:23:24.535681 / Iteration 5300: 0.4698846638202667 -1.12286376953125 
2020-11-15 00:23:24.540564 / Iteration 5300 valid loss -0.7174217104911804 
2020-11-15 00:23:24.541987 / saving best-val-loss model 
2020-11-15 00:23:29.188228 / Iteration 5400: 0.46989110112190247 -1.1235413551330566 
2020-11-15 00:23:29.192519 / Iteration 5400 valid loss -0.7181402444839478 
2020-11-15 00:23:29.193764 / saving best-val-loss model 
2020-11-15 00:23:33.797144 / Iteration 5500: 0.46990808844566345 -1.1254695653915405 
2020-11-15 00:23:33.801863 / Iteration 5500 valid loss -0.7121370434761047 
2020-11-15 00:23:38.462428 / Iteration 5600: 0.469923734664917 -1.1272995471954346 
2020-11-15 00:23:38.468097 / Iteration 5600 valid loss -0.7101815938949585 
2020-11-15 00:23:43.245678 / Iteration 5700: 0.4699362814426422 -1.1297751665115356 
2020-11-15 00:23:43.251269 / Iteration 5700 valid loss -0.7122422456741333 
2020-11-15 00:23:47.930574 / Iteration 5800: 0.46993234753608704 -1.1307305097579956 
2020-11-15 00:23:47.935075 / Iteration 5800 valid loss -0.7153550982475281 
2020-11-15 00:23:52.579397 / Iteration 5900: 0.46992969512939453 -1.1314988136291504 
2020-11-15 00:23:52.584632 / Iteration 5900 valid loss -0.7169320583343506 
2020-11-15 00:23:57.208729 / Iteration 6000: 0.469927579164505 -1.132347822189331 
2020-11-15 00:23:57.213001 / Iteration 6000 valid loss -0.7178082466125488 
2020-11-15 00:24:01.819132 / Iteration 6100: 0.46992626786231995 -1.1333353519439697 
2020-11-15 00:24:01.823657 / Iteration 6100 valid loss -0.718367874622345 
2020-11-15 00:24:01.824833 / saving best-val-loss model 
2020-11-15 00:24:06.453322 / Iteration 6200: 0.4699253439903259 -1.134476661682129 
2020-11-15 00:24:06.458026 / Iteration 6200 valid loss -0.7187926769256592 
2020-11-15 00:24:06.459335 / saving best-val-loss model 
2020-11-15 00:24:11.096143 / Iteration 6300: 0.46992582082748413 -1.135745644569397 
2020-11-15 00:24:11.100920 / Iteration 6300 valid loss -0.7192026376724243 
2020-11-15 00:24:11.102161 / saving best-val-loss model 
2020-11-15 00:24:15.728236 / Iteration 6400: 0.4699278771877289 -1.1370959281921387 
2020-11-15 00:24:15.732771 / Iteration 6400 valid loss -0.7195357084274292 
2020-11-15 00:24:15.734092 / saving best-val-loss model 
2020-11-15 00:24:20.255636 / Iteration 6500: 0.46993133425712585 -1.138438105583191 
2020-11-15 00:24:20.260044 / Iteration 6500 valid loss -0.7200102210044861 
2020-11-15 00:24:20.261445 / saving best-val-loss model 
2020-11-15 00:24:24.909903 / Iteration 6600: 0.46993517875671387 -1.1396652460098267 
2020-11-15 00:24:24.915507 / Iteration 6600 valid loss -0.7201391458511353 
2020-11-15 00:24:24.916823 / saving best-val-loss model 
2020-11-15 00:24:29.532116 / Iteration 6700: 0.4699401557445526 -1.1407092809677124 
2020-11-15 00:24:29.537122 / Iteration 6700 valid loss -0.7211397886276245 
2020-11-15 00:24:29.538211 / saving best-val-loss model 
2020-11-15 00:24:34.171812 / Iteration 6800: 0.4699448347091675 -1.141545057296753 
2020-11-15 00:24:34.176652 / Iteration 6800 valid loss -0.7216302156448364 
2020-11-15 00:24:34.178049 / saving best-val-loss model 
2020-11-15 00:24:38.820891 / Iteration 6900: 0.4699501693248749 -1.1422063112258911 
2020-11-15 00:24:38.825805 / Iteration 6900 valid loss -0.7222381830215454 
2020-11-15 00:24:38.827139 / saving best-val-loss model 
2020-11-15 00:24:43.418557 / Iteration 7000: 0.46995431184768677 -1.1427286863327026 
2020-11-15 00:24:43.423387 / Iteration 7000 valid loss -0.7226088047027588 
2020-11-15 00:24:43.424792 / saving best-val-loss model 
2020-11-15 00:24:48.194640 / Iteration 7100: 0.46995851397514343 -1.1431536674499512 
2020-11-15 00:24:48.199338 / Iteration 7100 valid loss -0.7231231927871704 
2020-11-15 00:24:48.200867 / saving best-val-loss model 
2020-11-15 00:25:21.800776 / Namespace(activation='ReLU', atoms=[], batch_size=25000, comet=True, data='ihdp', dataroot='/home/mila/r/raghupas/causal-benchmark/datasets', dim_h=2, dist='SigmoidFlow', dist_args=['ndim=32', 'base_distribution=normal'], early_stop=True, eval=True, grad_norm=inf, ignore_w=False, lr=0.001, n_hidden_layers=1, num_epochs=10000, num_univariate_tests=100, overwrite_reload='', patience=None, saveroot='save', seed=123, test_prop=0.4, test_size=None, train=True, train_prop=0.5, val_prop=0.1, w_transform='Standardize', y_transform='Normalize') 
2020-11-15 00:25:21.803994 / getting data: ihdp 
2020-11-15 00:25:44.342175 / comet url: https://www.comet.ml/sunandr/causal-benchmark/8a92d18bb2314e70bf724c6280a63eae 
2020-11-15 00:25:44.343819 / ate: None 
2020-11-15 00:25:44.345303 / <models.distributions.distributions.SigmoidFlow object at 0x7f18a4af35d0> ndim:32 base_distribution:normal 
2020-11-15 00:25:44.347050 / {'n_hidden_layers': 1, 'dim_h': 2, 'activation': ReLU()} 
2020-11-15 00:25:44.348652 / {'batch_size': 25000, 'lr': 0.001, 'num_epochs': 10000, 'verbose': True, 'print_every_iters': 100, 'optim': <class 'torch.optim.adam.Adam'>, 'eval_every': 100, 'plot_every': 100, 'p_every': 100, 'optim_args': {}} 
2020-11-15 00:25:45.565908 / Iteration 100: 0.6175324320793152 1.1806155443191528 
2020-11-15 00:25:45.573988 / Iteration 100 valid loss 1.7838554382324219 
2020-11-15 00:25:45.575491 / saving best-val-loss model 
2020-11-15 00:25:50.312170 / Iteration 200: 0.5945466756820679 0.7310079336166382 
2020-11-15 00:25:50.317714 / Iteration 200 valid loss 1.3813997507095337 
2020-11-15 00:25:50.319052 / saving best-val-loss model 
2020-11-15 00:25:54.984848 / Iteration 300: 0.5537706017494202 -0.1668223738670349 
2020-11-15 00:25:54.989947 / Iteration 300 valid loss 0.4887234568595886 
2020-11-15 00:25:54.991260 / saving best-val-loss model 
2020-11-15 00:25:59.901378 / Iteration 400: 0.47057992219924927 -0.5808604955673218 
2020-11-15 00:25:59.906375 / Iteration 400 valid loss -0.12693768739700317 
2020-11-15 00:25:59.907763 / saving best-val-loss model 
2020-11-15 00:26:04.819958 / Iteration 500: 0.459689736366272 -0.7480200529098511 
2020-11-15 00:26:04.824927 / Iteration 500 valid loss -0.29988202452659607 
2020-11-15 00:26:04.826038 / saving best-val-loss model 
2020-11-15 00:26:09.805197 / Iteration 600: 0.4551682770252228 -0.8147487640380859 
2020-11-15 00:26:09.810487 / Iteration 600 valid loss -0.32607075572013855 
2020-11-15 00:26:09.811889 / saving best-val-loss model 
2020-11-15 00:26:14.604104 / Iteration 700: 0.44948524236679077 -0.8431055545806885 
2020-11-15 00:26:14.609709 / Iteration 700 valid loss -0.3168230950832367 
2020-11-15 00:26:19.422258 / Iteration 800: 0.44464975595474243 -0.8624417781829834 
2020-11-15 00:26:19.427962 / Iteration 800 valid loss -0.32219693064689636 
2020-11-15 00:26:24.153621 / Iteration 900: 0.44108790159225464 -0.8787406086921692 
2020-11-15 00:26:24.158242 / Iteration 900 valid loss -0.322330117225647 
2020-11-15 00:26:29.093398 / Iteration 1000: 0.43888115882873535 -0.8905019760131836 
2020-11-15 00:26:29.097909 / Iteration 1000 valid loss -0.3192528784275055 
2020-11-15 00:26:33.910050 / Iteration 1100: 0.4368019700050354 -0.8990498781204224 
2020-11-15 00:26:33.914348 / Iteration 1100 valid loss -0.3278139531612396 
2020-11-15 00:26:33.915515 / saving best-val-loss model 
2020-11-15 00:26:38.750573 / Iteration 1200: 0.4354655146598816 -0.9079278111457825 
2020-11-15 00:26:38.755316 / Iteration 1200 valid loss -0.34675759077072144 
2020-11-15 00:26:38.756858 / saving best-val-loss model 
2020-11-15 00:26:43.502341 / Iteration 1300: 0.4354183077812195 -0.9203898310661316 
2020-11-15 00:26:43.507501 / Iteration 1300 valid loss -0.36671867966651917 
2020-11-15 00:26:43.508677 / saving best-val-loss model 
2020-11-15 00:26:48.254379 / Iteration 1400: 0.43647435307502747 -0.9409128427505493 
2020-11-15 00:26:48.259709 / Iteration 1400 valid loss -0.3757951855659485 
2020-11-15 00:26:48.260998 / saving best-val-loss model 
2020-11-15 00:26:53.089926 / Iteration 1500: 0.43727508187294006 -0.9636563658714294 
2020-11-15 00:26:53.094595 / Iteration 1500 valid loss -0.38120049238204956 
2020-11-15 00:26:53.095751 / saving best-val-loss model 
2020-11-15 00:26:57.882152 / Iteration 1600: 0.4382798671722412 -0.9905324578285217 
2020-11-15 00:26:57.887338 / Iteration 1600 valid loss -0.3808600902557373 
2020-11-15 00:27:02.643605 / Iteration 1700: 0.43936601281166077 -1.02306067943573 
2020-11-15 00:27:02.648733 / Iteration 1700 valid loss -0.3939272165298462 
2020-11-15 00:27:02.649898 / saving best-val-loss model 
2020-11-15 00:27:07.424034 / Iteration 1800: 0.4404178261756897 -1.051632046699524 
2020-11-15 00:27:07.428731 / Iteration 1800 valid loss -0.4184972047805786 
2020-11-15 00:27:07.430084 / saving best-val-loss model 
2020-11-15 00:27:12.248154 / Iteration 1900: 0.44301626086235046 -1.0791208744049072 
2020-11-15 00:27:12.253592 / Iteration 1900 valid loss -0.43940064311027527 
2020-11-15 00:27:12.254958 / saving best-val-loss model 
2020-11-15 00:27:17.004486 / Iteration 2000: 0.4464949667453766 -1.157669186592102 
2020-11-15 00:27:17.009425 / Iteration 2000 valid loss -0.47289392352104187 
2020-11-15 00:27:17.011012 / saving best-val-loss model 
2020-11-15 00:27:21.922241 / Iteration 2100: 0.45111098885536194 -1.2004036903381348 
2020-11-15 00:27:21.928169 / Iteration 2100 valid loss -0.4143425226211548 
2020-11-15 00:27:26.757902 / Iteration 2200: 0.456621915102005 -1.241317868232727 
2020-11-15 00:27:26.763198 / Iteration 2200 valid loss -0.3883565068244934 
2020-11-15 00:27:31.614516 / Iteration 2300: 0.4588371217250824 -1.2658261060714722 
2020-11-15 00:27:31.619240 / Iteration 2300 valid loss -0.3848361372947693 
2020-11-15 00:27:36.482134 / Iteration 2400: 0.4593904912471771 -1.2753363847732544 
2020-11-15 00:27:36.487878 / Iteration 2400 valid loss -0.3586021065711975 
2020-11-15 00:27:41.573544 / Iteration 2500: 0.45992311835289 -1.2842683792114258 
2020-11-15 00:27:41.578787 / Iteration 2500 valid loss -0.3406563401222229 
2020-11-15 00:27:46.513159 / Iteration 2600: 0.4605203866958618 -1.2946020364761353 
2020-11-15 00:27:46.518000 / Iteration 2600 valid loss -0.3159966468811035 
2020-11-15 00:27:51.423770 / Iteration 2700: 0.4608718454837799 -1.3032628297805786 
2020-11-15 00:27:51.428420 / Iteration 2700 valid loss -0.29553544521331787 
2020-11-15 00:27:56.237251 / Iteration 2800: 0.46081939339637756 -1.3111368417739868 
2020-11-15 00:27:56.241744 / Iteration 2800 valid loss -0.2857450842857361 
2020-11-15 00:28:01.132002 / Iteration 2900: 0.46052470803260803 -1.318203091621399 
2020-11-15 00:28:01.137109 / Iteration 2900 valid loss -0.2821365296840668 
2020-11-15 00:28:05.991846 / Iteration 3000: 0.4601214528083801 -1.3254791498184204 
2020-11-15 00:28:05.996291 / Iteration 3000 valid loss -0.27357017993927 
2020-11-15 00:28:10.775510 / Iteration 3100: 0.4596872925758362 -1.3326821327209473 
2020-11-15 00:28:10.780871 / Iteration 3100 valid loss -0.25397539138793945 
2020-11-15 00:28:15.724944 / Iteration 3200: 0.45913562178611755 -1.339749813079834 
2020-11-15 00:28:15.729835 / Iteration 3200 valid loss -0.2203681915998459 
2020-11-15 00:28:20.547010 / Iteration 3300: 0.4587194621562958 -1.3452913761138916 
2020-11-15 00:28:20.552204 / Iteration 3300 valid loss -0.18197211623191833 
2020-11-15 00:28:25.407434 / Iteration 3400: 0.4584130644798279 -1.3495774269104004 
2020-11-15 00:28:25.412528 / Iteration 3400 valid loss -0.14387881755828857 
2020-11-15 00:28:30.253937 / Iteration 3500: 0.45815929770469666 -1.3534729480743408 
2020-11-15 00:28:30.258525 / Iteration 3500 valid loss -0.10328099131584167 
2020-11-15 00:28:35.047210 / Iteration 3600: 0.45779895782470703 -1.3584891557693481 
2020-11-15 00:28:35.051998 / Iteration 3600 valid loss -0.049753427505493164 
2020-11-15 00:28:39.933504 / Iteration 3700: 0.45745328068733215 -1.3614614009857178 
2020-11-15 00:28:39.938310 / Iteration 3700 valid loss -0.01811191439628601 
2020-11-15 00:28:44.833731 / Iteration 3800: 0.4572906196117401 -1.3641905784606934 
2020-11-15 00:28:44.838666 / Iteration 3800 valid loss -0.0027911365032196045 
2020-11-15 00:28:49.774921 / Iteration 3900: 0.45719113945961 -1.3667006492614746 
2020-11-15 00:28:49.779574 / Iteration 3900 valid loss 0.015610843896865845 
2020-11-15 00:28:54.477097 / Iteration 4000: 0.45658886432647705 -1.3712687492370605 
2020-11-15 00:28:54.482210 / Iteration 4000 valid loss 0.03737756609916687 
2020-11-15 00:28:59.329849 / Iteration 4100: 0.4557921588420868 -1.3759530782699585 
2020-11-15 00:28:59.334575 / Iteration 4100 valid loss 0.058631330728530884 
2020-11-15 00:29:04.183202 / Iteration 4200: 0.45554885268211365 -1.3783448934555054 
2020-11-15 00:29:04.187580 / Iteration 4200 valid loss 0.04715564846992493 
2020-11-15 00:29:08.987378 / Iteration 4300: 0.4554257392883301 -1.3805564641952515 
2020-11-15 00:29:08.991943 / Iteration 4300 valid loss 0.043760716915130615 
2020-11-15 00:29:13.817310 / Iteration 4400: 0.455401211977005 -1.3828314542770386 
2020-11-15 00:29:13.822059 / Iteration 4400 valid loss 0.04179462790489197 
2020-11-15 00:29:18.604056 / Iteration 4500: 0.45542576909065247 -1.3853740692138672 
2020-11-15 00:29:18.609510 / Iteration 4500 valid loss 0.03892868757247925 
2020-11-15 00:29:23.410254 / Iteration 4600: 0.4554661512374878 -1.3892070055007935 
2020-11-15 00:29:23.415114 / Iteration 4600 valid loss 0.0261915922164917 
2020-11-15 00:29:28.176344 / Iteration 4700: 0.4556230306625366 -1.3984256982803345 
2020-11-15 00:29:28.180817 / Iteration 4700 valid loss 0.00048667192459106445 
2020-11-15 00:29:32.940180 / Iteration 4800: 0.455975741147995 -1.4168025255203247 
2020-11-15 00:29:32.944489 / Iteration 4800 valid loss -0.02176681160926819 
2020-11-15 00:29:37.901662 / Iteration 4900: 0.45615407824516296 -1.4248366355895996 
2020-11-15 00:29:37.905671 / Iteration 4900 valid loss -0.000658571720123291 
2020-11-15 00:29:42.734116 / Iteration 5000: 0.4561361074447632 -1.4306732416152954 
2020-11-15 00:29:42.738870 / Iteration 5000 valid loss 0.0125407874584198 
2020-11-15 00:29:47.693021 / Iteration 5100: 0.4563606083393097 -1.4391396045684814 
2020-11-15 00:29:47.698370 / Iteration 5100 valid loss 0.01209414005279541 
2020-11-15 00:29:52.647085 / Iteration 5200: 0.4563397765159607 -1.4447133541107178 
2020-11-15 00:29:52.652378 / Iteration 5200 valid loss 0.008114486932754517 
2020-11-15 00:29:57.678682 / Iteration 5300: 0.4563242793083191 -1.4499716758728027 
2020-11-15 00:29:57.683849 / Iteration 5300 valid loss 0.00571867823600769 
2020-11-15 00:30:02.565273 / Iteration 5400: 0.4563581943511963 -1.4565752744674683 
2020-11-15 00:30:02.569989 / Iteration 5400 valid loss 0.012455344200134277 
2020-11-15 00:30:07.598664 / Iteration 5500: 0.4563859701156616 -1.4625698328018188 
2020-11-15 00:30:07.603252 / Iteration 5500 valid loss 0.016620397567749023 
2020-11-15 00:30:12.635708 / Iteration 5600: 0.4565037488937378 -1.4681979417800903 
2020-11-15 00:30:12.640942 / Iteration 5600 valid loss 0.024445325136184692 
2020-11-15 00:30:17.549687 / Iteration 5700: 0.45661595463752747 -1.4733308553695679 
2020-11-15 00:30:17.554729 / Iteration 5700 valid loss 0.030113279819488525 
2020-11-15 00:30:22.513310 / Iteration 5800: 0.4567171037197113 -1.4783354997634888 
2020-11-15 00:30:22.517471 / Iteration 5800 valid loss 0.0388161838054657 
2020-11-15 00:30:27.506231 / Iteration 5900: 0.45660853385925293 -1.4840518236160278 
2020-11-15 00:30:27.510830 / Iteration 5900 valid loss 0.04594773054122925 
2020-11-15 00:30:32.572000 / Iteration 6000: 0.45658090710639954 -1.489395022392273 
2020-11-15 00:30:32.577091 / Iteration 6000 valid loss 0.05822250247001648 
2020-11-15 00:30:37.618183 / Iteration 6100: 0.4565484821796417 -1.4944720268249512 
2020-11-15 00:30:37.622949 / Iteration 6100 valid loss 0.07296085357666016 
2020-11-15 00:30:42.699733 / Iteration 6200: 0.45654773712158203 -1.4993311166763306 
2020-11-15 00:30:42.704558 / Iteration 6200 valid loss 0.0859442949295044 
2020-11-15 00:30:47.870046 / Iteration 6300: 0.456539511680603 -1.5031732320785522 
2020-11-15 00:30:47.875072 / Iteration 6300 valid loss 0.08998724818229675 
2020-11-15 00:30:52.967232 / Iteration 6400: 0.45649516582489014 -1.5070949792861938 
2020-11-15 00:30:52.972305 / Iteration 6400 valid loss 0.0872214138507843 
2020-11-15 00:30:58.137810 / Iteration 6500: 0.45621979236602783 -1.5105335712432861 
2020-11-15 00:30:58.142550 / Iteration 6500 valid loss 0.08189389109611511 
2020-11-15 00:31:03.508615 / Iteration 6600: 0.45615294575691223 -1.513748049736023 
2020-11-15 00:31:03.513346 / Iteration 6600 valid loss 0.07218042016029358 
2020-11-15 00:31:08.716308 / Iteration 6700: 0.4561309814453125 -1.5167590379714966 
2020-11-15 00:31:08.721528 / Iteration 6700 valid loss 0.06338855624198914 
2020-11-15 00:31:14.004397 / Iteration 6800: 0.45614954829216003 -1.5268383026123047 
2020-11-15 00:31:14.009163 / Iteration 6800 valid loss 0.07992586493492126 
2020-11-15 00:31:19.347394 / Iteration 6900: 0.45599058270454407 -1.5314208269119263 
2020-11-15 00:31:19.353067 / Iteration 6900 valid loss 0.08747881650924683 
2020-11-15 00:31:24.727047 / Iteration 7000: 0.45576444268226624 -1.5358130931854248 
2020-11-15 00:31:24.732259 / Iteration 7000 valid loss 0.08534300327301025 
2020-11-15 00:31:30.171091 / Iteration 7100: 0.45532655715942383 -1.5395746231079102 
2020-11-15 00:31:30.176107 / Iteration 7100 valid loss 0.0835648775100708 
2020-11-15 00:31:35.539854 / Iteration 7200: 0.4544621407985687 -1.5445002317428589 
2020-11-15 00:31:35.545070 / Iteration 7200 valid loss 0.07813200354576111 
2020-11-15 00:31:40.793078 / Iteration 7300: 0.4541573226451874 -1.5498192310333252 
2020-11-15 00:31:40.797335 / Iteration 7300 valid loss 0.07170343399047852 
2020-11-15 00:31:46.062053 / Iteration 7400: 0.4539421498775482 -1.554786205291748 
2020-11-15 00:31:46.067607 / Iteration 7400 valid loss 0.06869593262672424 
2020-11-15 00:31:51.307782 / Iteration 7500: 0.45381397008895874 -1.559561848640442 
2020-11-15 00:31:51.312887 / Iteration 7500 valid loss 0.06836065649986267 
2020-11-15 00:31:56.815886 / Iteration 7600: 0.4537762999534607 -1.564296007156372 
2020-11-15 00:31:56.821254 / Iteration 7600 valid loss 0.07291826605796814 
2020-11-15 00:32:02.164054 / Iteration 7700: 0.45367035269737244 -1.5746982097625732 
2020-11-15 00:32:02.168980 / Iteration 7700 valid loss 0.18101857602596283 
2020-11-15 00:32:07.632296 / Iteration 7800: 0.4536598026752472 -1.5826293230056763 
2020-11-15 00:32:07.637113 / Iteration 7800 valid loss 0.21215994656085968 
2020-11-15 00:32:13.150649 / Iteration 7900: 0.45364299416542053 -1.59010648727417 
2020-11-15 00:32:13.155665 / Iteration 7900 valid loss 0.26780983805656433 
2020-11-15 00:32:18.645662 / Iteration 8000: 0.4536162316799164 -1.6003121137619019 
2020-11-15 00:32:18.651520 / Iteration 8000 valid loss 0.30744925141334534 
2020-11-15 00:32:24.098473 / Iteration 8100: 0.45351067185401917 -1.6078649759292603 
2020-11-15 00:32:24.103549 / Iteration 8100 valid loss 0.3501841127872467 
2020-11-15 00:32:29.497707 / Iteration 8200: 0.45341476798057556 -1.613146424293518 
2020-11-15 00:32:29.503107 / Iteration 8200 valid loss 0.37199997901916504 
2020-11-15 00:32:34.993715 / Iteration 8300: 0.45334750413894653 -1.6170694828033447 
2020-11-15 00:32:34.998707 / Iteration 8300 valid loss 0.38999050855636597 
2020-11-15 00:32:40.267948 / Iteration 8400: 0.4533405303955078 -1.620303750038147 
2020-11-15 00:32:40.272790 / Iteration 8400 valid loss 0.4029068648815155 
2020-11-15 00:32:45.642826 / Iteration 8500: 0.45326560735702515 -1.6233614683151245 
2020-11-15 00:32:45.648531 / Iteration 8500 valid loss 0.411735862493515 
2020-11-15 00:32:51.129608 / Iteration 8600: 0.4532412886619568 -1.6260292530059814 
2020-11-15 00:32:51.134981 / Iteration 8600 valid loss 0.42284128069877625 
2020-11-15 00:32:56.563987 / Iteration 8700: 0.4532071650028229 -1.6284503936767578 
2020-11-15 00:32:56.569530 / Iteration 8700 valid loss 0.43388235569000244 
2020-11-15 00:33:02.005836 / Iteration 8800: 0.45323091745376587 -1.6311489343643188 
2020-11-15 00:33:02.011129 / Iteration 8800 valid loss 0.4503450393676758 
2020-11-15 00:33:07.413018 / Iteration 8900: 0.4532952904701233 -1.6335887908935547 
2020-11-15 00:33:07.418192 / Iteration 8900 valid loss 0.4758303761482239 
2020-11-15 00:33:12.806002 / Iteration 9000: 0.45333483815193176 -1.6361950635910034 
2020-11-15 00:33:12.810765 / Iteration 9000 valid loss 0.5005964636802673 
2020-11-15 00:33:18.141688 / Iteration 9100: 0.4533429443836212 -1.6384848356246948 
2020-11-15 00:33:18.146614 / Iteration 9100 valid loss 0.5362017750740051 
2020-11-15 00:33:23.556840 / Iteration 9200: 0.45331957936286926 -1.6405835151672363 
2020-11-15 00:33:23.561687 / Iteration 9200 valid loss 0.5773410797119141 
2020-11-15 00:33:28.872247 / Iteration 9300: 0.45331403613090515 -1.6424462795257568 
2020-11-15 00:33:28.877080 / Iteration 9300 valid loss 0.6319169998168945 
2020-11-15 00:33:34.162175 / Iteration 9400: 0.4532918334007263 -1.6448155641555786 
2020-11-15 00:33:34.167096 / Iteration 9400 valid loss 0.6875123381614685 
2020-11-15 00:33:39.492873 / Iteration 9500: 0.45325160026550293 -1.6466562747955322 
2020-11-15 00:33:39.498080 / Iteration 9500 valid loss 0.7485573291778564 
2020-11-15 00:33:45.161493 / Iteration 9600: 0.4533000588417053 -1.648642659187317 
2020-11-15 00:33:45.166332 / Iteration 9600 valid loss 0.8098033666610718 
2020-11-15 00:33:50.531078 / Iteration 9700: 0.45321258902549744 -1.6502920389175415 
2020-11-15 00:33:50.536851 / Iteration 9700 valid loss 0.8640703558921814 
2020-11-15 00:33:56.155666 / Iteration 9800: 0.4532122015953064 -1.6521854400634766 
2020-11-15 00:33:56.160808 / Iteration 9800 valid loss 0.9149377346038818 
2020-11-15 00:34:01.725314 / Iteration 9900: 0.4531828463077545 -1.6534228324890137 
2020-11-15 00:34:01.730808 / Iteration 9900 valid loss 0.9608818292617798 
2020-11-15 00:34:07.236859 / Iteration 10000: 0.45314034819602966 -1.655277132987976 
2020-11-15 00:34:07.242151 / Iteration 10000 valid loss 0.996454656124115 
2020-11-15 00:34:56.007332 / OrderedDict([('nll', -0.47289392352104187), ('avg_t_pval', 0.9203691184272923), ('avg_y_pval', 0.3157087878926132), ('min_t_pval', 0.28872345077441464), ('min_y_pval', 0.008709102383808287), ('q30_t_pval', 0.9816953289059294), ('q30_y_pval', 0.1524785601447504), ('q50_t_pval', 0.9999202980080956), ('q50_y_pval', 0.2734173425203113), ('ate_exact', 0.0), ('ate_noisy', 4.194891777038574)]) 
